{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    # random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 42\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O0TfzOhU-QlG"
   },
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 20\n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 300\n",
    "    self.local_epochs = 2\n",
    "    self.lr = None\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 300\n",
    "    self.worker_num = 20\n",
    "    self.sample_num = 20\n",
    "    self.device = device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion_ce = nn.CrossEntropyLoss()\n",
    "    self.criterion_kl = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned value\n",
    "lr = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = []\n",
    "lr_list.append(10**-3.0)\n",
    "lr_list.append(10**-2.5)\n",
    "lr_list.append(10**-2.0)\n",
    "lr_list.append(10**-1.5)\n",
    "lr_list.append(10**-1.0)\n",
    "lr_list.append(10**-0.5)\n",
    "lr_list.append(10**0.0)\n",
    "lr_list.append(10**0.5)\n",
    "\n",
    "args.lr = lr_list[lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,dataset,worker_id):\n",
    "    self.data = []\n",
    "    self.target = []\n",
    "    self.id = worker_id\n",
    "    for data in dataset.take(len(dataset)):\n",
    "      self.data.append(torch.tensor([data['pixels'].numpy()]))\n",
    "      self.target.append(torch.tensor(data['label'].numpy().astype(np.int64)))\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.data[index],self.target[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/federated_trainset_femnist.pickle', 'rb') as f:\n",
    "    all_federated_trainset = pickle.load(f)\n",
    "with open('../data/federated_testset_femnist.pickle', 'rb') as f:\n",
    "    all_federated_testset = pickle.load(f)\n",
    "all_worker_num = len(all_federated_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_id_list = random.sample(range(all_worker_num),args.worker_num)\n",
    "print(worker_id_list)\n",
    "federated_trainset = []\n",
    "federated_testset = []\n",
    "for i in worker_id_list:\n",
    "    federated_trainset.append(all_federated_trainset[i])\n",
    "    federated_testset.append(all_federated_testset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_valset = [None]*args.worker_num\n",
    "for i in range(args.worker_num):\n",
    "  n_samples = len(federated_trainset[i])\n",
    "  if n_samples==1:\n",
    "    federated_valset[i] = copy.deepcopy(federated_trainset[i])\n",
    "  else:\n",
    "    train_size = int(len(federated_trainset[i]) * 0.7) \n",
    "    val_size = n_samples - train_size \n",
    "    federated_trainset[i],federated_valset[i] = torch.utils.data.random_split(federated_trainset[i], [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1, self).__init__()\n",
    "        self.conv2d_1 = torch.nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.max_pooling = nn.MaxPool2d(2, stride=2)\n",
    "        self.dropout_1 = nn.Dropout(0.25)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_1 = nn.Linear(5408, 128)\n",
    "        self.dropout_2 = nn.Dropout(0.5)\n",
    "        self.linear_2 = nn.Linear(128,62)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.max_pooling(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear_1(x))\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "class CNN2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN2, self).__init__()\n",
    "        self.conv2d_1 = torch.nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.max_pooling = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv2d_2 = torch.nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.dropout_1 = nn.Dropout(0.25)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_1 = nn.Linear(9216, 128)\n",
    "        self.dropout_2 = nn.Dropout(0.5)\n",
    "        self.linear_2 = nn.Linear(128,62)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.max_pooling(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear_1(x))\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "class CNN3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN3, self).__init__()\n",
    "        self.conv2d_1 = torch.nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.max_pooling = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv2d_2 = torch.nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv2d_3 = torch.nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.dropout_1 = nn.Dropout(0.25)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_1 = nn.Linear(15488, 128)\n",
    "        self.dropout_2 = nn.Dropout(0.5)\n",
    "        self.linear_2 = nn.Linear(128,62)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.conv2d_3(x)\n",
    "        x = self.max_pooling(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear_1(x))\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "    \n",
    "class CNN4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN4, self).__init__()\n",
    "        self.conv2d_1 = torch.nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.max_pooling = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv2d_2 = torch.nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv2d_3 = torch.nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.conv2d_4 = torch.nn.Conv2d(128, 256, kernel_size=3)\n",
    "        self.dropout_1 = nn.Dropout(0.25)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_1 = nn.Linear(25600, 128)\n",
    "        self.dropout_2 = nn.Dropout(0.5)\n",
    "        self.linear_2 = nn.Linear(128,62)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.conv2d_3(x)\n",
    "        x = self.conv2d_4(x)\n",
    "        x = self.max_pooling(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear_1(x))\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self):\n",
    "    self.global_model = CNN2()\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(federated_trainset[i],federated_valset[i],federated_testset[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "\n",
    "  def send_model(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "\n",
    "    for worker in workers:\n",
    "      worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "      worker.global_model = copy.deepcopy(self.global_model)\n",
    "\n",
    "  def aggregate_model(self,workers):   \n",
    "    new_params = OrderedDict()\n",
    "    for i,worker in enumerate(workers):\n",
    "      worker_state = worker.global_model.state_dict()\n",
    "      for key in worker_state.keys():\n",
    "        if i==0:\n",
    "          new_params[key] = worker_state[key]*worker.aggregation_weight\n",
    "        else:\n",
    "          new_params[key] += worker_state[key]*worker.aggregation_weight\n",
    "      worker.global_model = worker.global_model.to('cpu')\n",
    "      del worker.global_model\n",
    "    self.global_model.load_state_dict(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,trainset,valset,testset):\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.local_model = CNN2()\n",
    "    self.global_model = None\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "    self.aggregation_weight = None\n",
    "\n",
    "  def local_train(self):\n",
    "    self.local_model = self.local_model.to(args.device)\n",
    "    self.global_model = self.global_model.to(args.device)\n",
    "    local_optimizer = optim.SGD(self.local_model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "    global_optimizer = optim.SGD(self.global_model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "    self.local_model.train()\n",
    "    self.global_model.train()\n",
    "    for epoch in range(args.local_epochs):\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in self.trainloader:\n",
    "        data,labels = Variable(data),Variable(labels)\n",
    "        data,labels = data.to(args.device),labels.to(args.device)\n",
    "        local_optimizer.zero_grad()\n",
    "        global_optimizer.zero_grad()\n",
    "        local_outputs = self.local_model(data)\n",
    "        global_outputs = self.global_model(data)\n",
    "        #train local_model\n",
    "        ce_loss = args.criterion_ce(local_outputs,labels)\n",
    "        kl_loss = args.criterion_kl(F.log_softmax(local_outputs, dim = 1),F.softmax(Variable(global_outputs), dim=1))\n",
    "        loss = ce_loss + kl_loss\n",
    "        running_loss += loss.item()\n",
    "        predicted = torch.argmax(local_outputs,dim=1)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "        count += len(labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), args.clip)\n",
    "        local_optimizer.step()\n",
    "\n",
    "        #train global_model\n",
    "        ce_loss = args.criterion_ce(global_outputs,labels)\n",
    "        kl_loss = args.criterion_kl(F.log_softmax(global_outputs, dim = 1),F.softmax(Variable(local_outputs), dim=1))\n",
    "        loss = ce_loss + kl_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.global_model.parameters(), args.clip)\n",
    "        global_optimizer.step()\n",
    "        \n",
    "    return 100.0*correct/count,running_loss/len(self.trainloader)\n",
    "\n",
    "\n",
    "  def validate(self):\n",
    "    acc,loss = test(self.local_model,args.criterion_ce,self.valloader)\n",
    "    self.local_model = self.local_model.to('cpu')\n",
    "    return acc,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7-GY66gROuEU"
   },
   "outputs": [],
   "source": [
    "def train(model,criterion,trainloader,epochs):\n",
    "  optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for (data,labels) in trainloader:\n",
    "      data,labels = Variable(data),Variable(labels)\n",
    "      data,labels = data.to(args.device),labels.to(args.device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(data)\n",
    "      loss = criterion(outputs,labels)\n",
    "      running_loss += loss.item()\n",
    "      predicted = torch.argmax(outputs,dim=1)\n",
    "      correct += (predicted==labels).sum().item()\n",
    "      count += len(labels)\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "      optimizer.step()\n",
    "\n",
    "  return 100.0*correct/count,running_loss/len(trainloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  count = 0\n",
    "  for (data,labels) in testloader:\n",
    "    data,labels = data.to(args.device),labels.to(args.device)\n",
    "    outputs = model(data)\n",
    "    running_loss += criterion(outputs,labels).item()\n",
    "    predicted = torch.argmax(outputs,dim=1)\n",
    "    correct += (predicted==labels).sum().item()\n",
    "    count += len(labels)\n",
    "\n",
    "  accuracy = 100.0*correct/count\n",
    "  loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "  return accuracy,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WMO7_WSLHeGl"
   },
   "outputs": [],
   "source": [
    "class Early_Stopping():\n",
    "  def __init__(self,partience):\n",
    "    self.step = 0\n",
    "    self.loss = float('inf')\n",
    "    self.partience = partience\n",
    "\n",
    "  def validate(self,loss):\n",
    "    if self.loss<loss:\n",
    "      self.step += 1\n",
    "      if self.step>self.partience:\n",
    "        return True\n",
    "    else:\n",
    "      self.step = 0\n",
    "      self.loss = loss\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "-noG_98IR-nZ",
    "outputId": "78a6ebe2-854a-4f83-dc45-5c4ac35b69e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1  loss:3.9758637785911564  accuracy:5.973255680153494\n",
      "Epoch2  loss:3.8969445824623112  accuracy:5.716659533029397\n",
      "Epoch3  loss:3.8460997104644767  accuracy:6.089228992555377\n",
      "Epoch4  loss:3.8059015393257143  accuracy:5.997011686052355\n",
      "Epoch5  loss:3.7872641324996943  accuracy:6.283252055513626\n",
      "Epoch6  loss:3.7539292097091677  accuracy:6.349184218745571\n",
      "Epoch7  loss:3.737978625297546  accuracy:6.364112552822841\n",
      "Epoch8  loss:3.729163289070129  accuracy:5.99290414482218\n",
      "Epoch9  loss:3.714316129684448  accuracy:6.315380259692443\n",
      "Epoch10  loss:3.7051325678825373  accuracy:7.024049036467532\n",
      "Epoch11  loss:3.6954625010490414  accuracy:6.972353980279281\n",
      "Epoch12  loss:3.683423256874085  accuracy:6.846865185401258\n",
      "Epoch13  loss:3.6847989082336428  accuracy:7.212818185369571\n",
      "Epoch14  loss:3.6787400603294373  accuracy:7.697442390676607\n",
      "Epoch15  loss:3.6661940813064575  accuracy:8.465884564560252\n",
      "Epoch16  loss:3.666495823860168  accuracy:7.6054755464654695\n",
      "Epoch17  loss:3.6493358016014104  accuracy:7.147534915420828\n",
      "Epoch18  loss:3.644423639774323  accuracy:8.504168848521674\n",
      "Epoch19  loss:3.628440678119659  accuracy:7.355048040683471\n",
      "Epoch20  loss:3.623735630512238  accuracy:9.246370351291867\n",
      "Epoch21  loss:3.6115884542465206  accuracy:9.42082224963542\n",
      "Epoch22  loss:3.6058225989341732  accuracy:7.880371719223117\n",
      "Epoch23  loss:3.6019762635231016  accuracy:9.222904268614975\n",
      "Epoch24  loss:3.590297448635101  accuracy:10.049914314262283\n",
      "Epoch25  loss:3.5762491226196285  accuracy:10.71973329704882\n",
      "Epoch26  loss:3.5680088996887216  accuracy:10.025283480647044\n",
      "Epoch27  loss:3.5542279839515687  accuracy:11.439528308149239\n",
      "Epoch28  loss:3.542012298107147  accuracy:10.64608512401286\n",
      "Epoch29  loss:3.532515108585358  accuracy:11.861827226888488\n",
      "Epoch30  loss:3.508877289295197  accuracy:13.162729861407874\n",
      "Epoch31  loss:3.499259746074677  accuracy:12.943655756021705\n",
      "Epoch32  loss:3.4823765039443964  accuracy:12.876629791953969\n",
      "Epoch33  loss:3.4643278956413264  accuracy:14.525246963913894\n",
      "Epoch34  loss:3.4546467661857614  accuracy:16.367413885384202\n",
      "Epoch35  loss:3.431581950187683  accuracy:15.58980302672582\n",
      "Epoch36  loss:3.4183065295219417  accuracy:17.12723744109838\n",
      "Epoch37  loss:3.4028251647949213  accuracy:17.563639684792232\n",
      "Epoch38  loss:3.386019492149353  accuracy:17.658981048854674\n",
      "Epoch39  loss:3.365111756324768  accuracy:19.782566646960735\n",
      "Epoch40  loss:3.3522059082984925  accuracy:19.74208562873432\n",
      "Epoch41  loss:3.318680059909821  accuracy:20.943594477221538\n",
      "Epoch42  loss:3.3000523447990413  accuracy:22.892246381980925\n",
      "Epoch43  loss:3.287758159637451  accuracy:22.037640773737767\n",
      "Epoch44  loss:3.2606297373771667  accuracy:23.680059339791292\n",
      "Epoch45  loss:3.2470065355300908  accuracy:24.368118048785732\n",
      "Epoch46  loss:3.220239782333374  accuracy:24.572234893820447\n",
      "Epoch47  loss:3.203541469573975  accuracy:25.323005856231237\n",
      "Epoch48  loss:3.1813677072525026  accuracy:26.90332534436828\n",
      "Epoch49  loss:3.1640098810195925  accuracy:26.399071020008556\n",
      "Epoch50  loss:3.132916402816772  accuracy:27.512193246763005\n",
      "Epoch51  loss:3.114245879650117  accuracy:28.55008664931533\n",
      "Epoch52  loss:3.0954589724540704  accuracy:29.341703779908407\n",
      "Epoch53  loss:3.0758015751838683  accuracy:30.67899321949819\n",
      "Epoch54  loss:3.0514045715332037  accuracy:31.389420584341337\n",
      "Epoch55  loss:3.027168548107147  accuracy:33.426054579666356\n",
      "Epoch56  loss:3.015722918510437  accuracy:32.15397671509841\n",
      "Epoch57  loss:2.9793680429458615  accuracy:34.08413060755306\n",
      "Epoch58  loss:2.9617385983467104  accuracy:35.26753574287037\n",
      "Epoch59  loss:2.9383554100990303  accuracy:36.50251206254595\n",
      "Epoch60  loss:2.9158801674842834  accuracy:36.70743734517377\n",
      "Epoch61  loss:2.893703389167786  accuracy:37.12887183772107\n",
      "Epoch62  loss:2.867193150520325  accuracy:40.50812443353534\n",
      "Epoch63  loss:2.848271811008453  accuracy:39.50090919198074\n",
      "Epoch64  loss:2.8266902208328246  accuracy:39.54427914887064\n",
      "Epoch65  loss:2.8036396861076356  accuracy:40.82739634622843\n",
      "Epoch66  loss:2.780736041069031  accuracy:42.052148357627736\n",
      "Epoch67  loss:2.763271468877792  accuracy:43.03899233083917\n",
      "Epoch68  loss:2.7314886033535  accuracy:43.77999098382606\n",
      "Epoch69  loss:2.717056101560593  accuracy:44.58048606362933\n",
      "Epoch70  loss:2.6988404929637912  accuracy:44.849067039946036\n",
      "Epoch71  loss:2.6838249146938318  accuracy:46.850523042223394\n",
      "Epoch72  loss:2.656666123867035  accuracy:47.571163506203085\n",
      "Epoch73  loss:2.6418552458286286  accuracy:46.97054357982383\n",
      "Epoch74  loss:2.6126598000526426  accuracy:48.03786244845253\n",
      "Epoch75  loss:2.5992862403392794  accuracy:49.23110939823051\n",
      "Epoch76  loss:2.5692027211189266  accuracy:50.489877399792576\n",
      "Epoch79  loss:2.5180304288864135  accuracy:51.30978477465168\n",
      "Epoch80  loss:2.508638513088226  accuracy:53.29472155461982\n",
      "Epoch81  loss:2.488160914182663  accuracy:51.76383995991018\n",
      "Epoch82  loss:2.4552377343177794  accuracy:54.077567300121025\n",
      "Epoch83  loss:2.453085601329804  accuracy:54.49927210337894\n",
      "Epoch84  loss:2.4294039428234093  accuracy:54.304813023453825\n",
      "Epoch85  loss:2.4106936097145075  accuracy:53.972976878827815\n",
      "Epoch86  loss:2.3916310429573056  accuracy:55.32036246245606\n",
      "Epoch87  loss:2.372426474094391  accuracy:56.00973596268246\n",
      "Epoch88  loss:2.3649513125419617  accuracy:55.934987193654415\n",
      "Epoch89  loss:2.3445575177669524  accuracy:56.76797230184812\n",
      "Epoch90  loss:2.3299640655517573  accuracy:57.55344529180839\n",
      "Epoch91  loss:2.317228364944458  accuracy:57.21372132749259\n",
      "Epoch92  loss:2.2958302557468415  accuracy:57.739898667660476\n",
      "Epoch93  loss:2.281137037277222  accuracy:59.00841740513115\n",
      "Epoch94  loss:2.2718475878238684  accuracy:58.007860375269594\n",
      "Epoch95  loss:2.2647616565227504  accuracy:59.27822202953582\n",
      "Epoch96  loss:2.2400305330753327  accuracy:59.54022762455031\n",
      "Epoch97  loss:2.2275758802890775  accuracy:59.896010190648624\n",
      "Epoch98  loss:2.206615841388702  accuracy:60.17496108398242\n",
      "Epoch99  loss:2.198713952302933  accuracy:59.47113097623769\n",
      "Epoch100  loss:2.1915643334388735  accuracy:59.95973474001031\n",
      "Epoch101  loss:2.1722208440303805  accuracy:60.754694806907985\n",
      "Epoch102  loss:2.160589700937271  accuracy:60.58074776048528\n",
      "Epoch103  loss:2.154858505725861  accuracy:61.24578061700572\n",
      "Epoch104  loss:2.136949402093887  accuracy:61.12087708732161\n",
      "Epoch105  loss:2.1218270361423492  accuracy:62.497347716747434\n",
      "Epoch106  loss:2.11970077753067  accuracy:61.45140107484061\n",
      "Epoch107  loss:2.1117157578468317  accuracy:62.46576951508158\n",
      "Epoch108  loss:2.0844278275966643  accuracy:62.095883362065834\n",
      "Epoch109  loss:2.082762604951858  accuracy:62.6890990980931\n",
      "Epoch110  loss:2.071157586574554  accuracy:63.3880618801968\n",
      "Epoch111  loss:2.061741983890533  accuracy:63.771965567927005\n",
      "Epoch112  loss:2.0502925455570224  accuracy:63.543245168768884\n",
      "Epoch113  loss:2.042052584886551  accuracy:64.37654788588982\n",
      "Epoch114  loss:2.0311949014663697  accuracy:63.77667969581322\n",
      "Epoch115  loss:2.0161583602428435  accuracy:64.64989415505438\n",
      "Epoch116  loss:2.020764923095703  accuracy:63.55745036995203\n",
      "Epoch117  loss:2.004972553253174  accuracy:64.69709491736727\n",
      "Epoch118  loss:2.00075290799141  accuracy:64.18002708704114\n",
      "Epoch119  loss:1.9914176166057587  accuracy:64.44996420122976\n",
      "Epoch120  loss:1.9714583337306977  accuracy:64.60275086571757\n",
      "Epoch121  loss:1.9672458589076995  accuracy:65.37760397622554\n",
      "Epoch122  loss:1.9675417482852937  accuracy:64.6385858329689\n",
      "Epoch123  loss:1.9469884991645812  accuracy:64.74937901615242\n",
      "Epoch124  loss:1.9505038559436798  accuracy:65.42417102836795\n",
      "Epoch125  loss:1.9373082995414732  accuracy:65.45191819582696\n",
      "Epoch126  loss:1.9338640093803405  accuracy:65.31155240247287\n",
      "Epoch127  loss:1.922180289030075  accuracy:66.3765867702213\n",
      "Epoch128  loss:1.9198567271232607  accuracy:64.61734137222236\n",
      "Epoch129  loss:1.9074325799942016  accuracy:66.15977880671403\n",
      "Epoch130  loss:1.8935073077678681  accuracy:65.36332237368232\n",
      "Epoch131  loss:1.888689363002777  accuracy:66.0747956278051\n",
      "Epoch132  loss:1.8929712295532226  accuracy:65.93424432427065\n",
      "Epoch133  loss:1.888352012634277  accuracy:65.55599755995387\n",
      "Epoch134  loss:1.879691231250763  accuracy:66.56981070266399\n",
      "Epoch135  loss:1.8684555649757386  accuracy:65.83004151797155\n",
      "Epoch136  loss:1.8543305993080141  accuracy:66.41620662905706\n",
      "Epoch137  loss:1.861139404773712  accuracy:65.38008026121933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch138  loss:1.8502069234848022  accuracy:65.83517775036628\n",
      "Epoch139  loss:1.848746865987778  accuracy:66.8108993402879\n",
      "Epoch140  loss:1.8367010235786434  accuracy:66.63256641172578\n",
      "Epoch141  loss:1.8364133954048156  accuracy:66.53655107240651\n",
      "Epoch142  loss:1.8228666603565216  accuracy:66.25896903234843\n",
      "Epoch143  loss:1.820577454566956  accuracy:66.64946040015244\n",
      "Epoch144  loss:1.814760601520538  accuracy:66.94247213764685\n",
      "Epoch145  loss:1.8070717990398406  accuracy:66.65234666486805\n",
      "Epoch146  loss:1.806824690103531  accuracy:66.12680475033414\n",
      "Epoch147  loss:1.7999475479125975  accuracy:67.13049121528185\n",
      "Epoch148  loss:1.7899877429008484  accuracy:67.3398970942195\n",
      "Epoch149  loss:1.785525208711624  accuracy:67.36296561241676\n",
      "Epoch150  loss:1.7840216398239135  accuracy:66.58453394739598\n",
      "Epoch151  loss:1.7747284293174743  accuracy:67.26608032029127\n",
      "Epoch152  loss:1.7818491876125335  accuracy:67.11828810351302\n",
      "Epoch153  loss:1.763768815994263  accuracy:67.54527364601938\n",
      "Epoch154  loss:1.7727110505104064  accuracy:67.27794656065656\n",
      "Epoch155  loss:1.7651806414127351  accuracy:66.93988108433717\n",
      "Epoch156  loss:1.7563812255859372  accuracy:67.86563171040903\n",
      "Epoch157  loss:1.7497710585594177  accuracy:67.9838836755945\n",
      "Epoch158  loss:1.7446165442466737  accuracy:67.6557206746147\n",
      "Epoch159  loss:1.751008439064026  accuracy:67.08328343106811\n",
      "Epoch160  loss:1.745555758476257  accuracy:67.55388515879716\n",
      "Epoch161  loss:1.7358710587024688  accuracy:67.54441617461165\n",
      "Epoch162  loss:1.7282845735549928  accuracy:67.4676022986923\n",
      "Epoch163  loss:1.7222078382968902  accuracy:67.0737788953941\n",
      "Epoch164  loss:1.7206716954708101  accuracy:67.68032790666706\n",
      "Epoch165  loss:1.716054797172546  accuracy:67.83680497437742\n",
      "Epoch166  loss:1.709801405668259  accuracy:68.0810889470255\n",
      "Epoch167  loss:1.7072398245334626  accuracy:68.68691782291836\n",
      "Epoch168  loss:1.711861860752106  accuracy:67.97455463614429\n",
      "Epoch169  loss:1.6930352270603184  accuracy:68.0670358168408\n",
      "Epoch172  loss:1.6933978915214536  accuracy:67.92825255275251\n",
      "Epoch173  loss:1.6861172080039974  accuracy:68.16735369139772\n",
      "Epoch174  loss:1.6807252109050745  accuracy:68.77749430085163\n",
      "Epoch175  loss:1.6825222432613374  accuracy:68.00794837197385\n",
      "Epoch176  loss:1.6816769897937769  accuracy:68.10812900937165\n",
      "Epoch177  loss:1.6778462648391723  accuracy:68.209427020198\n",
      "Epoch178  loss:1.668998223543167  accuracy:67.92904293198336\n",
      "Epoch179  loss:1.6597453534603115  accuracy:68.48549701989089\n",
      "Epoch180  loss:1.6678882479667665  accuracy:68.22644149472737\n",
      "Epoch181  loss:1.6617102146148681  accuracy:68.65659593452484\n",
      "Epoch182  loss:1.6501255095005034  accuracy:68.48819812686668\n",
      "Epoch183  loss:1.6457019209861754  accuracy:68.63060569857618\n",
      "Epoch184  loss:1.6479165136814116  accuracy:68.28973808075983\n",
      "Epoch185  loss:1.6430806994438172  accuracy:68.51150908963776\n",
      "Epoch186  loss:1.6402454853057862  accuracy:68.66792554746293\n",
      "Epoch187  loss:1.6380425274372101  accuracy:68.85068622938064\n",
      "Epoch188  loss:1.6379779696464536  accuracy:68.39940869178574\n",
      "Epoch189  loss:1.6322156846523281  accuracy:68.4561477450804\n",
      "Epoch190  loss:1.631673514842987  accuracy:68.71698568793693\n",
      "Epoch191  loss:1.6316381216049196  accuracy:68.52637959086185\n",
      "Epoch192  loss:1.6229171097278594  accuracy:68.63531079583005\n",
      "Epoch193  loss:1.622982156276703  accuracy:68.97282997612508\n",
      "Epoch194  loss:1.624132829904556  accuracy:68.54830391271068\n",
      "Epoch195  loss:1.614802628755569  accuracy:68.91117204597056\n",
      "Epoch196  loss:1.608645039796829  accuracy:68.42932449248639\n",
      "Epoch197  loss:1.6120322585105895  accuracy:68.46448842300563\n",
      "Epoch198  loss:1.6056287765502932  accuracy:68.5752283046628\n",
      "Epoch199  loss:1.6153670787811283  accuracy:68.02068749915885\n",
      "Epoch200  loss:1.6075740694999694  accuracy:69.0004108299802\n",
      "Epoch201  loss:1.6027185022830963  accuracy:68.84339307762451\n",
      "Epoch202  loss:1.5978836536407472  accuracy:68.69937273210499\n",
      "Epoch203  loss:1.599698704481125  accuracy:68.48974073252315\n",
      "Epoch204  loss:1.5960417985916138  accuracy:68.81729631179326\n",
      "Epoch205  loss:1.5926586627960204  accuracy:68.84738988773387\n",
      "Epoch206  loss:1.5899841725826265  accuracy:69.1277994309904\n",
      "Epoch207  loss:1.5878250896930697  accuracy:68.94827974152844\n",
      "Epoch208  loss:1.5801160216331482  accuracy:69.34576592865548\n",
      "Epoch209  loss:1.5762076556682583  accuracy:68.85040056059455\n",
      "Epoch210  loss:1.5762104570865632  accuracy:68.97481247421669\n",
      "Epoch211  loss:1.5783532738685608  accuracy:68.5311355048655\n",
      "Epoch212  loss:1.5718275129795074  accuracy:68.88268261825704\n",
      "Epoch213  loss:1.5744154810905455  accuracy:68.47428878524502\n",
      "Epoch214  loss:1.571101903915405  accuracy:68.46704974150671\n",
      "Epoch215  loss:1.5713646829128258  accuracy:68.69727613453578\n",
      "Epoch216  loss:1.5683511555194858  accuracy:68.80878406964797\n",
      "Epoch217  loss:1.5658098518848418  accuracy:68.88854099295148\n",
      "Epoch218  loss:1.5630415797233577  accuracy:68.94939523532744\n",
      "Epoch219  loss:1.5589426338672638  accuracy:68.80685158763248\n",
      "Epoch220  loss:1.557494932413101  accuracy:68.3967579289034\n",
      "Epoch221  loss:1.553834617137909  accuracy:68.63368336802986\n",
      "Epoch222  loss:1.5499680817127228  accuracy:69.20106647971336\n",
      "Epoch223  loss:1.549968373775482  accuracy:68.7818227331489\n",
      "Epoch224  loss:1.5496363580226897  accuracy:68.50724557117485\n",
      "Epoch225  loss:1.549736422300339  accuracy:68.68665118005211\n",
      "Epoch226  loss:1.5443758189678196  accuracy:68.53828851893218\n",
      "Epoch227  loss:1.54117830991745  accuracy:68.74667498538503\n",
      "Epoch228  loss:1.5477149963378904  accuracy:69.08707482113377\n",
      "Epoch229  loss:1.5353197574615474  accuracy:68.60662779326051\n",
      "Epoch230  loss:1.5362668395042418  accuracy:69.17223040568834\n",
      "Epoch231  loss:1.5332358956336973  accuracy:68.89731491686751\n",
      "Epoch232  loss:1.5275403976440431  accuracy:68.81906340449277\n",
      "Epoch233  loss:1.530833685398102  accuracy:68.96069434067758\n",
      "Epoch234  loss:1.527244412899017  accuracy:68.9694165617566\n",
      "Epoch235  loss:1.5246517121791838  accuracy:68.88545161920555\n",
      "Epoch236  loss:1.517865824699402  accuracy:69.05893318705581\n",
      "Epoch237  loss:1.5226059377193455  accuracy:69.05849624778068\n",
      "Epoch238  loss:1.5217106223106387  accuracy:68.90102917998956\n",
      "Epoch239  loss:1.5202384233474728  accuracy:68.88649945831959\n",
      "Epoch240  loss:1.5222543716430663  accuracy:68.88877323042823\n",
      "Epoch241  loss:1.515409624576569  accuracy:69.17003653729968\n",
      "Epoch242  loss:1.5117260575294493  accuracy:68.8154726341706\n",
      "Epoch243  loss:1.5114135146141048  accuracy:69.05928591358656\n",
      "Epoch244  loss:1.5098648428916928  accuracy:68.7717601592187\n",
      "Epoch245  loss:1.5098992705345156  accuracy:69.25016501219616\n",
      "Epoch246  loss:1.5086568355560306  accuracy:68.98768946472126\n",
      "Epoch247  loss:1.4998580932617187  accuracy:69.30937960712504\n",
      "Epoch248  loss:1.496071547269821  accuracy:69.02745831707163\n",
      "Epoch249  loss:1.4995854496955872  accuracy:68.93139978003376\n",
      "Epoch250  loss:1.5000199437141417  accuracy:69.13744370346475\n",
      "Epoch251  loss:1.4872646987438203  accuracy:69.16821220308472\n",
      "Epoch252  loss:1.4923692643642428  accuracy:69.23538875943571\n",
      "Epoch253  loss:1.4866997241973878  accuracy:69.4122300178162\n",
      "Epoch254  loss:1.486244511604309  accuracy:69.27463642935953\n",
      "Epoch255  loss:1.4908707737922668  accuracy:68.99968161499737\n",
      "Epoch256  loss:1.4853271007537843  accuracy:68.87272916575589\n",
      "Epoch257  loss:1.4814925849437717  accuracy:68.78300740432066\n",
      "Epoch258  loss:1.4820016384124755  accuracy:69.02739899813685\n",
      "Epoch259  loss:1.4830644369125368  accuracy:69.15214943308254\n",
      "Epoch260  loss:1.486777055263519  accuracy:68.65901264146123\n",
      "Epoch261  loss:1.476499193906784  accuracy:69.02976004337502\n",
      "Epoch262  loss:1.478808408975601  accuracy:69.72593645150792\n",
      "Epoch263  loss:1.4802268624305723  accuracy:69.01891186713665\n",
      "Epoch264  loss:1.4730858087539673  accuracy:69.17853643624818\n",
      "Epoch265  loss:1.4716590285301208  accuracy:68.90525992862196\n",
      "Epoch266  loss:1.4664153754711151  accuracy:68.88037627214825\n",
      "Epoch267  loss:1.4723202228546142  accuracy:68.80496902238758\n",
      "Epoch268  loss:1.4661207139492034  accuracy:68.8741484380595\n",
      "Epoch269  loss:1.4665493965148924  accuracy:69.14048779598114\n",
      "Epoch270  loss:1.46121501326561  accuracy:69.13281114468947\n",
      "Epoch271  loss:1.4642510354518892  accuracy:68.82451096534574\n",
      "Epoch272  loss:1.452051204442978  accuracy:69.09414151109978\n",
      "Epoch273  loss:1.456089079380035  accuracy:68.97137999845329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch274  loss:1.4566025495529176  accuracy:68.57435992560202\n",
      "Epoch275  loss:1.450417548418045  accuracy:69.02614612235143\n",
      "Epoch276  loss:1.4504843652248385  accuracy:69.0970415860299\n",
      "Epoch277  loss:1.454456549882889  accuracy:69.19153703097237\n",
      "Epoch278  loss:1.4505554795265196  accuracy:68.87679685750277\n",
      "Epoch279  loss:1.450756335258484  accuracy:69.13075213754699\n",
      "Epoch280  loss:1.4490863084793093  accuracy:68.8112999201981\n",
      "Epoch281  loss:1.4494518876075744  accuracy:69.108165648159\n",
      "Epoch282  loss:1.442663687467575  accuracy:69.3869303767037\n",
      "Epoch283  loss:1.4473471522331236  accuracy:69.3304622826195\n",
      "Epoch284  loss:1.442738312482834  accuracy:68.9690809200761\n",
      "Epoch285  loss:1.4480658292770388  accuracy:68.83655509956907\n",
      "Epoch286  loss:1.4396369576454164  accuracy:69.16026875066125\n",
      "Epoch287  loss:1.4406729996204375  accuracy:68.9136097647907\n",
      "Epoch288  loss:1.4391722142696382  accuracy:69.47650108236587\n",
      "Epoch289  loss:1.431801426410675  accuracy:69.39247418399385\n",
      "Epoch290  loss:1.4340625107288363  accuracy:69.07877214238711\n",
      "Epoch291  loss:1.4336089611053466  accuracy:69.23276115038873\n",
      "Epoch292  loss:1.4269269049167632  accuracy:68.80913450726825\n",
      "Epoch293  loss:1.4275631785392762  accuracy:69.09081525332198\n",
      "Epoch294  loss:1.4283880710601806  accuracy:69.22762039935664\n",
      "Epoch295  loss:1.4195988714694978  accuracy:69.29345943151635\n",
      "Epoch296  loss:1.4222897231578826  accuracy:69.32757745250595\n",
      "Epoch297  loss:1.4177993416786192  accuracy:70.01070206369525\n",
      "Epoch298  loss:1.4224598884582518  accuracy:69.346477910923\n",
      "Epoch299  loss:1.4256502509117128  accuracy:69.19581081967262\n",
      "Epoch300  loss:1.4224093109369278  accuracy:69.37479027576336\n"
     ]
    }
   ],
   "source": [
    "server = Server()\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset)\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "acc_valid = []\n",
    "loss_valid = []\n",
    "\n",
    "early_stopping = Early_Stopping(args.partience)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(args.global_epochs):\n",
    "  sample_worker = server.sample_worker(workers)\n",
    "  server.send_model(sample_worker)\n",
    "\n",
    "  acc_train_avg = 0.0\n",
    "  loss_train_avg = 0.0\n",
    "  acc_valid_avg = 0.0\n",
    "  loss_valid_avg = 0.0\n",
    "  for worker in sample_worker:\n",
    "    acc_train_tmp,loss_train_tmp = worker.local_train()\n",
    "    acc_valid_tmp,loss_valid_tmp = worker.validate()\n",
    "    acc_train_avg += acc_train_tmp/len(sample_worker)\n",
    "    loss_train_avg += loss_train_tmp/len(sample_worker)\n",
    "    acc_valid_avg += acc_valid_tmp/len(sample_worker)\n",
    "    loss_valid_avg += loss_valid_tmp/len(sample_worker)\n",
    "  server.aggregate_model(sample_worker)\n",
    "  '''\n",
    "  server.model.to(args.device)\n",
    "  for worker in workers:\n",
    "    acc_valid_tmp,loss_valid_tmp = test(server.model,args.criterion,worker.valloader)\n",
    "    acc_valid_avg += acc_valid_tmp/len(workers)\n",
    "    loss_valid_avg += loss_valid_tmp/len(workers)\n",
    "  server.model.to('cpu')\n",
    "  '''\n",
    "  print('Epoch{}  loss:{}  accuracy:{}'.format(epoch+1,loss_valid_avg,acc_valid_avg))\n",
    "  acc_train.append(acc_train_avg)\n",
    "  loss_train.append(loss_train_avg)\n",
    "  acc_valid.append(acc_valid_avg)\n",
    "  loss_valid.append(loss_valid_avg)\n",
    "\n",
    "  if early_stopping.validate(loss_valid_avg):\n",
    "    print('Early Stop')\n",
    "    break\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 accuracy:64.70588235294117  loss:1.7713159322738647\n",
      "Worker2 accuracy:91.42857142857143  loss:0.4578380584716797\n",
      "Worker3 accuracy:82.5  loss:0.9264413714408875\n",
      "Worker4 accuracy:55.55555555555556  loss:2.0811409950256348\n",
      "Worker5 accuracy:91.17647058823529  loss:0.7119754552841187\n",
      "Worker6 accuracy:66.66666666666667  loss:1.1937726736068726\n",
      "Worker7 accuracy:71.05263157894737  loss:1.3121081590652466\n",
      "Worker8 accuracy:71.875  loss:1.1475987434387207\n",
      "Worker9 accuracy:57.89473684210526  loss:1.9260201454162598\n",
      "Worker10 accuracy:87.8048780487805  loss:0.5791337490081787\n",
      "Worker11 accuracy:55.55555555555556  loss:2.4596071243286133\n",
      "Worker12 accuracy:73.6842105263158  loss:1.348358392715454\n",
      "Worker13 accuracy:52.63157894736842  loss:1.7714033126831055\n",
      "Worker14 accuracy:50.0  loss:2.265598773956299\n",
      "Worker15 accuracy:68.42105263157895  loss:1.4856492280960083\n",
      "Worker16 accuracy:64.70588235294117  loss:1.8881632089614868\n",
      "Worker17 accuracy:69.6969696969697  loss:1.0452823638916016\n",
      "Worker18 accuracy:80.64516129032258  loss:0.9797120094299316\n",
      "Worker19 accuracy:82.3529411764706  loss:0.9569657444953918\n",
      "Worker20 accuracy:55.55555555555556  loss:1.8580785989761353\n",
      "Test  loss:1.4083082020282744  accuracy:69.6954650397441\n"
     ]
    }
   ],
   "source": [
    "acc_test = []\n",
    "loss_test = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.global_model = copy.deepcopy(server.global_model)\n",
    "  _,_ = worker.local_train()\n",
    "  acc_tmp,loss_tmp = test(worker.local_model,args.criterion_ce,worker.testloader)\n",
    "  acc_test.append(acc_tmp)\n",
    "  loss_test.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "  worker.local_model = worker.local_model.to('cpu')\n",
    "  worker.global_model = worker.global_model.to('cpu')\n",
    "  del worker.global_model\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_test_avg = sum(acc_test)/len(acc_test)\n",
    "loss_test_avg = sum(loss_test)/len(loss_test)\n",
    "print('Test  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
