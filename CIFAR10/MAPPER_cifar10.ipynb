{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    # random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 42\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O0TfzOhU-QlG"
   },
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 40\n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 500\n",
    "    self.local_epochs = 2\n",
    "    self.lr = None\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 500\n",
    "    self.worker_num = 20\n",
    "    self.sample_num = 20\n",
    "    self.unlabeleddata_size = 1000\n",
    "    self.device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    self.alpha_label = 0.5\n",
    "    self.alpha_size = 10\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = []\n",
    "lr_list.append(10**-3.0)\n",
    "lr_list.append(10**-2.5)\n",
    "lr_list.append(10**-2.0)\n",
    "lr_list.append(10**-1.5)\n",
    "lr_list.append(10**-1.0)\n",
    "lr_list.append(10**-0.5)\n",
    "lr_list.append(10**0.0)\n",
    "lr_list.append(10**0.5)\n",
    "\n",
    "args.lr = lr_list[lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r5PuCcqmJNUQ"
   },
   "outputs": [],
   "source": [
    "class LocalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.label[idx]\n",
    "        if self.transform:\n",
    "            out_data = self.transform(out_data)\n",
    "        return out_data, out_label\n",
    "    \n",
    "class DatasetFromSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.subset[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    \n",
    "class GlobalDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,federated_dataset,transform=None):\n",
    "    self.transform = transform\n",
    "    self.data = []\n",
    "    self.label = []\n",
    "    for dataset in federated_dataset:\n",
    "      for (data,label) in dataset:\n",
    "        self.data.append(data)\n",
    "        self.label.append(label)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    out_data = self.data[idx]\n",
    "    out_label = self.label[idx]\n",
    "    if self.transform:\n",
    "        out_data = self.transform(out_data)\n",
    "    return out_data, out_label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "class UnlabeledDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,transform=None):\n",
    "    self.transform = transform\n",
    "    self.data = []\n",
    "    self.target = None\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    out_data = self.data[idx]\n",
    "    out_label = 'unlabeled'\n",
    "    if self.transform:\n",
    "        out_data = self.transform(out_data)\n",
    "    return out_data, out_label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(Centralized=False,unlabeled_data=False):\n",
    "    \n",
    "    transform_train = transforms.Compose([transforms.ToPILImage(),\n",
    "                                    transforms.RandomCrop(32, padding=2),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.491372549, 0.482352941, 0.446666667), (0.247058824, 0.243529412, 0.261568627))])\n",
    "    transform_test = transforms.Compose([transforms.ToPILImage(),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.491372549, 0.482352941, 0.446666667), (0.247058824, 0.243529412, 0.261568627))])\n",
    "\n",
    "    # download train data\n",
    "    all_trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True)\n",
    "    #trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "    # download test data\n",
    "    all_testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True)\n",
    "    #testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "    \n",
    "    ## get unlabeled dataset\n",
    "    if unlabeled_data:\n",
    "        unlabeled_dataset = UnlabeledDataset(transform_test)\n",
    "        idx = sorted(random.sample(range(len(all_trainset)),args.unlabeleddata_size))\n",
    "        unlabeled_dataset.data = np.array([all_trainset.data[i]  for i in idx])\n",
    "        all_trainset.data = np.delete(all_trainset.data,idx,0)\n",
    "        all_trainset.targets = np.delete(all_trainset.targets,idx,0)\n",
    "    all_train_data = np.array(all_trainset.data)\n",
    "    all_train_label = np.array(all_trainset.targets)\n",
    "    all_test_data = np.array(all_testset.data)\n",
    "    all_test_label = np.array(all_testset.targets)\n",
    "    print('Train:{} Test:{}'.format(len(all_train_data),len(all_test_data)))\n",
    "\n",
    "\n",
    "    ## Data size heterogeneity\n",
    "    data_proportions = np.random.dirichlet(np.repeat(args.alpha_size, args.worker_num))\n",
    "    train_data_proportions = np.array([0 for _ in range(args.worker_num)])\n",
    "    test_data_proportions = np.array([0 for _ in range(args.worker_num)])\n",
    "    for i in range(len(data_proportions)):\n",
    "        if i==(len(data_proportions)-1):\n",
    "            train_data_proportions = train_data_proportions.astype('int64')\n",
    "            test_data_proportions = test_data_proportions.astype('int64')\n",
    "            train_data_proportions[-1] = len(all_train_data) - np.sum(train_data_proportions[:-1])\n",
    "            test_data_proportions[-1] = len(all_test_data) - np.sum(test_data_proportions[:-1])\n",
    "        else:\n",
    "            train_data_proportions[i] = (data_proportions[i] * len(all_train_data))\n",
    "            test_data_proportions[i] = (data_proportions[i] * len(all_test_data))\n",
    "    min_size = 0\n",
    "    K = 10\n",
    "\n",
    "    '''\n",
    "    label_list = np.arange(10)\n",
    "    np.random.shuffle(label_list)\n",
    "    '''\n",
    "    label_list = list(range(K))\n",
    "\n",
    "\n",
    "    ## Data distribution heterogeneity\n",
    "    while min_size<10:\n",
    "        idx_train_batch = [[] for _ in range(args.worker_num)]\n",
    "        idx_test_batch = [[] for _ in range(args.worker_num)]\n",
    "        for k in label_list:\n",
    "            proportions_train = np.random.dirichlet(np.repeat(args.alpha_label, args.worker_num))\n",
    "            proportions_test = copy.deepcopy(proportions_train)\n",
    "            idx_k_train = np.where(all_train_label == k)[0]\n",
    "            idx_k_test = np.where(all_test_label == k)[0]\n",
    "            np.random.shuffle(idx_k_train)\n",
    "            np.random.shuffle(idx_k_test)\n",
    "            ## Balance (train)\n",
    "            proportions_train = np.array([p*(len(idx_j)<train_data_proportions[i]) for i,(p,idx_j) in enumerate(zip(proportions_train,idx_train_batch))])\n",
    "            proportions_train = proportions_train/proportions_train.sum()\n",
    "            proportions_train = (np.cumsum(proportions_train)*len(idx_k_train)).astype(int)[:-1]\n",
    "            idx_train_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_train_batch,np.split(idx_k_train,proportions_train))]\n",
    "\n",
    "            ## Balance (test)\n",
    "            proportions_test = np.array([p*(len(idx_j)<test_data_proportions[i]) for i,(p,idx_j) in enumerate(zip(proportions_test,idx_test_batch))])\n",
    "            proportions_test = proportions_test/proportions_test.sum()\n",
    "            proportions_test = (np.cumsum(proportions_test)*len(idx_k_test)).astype(int)[:-1]\n",
    "            idx_test_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_test_batch,np.split(idx_k_test,proportions_test))]\n",
    "\n",
    "            min_size = min([len(idx_j) for idx_j in idx_train_batch])\n",
    "\n",
    "    federated_trainset = []\n",
    "    federated_testset = []\n",
    "    for i in range(args.worker_num):\n",
    "        ## create trainset\n",
    "        data = [all_train_data[idx] for idx in idx_train_batch[i]]\n",
    "        label = [all_train_label[idx] for idx in idx_train_batch[i]]\n",
    "        federated_trainset.append(LocalDataset())\n",
    "        federated_trainset[-1].data = data\n",
    "        federated_trainset[-1].label = label\n",
    "\n",
    "        ## create testset\n",
    "        data = [all_test_data[idx] for idx in idx_test_batch[i]]\n",
    "        label = [all_test_label[idx] for idx in idx_test_batch[i]]\n",
    "        federated_testset.append(LocalDataset())\n",
    "        federated_testset[-1].data = data\n",
    "        federated_testset[-1].label = label\n",
    "\n",
    "        \n",
    "    ## split trainset\n",
    "    federated_valset = [None]*args.worker_num\n",
    "    for i in range(args.worker_num):\n",
    "        n_samples = len(federated_trainset[i])\n",
    "        if n_samples==1:\n",
    "            train_subset = federated_trainset[i]\n",
    "            val_subset = copy.deepcopy(federated_trainset[i])\n",
    "        else:\n",
    "            train_size = int(len(federated_trainset[i]) * 0.8) \n",
    "            val_size = n_samples - train_size \n",
    "            train_subset,val_subset = torch.utils.data.random_split(federated_trainset[i], [train_size, val_size])\n",
    "\n",
    "        federated_trainset[i] = DatasetFromSubset(train_subset)\n",
    "        federated_valset[i] = DatasetFromSubset(val_subset)\n",
    "\n",
    "    ## show data distribution\n",
    "    H = 4\n",
    "    W = 5\n",
    "    fig, axs = plt.subplots(H, W, figsize=(20, 5))\n",
    "    x = np.arange(1,11)\n",
    "    for i, (trainset,valset,testset) in enumerate(zip(federated_trainset,federated_valset,federated_testset)):\n",
    "        bottom = [0]*10\n",
    "        count = [0]*10\n",
    "        for _,label in trainset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        for j in range(len(count)):\n",
    "            bottom[j]+=count[j]\n",
    "        count = [0]*10\n",
    "        for _,label in valset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        for j in range(len(count)):\n",
    "            bottom[j]+=count[j]\n",
    "        count = [0]*10\n",
    "        for _,label in testset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        #axs[int(i/W), i%W].title(\"worker{}\".format(i+1), fontsize=12, color = \"green\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ## get global dataset\n",
    "    if Centralized:\n",
    "        global_trainset = GlobalDataset(federated_trainset)\n",
    "        global_valset = GlobalDataset(federated_valset)\n",
    "        global_testset =  GlobalDataset(federated_testset)\n",
    "        \n",
    "        #show_cifer(global_trainset.data,global_testset.label, cifar10_labels)\n",
    "\n",
    "        global_trainset.transform = transform_train\n",
    "        global_valset.transform = transform_test\n",
    "        global_testset.transform = transform_test\n",
    "\n",
    "        global_trainloader = torch.utils.data.DataLoader(global_trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "        global_valloader = torch.utils.data.DataLoader(global_valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "        global_testloader = torch.utils.data.DataLoader(global_testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "\n",
    "    ## set transform\n",
    "    for i in range(args.worker_num):\n",
    "        federated_trainset[i].transform = transform_train\n",
    "        federated_valset[i].transform = transform_test\n",
    "        federated_testset[i].transform = transform_test\n",
    "    \n",
    "    if Centralized and unlabeled_data:\n",
    "        return federated_trainset,federated_valset,federated_testset,global_trainloader,global_valloader,global_testloader,unlabeled_dataset\n",
    "    if Centralized:\n",
    "        return federated_trainset,federated_valset,federated_testset,global_trainloader,global_valloader,global_testloader\n",
    "    elif unlabeled_data:\n",
    "        return federated_trainset,federated_valset,federated_testset,unlabeled_dataset\n",
    "    else:\n",
    "        return federated_trainset,federated_valset,federated_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train:49000 Test:10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "federated_trainset,federated_valset,federated_testset,unlabeled_dataset = get_dataset(unlabeled_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39191, 9809, 10000]\n"
     ]
    }
   ],
   "source": [
    "total = [0,0,0]\n",
    "for i in range(args.worker_num):\n",
    "    total[0]+=len(federated_trainset[i])\n",
    "    total[1]+=len(federated_valset[i])\n",
    "    total[2]+=len(federated_testset[i])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZU3vAAb9-6SD"
   },
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG model \n",
    "    '''\n",
    "    def __init__(self, features, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            #print(\"in_channels: {}, v: {}\".format(in_channels, v))\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
    "          512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGGConvBlocks(nn.Module):\n",
    "    '''\n",
    "    VGG containers that only contains the conv layers \n",
    "    '''\n",
    "    def __init__(self, features, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class VGGContainer(nn.Module):\n",
    "    '''\n",
    "    VGG model \n",
    "    '''\n",
    "    def __init__(self, features, input_dim, hidden_dims, num_classes=10):\n",
    "        super(VGGContainer, self).__init__()\n",
    "        self.features = features\n",
    "        # note: we hard coded here a bit by assuming we only have two hidden layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dims[1], num_classes),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def matched_vgg11(matched_shapes):\n",
    "    # [(67, 27), (67,), (132, 603), (132,), (260, 1188), (260,), (261, 2340), (261,), (516, 2349), (516,), (517, 4644), (517,), \n",
    "    # (516, 4653), (516,), (516, 4644), (516,), (516, 515), (515,), (515, 515), (515,), (515, 10), (10,)]\n",
    "    processed_matched_shape = [matched_shapes[0][0], \n",
    "                                'M', \n",
    "                                matched_shapes[2][0], \n",
    "                                'M', \n",
    "                                matched_shapes[4][0], \n",
    "                                matched_shapes[6][0], \n",
    "                                'M', \n",
    "                                matched_shapes[8][0], \n",
    "                                matched_shapes[10][0], \n",
    "                                'M', \n",
    "                                matched_shapes[12][0], \n",
    "                                matched_shapes[14][0], \n",
    "                                'M']\n",
    "    return VGGContainer(make_layers(processed_matched_shape), input_dim=matched_shapes[16][0], \n",
    "            hidden_dims=[matched_shapes[16][1], matched_shapes[18][1]], num_classes=10)\n",
    "\n",
    "\n",
    "def vgg11():\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
    "    return VGG(make_layers(cfg['A']))\n",
    "\n",
    "\n",
    "def vgg11_bn(num_classes=10):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['A'], batch_norm=True), num_classes=num_classes)\n",
    "\n",
    "\n",
    "def vgg13():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n",
    "    return VGG(make_layers(cfg['B']))\n",
    "\n",
    "\n",
    "def vgg13_bn():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg16():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n",
    "    return VGG(make_layers(cfg['D']))\n",
    "\n",
    "\n",
    "def vgg16_bn():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n",
    "    return VGG(make_layers(cfg['E']))\n",
    "\n",
    "\n",
    "def vgg19_bn():\n",
    "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['E'], batch_norm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self):\n",
    "    self.global_model = vgg13()\n",
    " \n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(federated_trainset[i],federated_valset[i],federated_testset[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "\n",
    "  def send_model(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "\n",
    "    for worker in workers:\n",
    "      worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "      worker.global_model = copy.deepcopy(self.global_model)\n",
    "\n",
    "\n",
    "  def aggregate_model(self,workers):   \n",
    "    new_params = OrderedDict()\n",
    "    for i,worker in enumerate(workers):\n",
    "      worker_state = worker.global_model.state_dict()\n",
    "      for key in worker_state.keys():\n",
    "        if i==0:\n",
    "          new_params[key] = worker_state[key]*worker.aggregation_weight\n",
    "        else:\n",
    "          new_params[key] += worker_state[key]*worker.aggregation_weight\n",
    "      del worker.global_model \n",
    "    self.global_model.load_state_dict(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,trainset,valset,testset):\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "    if len(trainset)<=2 :\n",
    "      trainset = [trainset,copy.deepcopy(trainset),copy.deepcopy(trainset)]\n",
    "    else:\n",
    "      split_len = [(len(trainset) + i) // 3 for i in range(3)]\n",
    "      trainset = torch.utils.data.random_split(trainset,split_len)\n",
    "    self.trainloader_local = torch.utils.data.DataLoader(trainset[0],batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.trainloader_lambda = torch.utils.data.DataLoader(trainset[1],batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.trainloader_global = torch.utils.data.DataLoader(trainset[2],batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.global_model = None\n",
    "    self.local_model = vgg13()\n",
    "    self.mix_model = None\n",
    "    self.lmd = None\n",
    "    self.aggregation_weight = None\n",
    "\n",
    "\n",
    "  def local_train(self):\n",
    "    loss = float('inf')\n",
    "    self.mix_model = vgg13()\n",
    "    model_tmp = vgg13()\n",
    "    for lmd in np.arange(0.0,1.1,0.1):\n",
    "      new_params = OrderedDict()\n",
    "      global_state = self.global_model.state_dict()\n",
    "      local_state = self.local_model.state_dict()\n",
    "      for key in global_state.keys():\n",
    "        new_params[key] = lmd*local_state[key] + (1.0-lmd)*global_state[key]     \n",
    "      model_tmp.load_state_dict(new_params)\n",
    "      model_tmp = model_tmp.to(args.device)\n",
    "      if lmd!=0.0:\n",
    "        _,_ = train(model_tmp,args.criterion,self.trainloader_local,args.local_epochs)\n",
    "\n",
    "      _,tmp = test(model_tmp,args.criterion,self.trainloader_lambda)\n",
    "      if tmp<loss:\n",
    "        loss = tmp\n",
    "        self.lmd = lmd\n",
    "        self.mix_model = copy.deepcopy(model_tmp)\n",
    "\n",
    "    if self.lmd!=0.0:\n",
    "      self.mix_model = self.mix_model.to('cpu')\n",
    "      new_params = OrderedDict()\n",
    "      mix_state = self.mix_model.state_dict()\n",
    "      global_state = self.global_model.state_dict()\n",
    "      for key in global_state.keys():\n",
    "        new_params[key] = (mix_state[key] - (1.0-self.lmd)*global_state[key])/self.lmd    \n",
    "      self.local_model.load_state_dict(new_params)\n",
    "      self.mix_model = self.mix_model.to(args.device)\n",
    "    \n",
    "    model_tmp = model_tmp.to('cpu')\n",
    "    del model_tmp\n",
    "\n",
    "\n",
    "  def global_train(self):\n",
    "    model_tmp = copy.deepcopy(self.mix_model)\n",
    "    model_tmp = model_tmp.to('cpu')\n",
    "    acc_train,loss_train = train(self.mix_model,args.criterion,self.trainloader_global,args.local_epochs)\n",
    "    acc_valid,loss_valid = test(self.mix_model,args.criterion,self.valloader)\n",
    "    self.mix_model = self.mix_model.to('cpu')\n",
    "    new_params = OrderedDict()\n",
    "    new_state = self.mix_model.state_dict()\n",
    "    old_state = model_tmp.state_dict()\n",
    "    global_state = self.global_model.state_dict()\n",
    "    for key in new_state.keys():\n",
    "        new_params[key] = global_state[key]+new_state[key]-old_state[key]  \n",
    "    self.global_model.load_state_dict(new_params)\n",
    "    del self.mix_model\n",
    "    del model_tmp\n",
    "    return acc_train,loss_train,acc_valid,loss_valid\n",
    "\n",
    "\n",
    "  def test_step(self):\n",
    "    self.local_train()\n",
    "    mix_model = vgg13()\n",
    "    new_params = OrderedDict()\n",
    "    global_state = self.global_model.state_dict()\n",
    "    local_state = self.local_model.state_dict()\n",
    "    for key in global_state.keys():\n",
    "      new_params[key] = self.lmd*local_state[key] + (1.0-self.lmd)*global_state[key]     \n",
    "    mix_model.load_state_dict(new_params)\n",
    "    mix_model = mix_model.to(args.device)\n",
    "    acc,loss = test(mix_model,args.criterion,self.testloader)\n",
    "    return acc,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7-GY66gROuEU"
   },
   "outputs": [],
   "source": [
    "def train(model,criterion,trainloader,epochs):\n",
    "  optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for (data,labels) in trainloader:\n",
    "      data,labels = Variable(data),Variable(labels)\n",
    "      data,labels = data.to(args.device),labels.to(args.device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(data)\n",
    "      loss = criterion(outputs,labels)\n",
    "      running_loss += loss.item()\n",
    "      predicted = torch.argmax(outputs,dim=1)\n",
    "      correct += (predicted==labels).sum().item()\n",
    "      count += len(labels)\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "      optimizer.step()\n",
    "\n",
    "  return 100.0*correct/count,running_loss/len(trainloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  count = 0\n",
    "  for (data,labels) in testloader:\n",
    "    data,labels = data.to(args.device),labels.to(args.device)\n",
    "    outputs = model(data)\n",
    "    running_loss += criterion(outputs,labels).item()\n",
    "    predicted = torch.argmax(outputs,dim=1)\n",
    "    correct += (predicted==labels).sum().item()\n",
    "    count += len(labels)\n",
    "\n",
    "  accuracy = 100.0*correct/count\n",
    "  loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "  return accuracy,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WMO7_WSLHeGl"
   },
   "outputs": [],
   "source": [
    "class Early_Stopping():\n",
    "  def __init__(self,partience):\n",
    "    self.step = 0\n",
    "    self.loss = float('inf')\n",
    "    self.partience = partience\n",
    "\n",
    "  def validate(self,loss):\n",
    "    if self.loss<loss:\n",
    "      self.step += 1\n",
    "      if self.step>self.partience:\n",
    "        return True\n",
    "    else:\n",
    "      self.step = 0\n",
    "      self.loss = loss\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "-noG_98IR-nZ",
    "outputId": "78a6ebe2-854a-4f83-dc45-5c4ac35b69e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1  loss:1.9024672597646715  accuracy:43.361738372675624\n",
      "Epoch2  loss:1.725651529431343  accuracy:43.01639389814469\n",
      "Epoch3  loss:1.64758386015892  accuracy:43.253198286586645\n",
      "Epoch4  loss:1.5574744969606398  accuracy:43.294766489151115\n",
      "Epoch5  loss:1.507778772711754  accuracy:43.65411625525491\n",
      "Epoch6  loss:1.492500802874565  accuracy:42.773352788698254\n",
      "Epoch7  loss:1.4849084585905077  accuracy:43.46917181408191\n",
      "Epoch8  loss:1.4792048543691636  accuracy:42.90114351557359\n",
      "Epoch9  loss:1.4662846982479096  accuracy:43.31910945632757\n",
      "Epoch10  loss:1.4622665405273438  accuracy:43.352575875033295\n",
      "Epoch11  loss:1.4498973667621613  accuracy:43.403309668035526\n",
      "Epoch12  loss:1.4478833287954331  accuracy:45.220156327990296\n",
      "Epoch13  loss:1.4387569487094878  accuracy:44.42080670861843\n",
      "Epoch14  loss:1.4409559071063995  accuracy:43.55640289898183\n",
      "Epoch15  loss:1.4299666523933412  accuracy:45.07803777535489\n",
      "Epoch16  loss:1.4285402745008469  accuracy:45.449982359277385\n",
      "Epoch17  loss:1.4205016493797304  accuracy:45.57401815606804\n",
      "Epoch18  loss:1.4255208730697635  accuracy:45.2701837722596\n",
      "Epoch19  loss:1.4146694868803027  accuracy:45.50131347467886\n",
      "Epoch20  loss:1.4070454269647599  accuracy:45.91139690220383\n",
      "Epoch21  loss:1.4095594495534896  accuracy:46.28838975262663\n",
      "Epoch22  loss:1.3935518443584445  accuracy:46.87263954070723\n",
      "Epoch23  loss:1.403050681948662  accuracy:46.718779814703254\n",
      "Epoch24  loss:1.3934425473213194  accuracy:46.983191350961214\n",
      "Epoch25  loss:1.3799084007740021  accuracy:46.86851436443174\n",
      "Epoch26  loss:1.3800721615552902  accuracy:48.16678793208061\n",
      "Epoch27  loss:1.3754786372184753  accuracy:47.09543195388482\n",
      "Epoch28  loss:1.3667970567941667  accuracy:47.955413676466655\n",
      "Epoch29  loss:1.3623202294111252  accuracy:48.969914882807736\n",
      "Epoch30  loss:1.3633280366659162  accuracy:47.18221950841987\n",
      "Epoch31  loss:1.356508573889732  accuracy:48.69439293307755\n",
      "Epoch32  loss:1.3666496455669404  accuracy:48.930823595656776\n",
      "Epoch33  loss:1.3482460647821428  accuracy:50.4510618412416\n",
      "Epoch34  loss:1.3392843991518022  accuracy:51.13086442064653\n",
      "Epoch35  loss:1.341612002253532  accuracy:50.99183445013432\n",
      "Epoch36  loss:1.3350293934345245  accuracy:51.3585250704072\n",
      "Epoch37  loss:1.3381029903888702  accuracy:51.05988349299877\n",
      "Epoch38  loss:1.3247094094753264  accuracy:51.16441396287318\n",
      "Epoch39  loss:1.312485656142235  accuracy:51.385411038605184\n",
      "Epoch40  loss:1.3122509628534318  accuracy:51.88928705578232\n",
      "Epoch41  loss:1.3178239911794662  accuracy:51.78442913818335\n",
      "Epoch42  loss:1.3073796123266221  accuracy:52.302121800114854\n",
      "Epoch43  loss:1.2992103070020673  accuracy:52.06656734247436\n",
      "Epoch44  loss:1.2936887025833133  accuracy:52.36717883523906\n",
      "Epoch45  loss:1.2899836704134942  accuracy:52.66163497281359\n",
      "Epoch46  loss:1.2909168750047681  accuracy:52.56851544659939\n",
      "Epoch47  loss:1.2795073628425597  accuracy:53.081092966533795\n",
      "Epoch48  loss:1.28630114197731  accuracy:52.13665124269077\n",
      "Epoch49  loss:1.264367589354515  accuracy:52.94244921790366\n",
      "Epoch50  loss:1.2755203694105146  accuracy:53.37350293817237\n",
      "Epoch51  loss:1.2611200496554376  accuracy:53.3791212607146\n",
      "Epoch52  loss:1.2731303572654724  accuracy:53.14537969571506\n",
      "Epoch53  loss:1.265927317738533  accuracy:52.88639226630244\n",
      "Epoch54  loss:1.2630998522043233  accuracy:52.75791601107365\n",
      "Epoch55  loss:1.2438983529806136  accuracy:53.9795471983621\n",
      "Epoch56  loss:1.234702548384666  accuracy:54.52985361411314\n",
      "Epoch57  loss:1.2482559800148012  accuracy:52.388486606137164\n",
      "Epoch58  loss:1.2430944606661793  accuracy:52.4422463814903\n",
      "Epoch59  loss:1.2292862370610238  accuracy:54.492179389797\n",
      "Epoch60  loss:1.2207947731018065  accuracy:54.75667083906289\n",
      "Epoch61  loss:1.220307996869087  accuracy:54.75783532228404\n",
      "Epoch62  loss:1.2197605296969416  accuracy:55.28574171722973\n",
      "Epoch63  loss:1.217879483103752  accuracy:55.04633737687565\n",
      "Epoch64  loss:1.1958478562533856  accuracy:55.49284983371863\n",
      "Epoch65  loss:1.2052258342504503  accuracy:54.690041884675175\n",
      "Epoch66  loss:1.1982957676053045  accuracy:55.57996080642378\n",
      "Epoch67  loss:1.1851436145603658  accuracy:56.071564672861506\n",
      "Epoch68  loss:1.1950340956449508  accuracy:55.60769794536505\n",
      "Epoch69  loss:1.189458705484867  accuracy:56.42779104688002\n",
      "Epoch70  loss:1.190133298933506  accuracy:55.677789887962014\n",
      "Epoch71  loss:1.17692973613739  accuracy:56.732773383186945\n",
      "Epoch72  loss:1.2102881357073785  accuracy:55.56991825446276\n",
      "Epoch73  loss:1.1877947404980658  accuracy:56.273597538973185\n",
      "Epoch74  loss:1.1866845309734346  accuracy:56.2688542304187\n",
      "Epoch75  loss:1.1718080624938014  accuracy:56.80300220167793\n",
      "Epoch76  loss:1.1724954426288605  accuracy:56.022945225097736\n",
      "Epoch77  loss:1.166967660188675  accuracy:55.99268904118939\n",
      "Epoch78  loss:1.185773853957653  accuracy:56.66708423154418\n",
      "Epoch79  loss:1.166349609196186  accuracy:56.61353850123798\n",
      "Epoch80  loss:1.1610929280519486  accuracy:56.88459142734933\n",
      "Epoch81  loss:1.1641344115138053  accuracy:57.1159273595478\n",
      "Epoch82  loss:1.1738517045974737  accuracy:56.8781999744955\n",
      "Epoch83  loss:1.1588627398014069  accuracy:57.647917230811444\n",
      "Epoch84  loss:1.1545062385499478  accuracy:58.19573626301563\n",
      "Epoch85  loss:1.1503093756735325  accuracy:57.440991892882245\n",
      "Epoch86  loss:1.139158183336258  accuracy:58.5429143846888\n",
      "Epoch87  loss:1.1637414067983627  accuracy:58.682292412682806\n",
      "Epoch88  loss:1.13837640658021  accuracy:58.64377255849375\n",
      "Epoch89  loss:1.1383861154317854  accuracy:58.83690982477717\n",
      "Epoch90  loss:1.137645711004734  accuracy:59.011156706625314\n",
      "Epoch91  loss:1.139328433573246  accuracy:58.55734904232766\n",
      "Epoch92  loss:1.1362593449652194  accuracy:58.77168951114059\n",
      "Epoch93  loss:1.133925861865282  accuracy:58.702151077756476\n",
      "Epoch94  loss:1.1261757425963876  accuracy:59.00601302170223\n",
      "Epoch95  loss:1.1357318907976148  accuracy:58.94546583377676\n",
      "Epoch96  loss:1.1277963504195214  accuracy:59.37761677570393\n",
      "Epoch97  loss:1.1464778155088424  accuracy:58.06126449853098\n",
      "Epoch98  loss:1.123211582005024  accuracy:59.0446862867335\n",
      "Epoch99  loss:1.1237321771681308  accuracy:59.09650879382551\n",
      "Epoch100  loss:1.1321534246206284  accuracy:59.679975304290814\n",
      "Epoch101  loss:1.119961403310299  accuracy:59.56545777270529\n",
      "Epoch102  loss:1.1336403712630272  accuracy:58.926650083904754\n",
      "Epoch103  loss:1.1219831123948096  accuracy:59.57494173831448\n",
      "Epoch104  loss:1.1386650577187536  accuracy:58.637278894854724\n",
      "Epoch105  loss:1.1184716947376727  accuracy:59.674442094802835\n",
      "Epoch106  loss:1.1130598969757555  accuracy:59.599853737393175\n",
      "Epoch107  loss:1.1143810503184794  accuracy:59.49801576471481\n",
      "Epoch108  loss:1.1092408366501332  accuracy:60.127758862667484\n",
      "Epoch109  loss:1.1221108317375184  accuracy:59.63737264865379\n",
      "Epoch110  loss:1.1159731298685074  accuracy:59.988293772392616\n",
      "Epoch111  loss:1.1053688742220402  accuracy:60.28535678652378\n",
      "Epoch112  loss:1.1061976507306102  accuracy:60.1609508464087\n",
      "Epoch113  loss:1.1037985950708389  accuracy:60.01562064103814\n",
      "Epoch114  loss:1.1035956412553787  accuracy:60.595554486751745\n",
      "Epoch115  loss:1.112635338306427  accuracy:60.145927690587065\n",
      "Epoch116  loss:1.1036061167716982  accuracy:60.26739137779988\n",
      "Epoch117  loss:1.0946691595017908  accuracy:60.62010544905779\n",
      "Epoch118  loss:1.0886699117720127  accuracy:60.721760542552204\n",
      "Epoch119  loss:1.0875713318586349  accuracy:60.328699076661465\n",
      "Epoch120  loss:1.1032637640833853  accuracy:60.767532233939505\n",
      "Epoch121  loss:1.0963305324316026  accuracy:60.66684164090438\n",
      "Epoch122  loss:1.0892953142523765  accuracy:61.02515942924388\n",
      "Epoch123  loss:1.09583720266819  accuracy:60.510650286786976\n",
      "Epoch124  loss:1.0850243121385579  accuracy:61.12414711198023\n",
      "Epoch125  loss:1.1035332709550856  accuracy:61.0191439260763\n",
      "Epoch126  loss:1.1109371080994606  accuracy:60.695090044731565\n",
      "Epoch127  loss:1.0779183439910414  accuracy:60.77021804110074\n",
      "Epoch128  loss:1.0856129139661788  accuracy:60.32369841797613\n",
      "Epoch129  loss:1.096006715297699  accuracy:60.95006065039183\n",
      "Epoch130  loss:1.0842047289013865  accuracy:61.0277455743772\n",
      "Epoch131  loss:1.0806964568793775  accuracy:60.83257295652487\n",
      "Epoch132  loss:1.0870889395475387  accuracy:61.15187757275893\n",
      "Epoch133  loss:1.0797089233994486  accuracy:61.451709310353145\n",
      "Epoch134  loss:1.083376970887184  accuracy:61.180007939069746\n",
      "Epoch135  loss:1.085154722630978  accuracy:61.14271360682025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch136  loss:1.0886275619268415  accuracy:61.28404070569141\n",
      "Epoch137  loss:1.0785778790712357  accuracy:61.681431350288044\n",
      "Epoch138  loss:1.0711193919181825  accuracy:61.67065072086708\n",
      "Epoch139  loss:1.0876996099948881  accuracy:61.226326696494425\n",
      "Epoch140  loss:1.0849184438586235  accuracy:61.24107883707998\n",
      "Epoch141  loss:1.0773501247167585  accuracy:61.341004609491314\n",
      "Epoch142  loss:1.0674826726317408  accuracy:61.255879250142584\n",
      "Epoch143  loss:1.0855776846408844  accuracy:60.814320510786025\n",
      "Epoch144  loss:1.0826880410313606  accuracy:61.94873667970858\n",
      "Epoch145  loss:1.079545506834984  accuracy:61.63133109330131\n",
      "Epoch146  loss:1.0713707819581033  accuracy:62.020174047533246\n",
      "Epoch147  loss:1.0798388808965682  accuracy:61.72652297003235\n",
      "Epoch148  loss:1.0772430270910263  accuracy:61.4970610454445\n",
      "Epoch149  loss:1.0677188381552696  accuracy:62.227432589953054\n",
      "Epoch150  loss:1.0659338235855103  accuracy:61.787434478840005\n",
      "Epoch151  loss:1.0816661432385442  accuracy:61.59850257028169\n",
      "Epoch152  loss:1.0720720827579497  accuracy:61.70773976704017\n",
      "Epoch153  loss:1.0500955685973168  accuracy:62.805232124808235\n",
      "Epoch154  loss:1.0761174678802492  accuracy:62.32217385205971\n",
      "Epoch155  loss:1.0616258196532726  accuracy:61.662370208123924\n",
      "Epoch156  loss:1.0647764883935453  accuracy:61.66302202293177\n",
      "Epoch157  loss:1.075563882291317  accuracy:61.48093647273843\n",
      "Epoch158  loss:1.079364374279976  accuracy:61.17644223165089\n",
      "Epoch159  loss:1.0645952604711055  accuracy:61.829140238639596\n",
      "Epoch160  loss:1.0647994965314866  accuracy:62.278837365808045\n",
      "Epoch161  loss:1.0591854006052017  accuracy:62.323361046331144\n",
      "Epoch162  loss:1.0783745072782038  accuracy:61.13416649642097\n",
      "Epoch163  loss:1.0590850517153738  accuracy:62.651592164299174\n",
      "Epoch164  loss:1.060333314538002  accuracy:62.3516587787933\n",
      "Epoch165  loss:1.0540146343410015  accuracy:62.86212064093613\n",
      "Epoch166  loss:1.0572504997253416  accuracy:62.91357970871143\n",
      "Epoch167  loss:1.0570125348865986  accuracy:62.60395123406528\n",
      "Epoch168  loss:1.0486388463526963  accuracy:61.99501247314252\n",
      "Epoch169  loss:1.0588974595069887  accuracy:62.64597753571058\n",
      "Epoch170  loss:1.0448431558907032  accuracy:62.77566634343367\n",
      "Epoch171  loss:1.0474660497158765  accuracy:62.29664240894901\n",
      "Epoch172  loss:1.0386458162218333  accuracy:62.835939146267854\n",
      "Epoch173  loss:1.076446856558323  accuracy:62.22660066138609\n",
      "Epoch174  loss:1.0585430696606637  accuracy:63.03751272381171\n",
      "Epoch175  loss:1.0602500185370447  accuracy:61.91973277931884\n",
      "Epoch176  loss:1.0427788130939006  accuracy:62.69935751976274\n",
      "Epoch177  loss:1.07091968357563  accuracy:62.45239165421961\n",
      "Epoch178  loss:1.051365680247545  accuracy:62.881292573950894\n",
      "Epoch179  loss:1.0574067890644074  accuracy:62.64452385726503\n",
      "Epoch180  loss:1.0693053737282752  accuracy:62.139854304750095\n",
      "Epoch181  loss:1.0621354028582572  accuracy:63.16688167205064\n",
      "Epoch182  loss:1.0519594728946686  accuracy:62.42811002417186\n",
      "Epoch183  loss:1.0760547325015066  accuracy:62.079778492296924\n",
      "Epoch184  loss:1.079642254114151  accuracy:62.46579066923785\n",
      "Epoch185  loss:1.054642426967621  accuracy:62.639125524591016\n",
      "Epoch186  loss:1.0552447497844695  accuracy:62.11270785604021\n",
      "Epoch187  loss:1.0493719197809697  accuracy:63.300994812101266\n",
      "Epoch188  loss:1.0451434046030044  accuracy:62.97768963356566\n",
      "Epoch189  loss:1.0458171769976616  accuracy:62.91591517576657\n",
      "Epoch190  loss:1.03610305711627  accuracy:63.611489470135396\n",
      "Epoch191  loss:1.052681663632393  accuracy:62.908178648024425\n",
      "Epoch192  loss:1.0579310819506644  accuracy:62.74826005931206\n",
      "Epoch193  loss:1.0434153258800507  accuracy:62.73501835982991\n",
      "Epoch194  loss:1.04777876585722  accuracy:62.20636853566801\n",
      "Epoch195  loss:1.0765775680541991  accuracy:62.31249314455943\n",
      "Epoch196  loss:1.0427080929279327  accuracy:62.97785659603586\n",
      "Epoch197  loss:1.0518495067954063  accuracy:62.459293100040924\n",
      "Epoch198  loss:1.0435448892414572  accuracy:62.30673187167814\n",
      "Epoch199  loss:1.054112330824137  accuracy:62.743365005597084\n",
      "Epoch200  loss:1.070991723239422  accuracy:62.724447950054696\n",
      "Epoch201  loss:1.0506249234080316  accuracy:62.932847178216164\n",
      "Epoch202  loss:1.0483823250979183  accuracy:62.59923088296646\n",
      "Epoch203  loss:1.0416709847748278  accuracy:63.77772435241932\n",
      "Epoch204  loss:1.0418404713273048  accuracy:63.108961100451594\n",
      "Epoch205  loss:1.0491450667381286  accuracy:63.24063412667863\n",
      "Epoch206  loss:1.0431966647505762  accuracy:63.12467955254479\n",
      "Epoch207  loss:1.043934776633978  accuracy:62.772872358792796\n",
      "Epoch208  loss:1.061226687580347  accuracy:63.07619624871134\n",
      "Epoch209  loss:1.0539670541882515  accuracy:63.30741177089399\n",
      "Epoch210  loss:1.0541256111115216  accuracy:62.89884240133387\n",
      "Epoch211  loss:1.040655278041959  accuracy:63.32431056674258\n",
      "Epoch212  loss:1.03911564797163  accuracy:63.89152206387944\n",
      "Epoch213  loss:1.065076333284378  accuracy:63.27366245735999\n",
      "Epoch214  loss:1.0502638146281242  accuracy:63.123996457777\n",
      "Epoch215  loss:1.0446806697174906  accuracy:63.79704449001111\n",
      "Epoch216  loss:1.0394420139491558  accuracy:63.078498263194376\n",
      "Epoch217  loss:1.0334582317620518  accuracy:63.34356612754495\n",
      "Epoch218  loss:1.028465785458684  accuracy:64.25279942901113\n",
      "Epoch219  loss:1.0275359913706779  accuracy:63.828168501705164\n",
      "Epoch220  loss:1.0572980746626852  accuracy:62.337717648816955\n",
      "Epoch221  loss:1.0623972456902266  accuracy:62.544177356262786\n",
      "Epoch222  loss:1.0459875512868164  accuracy:63.819996334895436\n",
      "Epoch223  loss:1.0426171727478504  accuracy:63.47449072682205\n",
      "Epoch224  loss:1.0306424481794239  accuracy:63.118232054654825\n",
      "Epoch225  loss:1.0528876259922981  accuracy:63.400019989767976\n",
      "Epoch226  loss:1.0299067597836256  accuracy:64.1402366359373\n",
      "Epoch227  loss:1.0265543416142464  accuracy:63.988526967503724\n",
      "Epoch228  loss:1.034441613033414  accuracy:63.513525050865596\n",
      "Epoch229  loss:1.0271766785532237  accuracy:64.46835299914825\n",
      "Epoch230  loss:1.0215112943202256  accuracy:63.90512050099903\n",
      "Epoch231  loss:1.0503431405872106  accuracy:63.34989750385506\n",
      "Epoch232  loss:1.025980055332184  accuracy:64.0430121419841\n",
      "Epoch233  loss:1.0115185830742122  accuracy:64.42892539341011\n",
      "Epoch234  loss:1.0390381738543508  accuracy:63.9834381303088\n",
      "Epoch235  loss:1.028227784484625  accuracy:63.540366572504894\n",
      "Epoch236  loss:1.0183276779949666  accuracy:64.36395052856912\n",
      "Epoch237  loss:1.0319480687379838  accuracy:64.24005202087301\n",
      "Epoch238  loss:1.0218833219259977  accuracy:64.14749750005123\n",
      "Epoch239  loss:1.031901491433382  accuracy:63.652674673117104\n",
      "Epoch240  loss:1.018933602422476  accuracy:64.30000524015175\n",
      "Epoch241  loss:1.0292569905519484  accuracy:63.62378016614251\n",
      "Epoch242  loss:1.0646827064454556  accuracy:62.711482994306415\n",
      "Epoch243  loss:1.053503939509392  accuracy:63.362683474285895\n",
      "Epoch244  loss:1.0289673704653979  accuracy:63.81955281063203\n",
      "Epoch245  loss:1.0221165871247646  accuracy:64.60349543566814\n",
      "Epoch246  loss:1.0705409348011017  accuracy:63.3660679915824\n",
      "Epoch247  loss:1.0371806658804417  accuracy:63.847472179054215\n",
      "Epoch248  loss:1.031106488406658  accuracy:64.00947288328891\n",
      "Epoch249  loss:1.0490879461169242  accuracy:64.407253850154\n",
      "Epoch250  loss:1.0439695928245782  accuracy:63.42481203962181\n",
      "Epoch251  loss:1.055400438606739  accuracy:63.93298267185217\n",
      "Epoch252  loss:1.0636936649680138  accuracy:63.217044837596525\n",
      "Epoch253  loss:1.0291453689336778  accuracy:64.25703974439777\n",
      "Epoch254  loss:1.0525054190307854  accuracy:63.70478890047491\n",
      "Epoch255  loss:1.0572794154286385  accuracy:63.73422377247252\n",
      "Epoch256  loss:1.064261393621564  accuracy:63.29906038771311\n",
      "Epoch257  loss:1.0712186966091393  accuracy:62.97067152111587\n",
      "Epoch258  loss:1.0737270168960091  accuracy:62.95077536020221\n",
      "Epoch259  loss:1.0195643231272697  accuracy:64.63780008622469\n",
      "Epoch260  loss:1.0552333831787108  accuracy:63.22174214511873\n",
      "Epoch261  loss:1.0640127155929802  accuracy:63.406599123679584\n",
      "Epoch262  loss:1.096967062354088  accuracy:61.96631120881244\n",
      "Epoch263  loss:1.0487671986222264  accuracy:63.56750435974245\n",
      "Epoch264  loss:1.0504889443516732  accuracy:64.05943583582129\n",
      "Epoch265  loss:1.0528402451425791  accuracy:64.06100101157053\n",
      "Epoch266  loss:1.0517988488078116  accuracy:63.95678692501668\n",
      "Epoch267  loss:1.0342202950268986  accuracy:64.48983280571744\n",
      "Epoch268  loss:1.053582015633583  accuracy:64.18805431866784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch269  loss:1.067303716391325  accuracy:63.729333645210794\n",
      "Epoch270  loss:1.0717441733926536  accuracy:63.253417907755384\n",
      "Epoch271  loss:1.067143155634403  accuracy:62.57069975733381\n",
      "Epoch272  loss:1.0565537681803108  accuracy:64.28854412999084\n",
      "Epoch273  loss:1.0616679593920706  accuracy:63.954458279318054\n",
      "Epoch274  loss:1.0646733846515417  accuracy:63.26604264785533\n",
      "Epoch275  loss:1.0328348599374295  accuracy:64.37088687540385\n",
      "Epoch276  loss:1.0548446252942085  accuracy:64.36572147221985\n",
      "Epoch277  loss:1.050640894472599  accuracy:63.97801032228219\n",
      "Epoch278  loss:1.0607966117560863  accuracy:63.35707554600577\n",
      "Epoch279  loss:1.0574636764824388  accuracy:63.36134542508207\n",
      "Epoch280  loss:1.025670679286122  accuracy:64.81149084710745\n",
      "Epoch281  loss:1.0478616178035733  accuracy:63.182153304306205\n",
      "Epoch282  loss:1.0681006312370298  accuracy:63.70485583963409\n",
      "Epoch283  loss:1.0458003960549829  accuracy:64.18373370532818\n",
      "Epoch284  loss:1.0308327566832303  accuracy:64.30821781915354\n",
      "Epoch285  loss:1.048933001793921  accuracy:64.0694196290366\n",
      "Epoch286  loss:1.0778833642601966  accuracy:62.45517811282499\n",
      "Epoch287  loss:1.0238788424059748  accuracy:64.7292927006079\n",
      "Epoch288  loss:1.050562667287886  accuracy:64.11800075037365\n",
      "Epoch289  loss:1.054216270148754  accuracy:63.60842968996805\n",
      "Epoch290  loss:1.0624499674886465  accuracy:63.279448131707376\n",
      "Epoch291  loss:1.0524496793746947  accuracy:63.65222742803197\n",
      "Epoch292  loss:1.0703481301665305  accuracy:63.30110620063144\n",
      "Epoch293  loss:1.0684716656804085  accuracy:62.924557497644166\n",
      "Epoch294  loss:1.0495364889502525  accuracy:63.90597326738325\n",
      "Epoch295  loss:1.1065969586372375  accuracy:61.723426204239495\n",
      "Epoch296  loss:1.0756272971630096  accuracy:62.53351386459118\n",
      "Epoch297  loss:1.0516793847084045  accuracy:63.96864396829089\n",
      "Epoch298  loss:1.057485457509756  accuracy:63.56803868176509\n",
      "Epoch299  loss:1.111231331527233  accuracy:62.3203075987188\n",
      "Epoch300  loss:1.0180797705426812  accuracy:64.75373706964017\n",
      "Epoch301  loss:1.040738458931446  accuracy:64.39274139800418\n",
      "Epoch302  loss:1.0526940628886223  accuracy:63.83860845728415\n",
      "Epoch303  loss:1.0412382993847131  accuracy:64.49410599518188\n",
      "Epoch304  loss:1.0557922847568988  accuracy:63.77751979784562\n",
      "Epoch305  loss:1.0492228522896767  accuracy:63.4989286019697\n",
      "Epoch306  loss:1.064024817943573  accuracy:62.26044447739149\n",
      "Epoch307  loss:1.0477263838052748  accuracy:63.27818917218989\n",
      "Epoch308  loss:1.0542523309588434  accuracy:63.928207803462634\n",
      "Epoch309  loss:1.035979287326336  accuracy:64.26786685066848\n",
      "Epoch310  loss:1.1346413165330886  accuracy:61.933553117408955\n",
      "Epoch311  loss:1.0896577984094618  accuracy:61.70590693169349\n",
      "Epoch312  loss:1.0432845272123814  accuracy:64.37981838649895\n",
      "Epoch313  loss:1.0473781281150876  accuracy:64.02920469570329\n",
      "Epoch314  loss:1.043365815281868  accuracy:64.5350562149075\n",
      "Epoch315  loss:1.0571229748427868  accuracy:63.91244938306123\n",
      "Epoch316  loss:1.0701223835349083  accuracy:63.29070183811804\n",
      "Epoch317  loss:1.040319592691958  accuracy:63.87960392108244\n",
      "Epoch318  loss:1.0699250996112826  accuracy:63.744369253355174\n",
      "Epoch319  loss:1.0358549788594247  accuracy:64.25169333006721\n",
      "Epoch320  loss:1.0517894715070724  accuracy:64.02042499962482\n",
      "Epoch321  loss:1.1041542118415235  accuracy:62.97350757207873\n",
      "Epoch322  loss:1.0906794626265766  accuracy:63.24597325544474\n",
      "Epoch323  loss:1.036371224373579  accuracy:64.8239296183477\n",
      "Epoch324  loss:1.1536943703889848  accuracy:61.721882039136936\n",
      "Epoch325  loss:1.0537893056869507  accuracy:64.73319912809572\n",
      "Epoch326  loss:1.048072796314955  accuracy:63.63883232005043\n",
      "Epoch327  loss:1.091811229288578  accuracy:63.487144417331876\n",
      "Epoch328  loss:1.1091442197561263  accuracy:63.36803561334841\n",
      "Epoch329  loss:1.0997727051377297  accuracy:62.831194135564886\n",
      "Epoch330  loss:1.0655546906404196  accuracy:64.38364502375849\n",
      "Epoch331  loss:1.0757543468847872  accuracy:62.91703143491388\n",
      "Epoch332  loss:1.106781440973282  accuracy:62.7004595132177\n",
      "Epoch333  loss:1.1301867663860319  accuracy:63.435293741583216\n",
      "Epoch334  loss:1.1095800034701826  accuracy:61.80455896304236\n",
      "Epoch335  loss:1.0701284896582366  accuracy:63.56136656892471\n",
      "Epoch336  loss:1.1216807678341865  accuracy:62.77214479468153\n",
      "Epoch337  loss:1.0824131935834886  accuracy:63.23888051329897\n",
      "Epoch338  loss:1.0739979963749646  accuracy:64.18353489042086\n",
      "Epoch339  loss:1.046304580569267  accuracy:63.769811322549074\n",
      "Epoch340  loss:1.0767906088382007  accuracy:64.11438997129768\n",
      "Epoch341  loss:1.1166884494014084  accuracy:62.618792029767185\n",
      "Epoch342  loss:1.061667535454035  accuracy:63.71118608283097\n",
      "Epoch343  loss:1.1299475230276583  accuracy:62.64195344892605\n",
      "Epoch344  loss:1.092906592786312  accuracy:64.01237904057123\n",
      "Epoch345  loss:1.1158757979050276  accuracy:63.40931247145402\n",
      "Epoch346  loss:1.1241450384259224  accuracy:62.66940013935552\n",
      "Epoch347  loss:1.126744414307177  accuracy:62.407693952149906\n",
      "Epoch348  loss:1.1015952065587045  accuracy:63.30430255163901\n",
      "Epoch349  loss:1.1674277737736705  accuracy:63.02832205391344\n",
      "Epoch350  loss:1.0570909529924393  accuracy:64.13742250404931\n",
      "Epoch351  loss:1.0738166280090808  accuracy:63.05139528266325\n",
      "Epoch352  loss:1.1082845479249952  accuracy:63.66673746538321\n",
      "Epoch353  loss:1.0479854524135592  accuracy:64.67378616791339\n",
      "Epoch354  loss:1.0907861799001692  accuracy:63.24826173295278\n",
      "Epoch355  loss:1.0965424999594688  accuracy:63.39346831363449\n",
      "Epoch356  loss:1.0469181776046752  accuracy:63.69717563647452\n",
      "Epoch357  loss:1.0477802872657775  accuracy:63.64109025923717\n",
      "Epoch358  loss:1.092083522118628  accuracy:62.65316224544284\n",
      "Epoch359  loss:1.106469601392746  accuracy:62.973062422190935\n",
      "Epoch360  loss:1.122590412572026  accuracy:63.13187852120734\n",
      "Epoch361  loss:1.126916579157114  accuracy:62.155866233890194\n",
      "Epoch362  loss:1.050723810493946  accuracy:64.1944696607397\n",
      "Epoch363  loss:1.1458536684513092  accuracy:62.57520801675467\n",
      "Epoch364  loss:1.074523193202913  accuracy:64.01885602281867\n",
      "Epoch365  loss:1.073236408829689  accuracy:62.97743620461765\n",
      "Epoch366  loss:1.0840937428176403  accuracy:62.780031266010496\n",
      "Epoch367  loss:1.1211845420300959  accuracy:63.89076177520241\n",
      "Epoch368  loss:1.1647525280714033  accuracy:61.560583955524734\n",
      "Epoch369  loss:1.0742344519123435  accuracy:62.929468492468196\n",
      "Epoch370  loss:1.1182322710752488  accuracy:63.05652100904695\n",
      "Epoch371  loss:1.1131379157304762  accuracy:62.972077190445845\n",
      "Epoch372  loss:1.1397002261132  accuracy:62.3479799493124\n",
      "Epoch373  loss:1.1434101887047292  accuracy:61.58050873931498\n",
      "Epoch374  loss:1.106027979031205  accuracy:62.99193974106665\n",
      "Epoch375  loss:1.1254117824137213  accuracy:62.614553485164194\n",
      "Epoch376  loss:1.0374626170843841  accuracy:64.56478836441663\n",
      "Epoch377  loss:1.1097531363368036  accuracy:63.2010183468\n",
      "Epoch378  loss:1.1078405309468506  accuracy:62.966849146506675\n",
      "Epoch379  loss:1.1174601435661315  accuracy:63.33381109674129\n",
      "Epoch380  loss:1.1084174752235414  accuracy:63.32515806217359\n",
      "Epoch381  loss:1.0615388832986357  accuracy:64.68154288991828\n",
      "Epoch382  loss:1.0662420302629472  accuracy:63.17327681194225\n",
      "Epoch383  loss:1.0855256736278536  accuracy:64.15720244696378\n",
      "Epoch384  loss:1.0978784464299682  accuracy:63.71803851937496\n",
      "Epoch385  loss:1.0864679273217914  accuracy:63.63954091376883\n",
      "Epoch386  loss:1.1059273764491082  accuracy:63.400091643869814\n",
      "Epoch387  loss:1.1129939764738084  accuracy:63.685167151167676\n",
      "Epoch388  loss:1.2270986676216127  accuracy:63.66380390479943\n",
      "Epoch389  loss:1.0907981161028146  accuracy:63.73891647805114\n",
      "Epoch390  loss:1.0782736971974374  accuracy:64.33528256802548\n",
      "Epoch391  loss:1.101526055857539  accuracy:62.911309298804674\n",
      "Epoch392  loss:1.1017573952674864  accuracy:63.11046126392259\n",
      "Epoch393  loss:1.1727595530450345  accuracy:62.26339698058111\n",
      "Epoch394  loss:1.1212785817682744  accuracy:62.718799513752046\n",
      "Epoch395  loss:1.0604520820081234  accuracy:63.940044184581666\n",
      "Epoch396  loss:1.0805286474525926  accuracy:63.997522650743385\n",
      "Epoch397  loss:1.1152926601469517  accuracy:62.63348370495355\n",
      "Epoch398  loss:1.062515613436699  accuracy:63.967042652913086\n",
      "Epoch399  loss:1.142390548437834  accuracy:62.93768783477602\n",
      "Epoch400  loss:1.220893047004938  accuracy:61.04563334025606\n",
      "Epoch401  loss:1.1024971019476653  accuracy:63.472227869632235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch402  loss:1.0876073494553566  accuracy:63.345044674812726\n",
      "Epoch403  loss:1.059164663590491  accuracy:64.7258179505298\n",
      "Epoch404  loss:1.1739774793386462  accuracy:60.83104765252247\n",
      "Epoch405  loss:1.1159338703379036  accuracy:62.765515002291195\n",
      "Epoch406  loss:1.1129788979887962  accuracy:63.704972537583615\n",
      "Epoch407  loss:1.1013968452811242  accuracy:63.01746197885051\n",
      "Epoch408  loss:1.1721352599561212  accuracy:61.09461172582728\n",
      "Epoch409  loss:1.079307480342686  accuracy:64.44412943686962\n",
      "Epoch410  loss:1.0898219354450702  accuracy:64.0555196025366\n",
      "Epoch411  loss:1.066540462523699  accuracy:63.853924197452855\n",
      "Epoch412  loss:1.101203860342503  accuracy:63.43537351049741\n",
      "Epoch413  loss:1.132410748489201  accuracy:61.769249007176235\n",
      "Epoch414  loss:1.1272393494844437  accuracy:61.94696192026489\n",
      "Epoch415  loss:1.0613232623320072  accuracy:63.517619966174934\n",
      "Epoch416  loss:1.1687615543603898  accuracy:60.77310815741037\n",
      "Epoch417  loss:1.0981245163828135  accuracy:63.17946527890714\n",
      "Epoch418  loss:1.1549482304602863  accuracy:61.792501527818196\n",
      "Epoch419  loss:1.1103244736790654  accuracy:62.51916136180946\n",
      "Epoch420  loss:1.0948873167857527  accuracy:63.82068401503685\n",
      "Epoch421  loss:1.1159767389297486  accuracy:62.9603982721108\n",
      "Epoch422  loss:1.1086546722799542  accuracy:63.5424815610189\n",
      "Epoch423  loss:1.1099172526970504  accuracy:62.755652440643196\n",
      "Epoch424  loss:1.1584540590643881  accuracy:61.8126859366453\n",
      "Epoch425  loss:1.0889150459319352  accuracy:63.732210378079515\n",
      "Epoch426  loss:1.1302907291799782  accuracy:62.427112329862275\n",
      "Epoch427  loss:1.165934769809246  accuracy:59.53593706358027\n",
      "Epoch428  loss:1.115083618275821  accuracy:63.523492568764595\n",
      "Epoch429  loss:1.16183070410043  accuracy:60.91247223627679\n",
      "Epoch430  loss:1.1931541308760645  accuracy:60.84653230063645\n",
      "Epoch431  loss:1.106155006820336  accuracy:62.666055570881426\n",
      "Epoch432  loss:1.1520120412111283  accuracy:60.62189353584677\n",
      "Epoch433  loss:1.129085800051689  accuracy:62.17336009368746\n",
      "Epoch434  loss:1.1691211104393007  accuracy:60.435912548124506\n",
      "Epoch435  loss:1.1115494385361668  accuracy:61.972735197198695\n",
      "Epoch436  loss:1.1173185959458354  accuracy:63.41793005691533\n",
      "Epoch437  loss:1.1187131829559804  accuracy:62.05904133005962\n",
      "Epoch438  loss:1.1095795836299658  accuracy:61.77580225057671\n",
      "Epoch439  loss:1.1431950964033604  accuracy:61.88056010889223\n",
      "Epoch440  loss:1.1236912557855248  accuracy:61.83578608184328\n",
      "Epoch441  loss:1.123048022389412  accuracy:62.91901716729299\n",
      "Epoch442  loss:1.1330886036157606  accuracy:62.26897011422247\n",
      "Epoch443  loss:1.132103781402111  accuracy:61.38472454552485\n",
      "Epoch444  loss:1.1330985872074961  accuracy:63.24876511643715\n",
      "Epoch445  loss:1.146348312497139  accuracy:60.58993957010239\n",
      "Epoch446  loss:1.1421632252633571  accuracy:62.382113143731026\n",
      "Epoch447  loss:1.0664099801331757  accuracy:64.60527326725553\n",
      "Epoch448  loss:1.1332599667832257  accuracy:62.13915717318294\n",
      "Epoch449  loss:1.1001572262495756  accuracy:62.087905684049545\n",
      "Epoch450  loss:1.1209249508567154  accuracy:61.97659128948976\n",
      "Epoch451  loss:1.1953164510428902  accuracy:61.11287374067364\n",
      "Epoch452  loss:1.1762063845992092  accuracy:61.04402689538716\n",
      "Epoch453  loss:1.1399869903922082  accuracy:60.191511443478774\n",
      "Epoch454  loss:1.1545086994767186  accuracy:61.31899969445951\n",
      "Epoch455  loss:1.1612815652042627  accuracy:61.22740800273501\n",
      "Epoch456  loss:1.1802609708160163  accuracy:60.0521264013296\n",
      "Epoch457  loss:1.1573410049080848  accuracy:61.04041760236647\n",
      "Epoch458  loss:1.1885662995278838  accuracy:61.54886105190616\n",
      "Epoch459  loss:1.1481917533557862  accuracy:62.112593382845255\n",
      "Epoch460  loss:1.1086535468697547  accuracy:62.84802204801995\n",
      "Epoch461  loss:1.1502674148418008  accuracy:60.78528544517556\n",
      "Epoch462  loss:1.134753203764558  accuracy:61.828183064813345\n",
      "Epoch463  loss:1.1236650548875333  accuracy:62.01789924799522\n",
      "Epoch464  loss:1.0934405088424684  accuracy:62.32886753042285\n",
      "Epoch465  loss:1.0995522026903928  accuracy:63.536448667195565\n",
      "Epoch466  loss:1.1239199962466955  accuracy:63.529011694385154\n",
      "Epoch467  loss:1.1539310879074038  accuracy:61.576534814440805\n",
      "Epoch468  loss:1.1406486853957176  accuracy:62.75543516661945\n",
      "Epoch469  loss:1.1785996466875075  accuracy:61.23048227661896\n",
      "Epoch470  loss:1.1600836120545865  accuracy:62.536633896505386\n",
      "Epoch471  loss:1.109497867152095  accuracy:62.459979726833836\n",
      "Epoch472  loss:1.1934284597635267  accuracy:60.20725658366905\n",
      "Epoch473  loss:1.1500846669077873  accuracy:61.155652246015066\n",
      "Epoch474  loss:1.1468898123595863  accuracy:60.078416821535185\n",
      "Epoch475  loss:1.6351713446900247  accuracy:62.40933114876645\n",
      "Epoch476  loss:1.1281989213079218  accuracy:62.56003082013733\n",
      "Epoch477  loss:1.1791618376970292  accuracy:59.96377288676758\n",
      "Epoch478  loss:1.1454161108471452  accuracy:61.04094039486407\n",
      "Epoch479  loss:1.214803022891283  accuracy:63.033069866371726\n",
      "Epoch480  loss:1.088794006407261  accuracy:65.59418242146627\n",
      "Epoch481  loss:1.1527934450656177  accuracy:60.17687881040111\n",
      "Epoch482  loss:1.2342553161084653  accuracy:59.13042860702827\n",
      "Epoch483  loss:1.1997701013460755  accuracy:60.75371926080242\n",
      "Epoch484  loss:1.1429543927311898  accuracy:63.094625122848306\n",
      "Epoch485  loss:1.1945131957530974  accuracy:60.44866638396828\n",
      "Epoch486  loss:1.1281821636483074  accuracy:62.24883767993863\n",
      "Epoch487  loss:1.1159731224179266  accuracy:62.843357296462585\n",
      "Epoch488  loss:1.153503477945924  accuracy:62.216348998592785\n",
      "Epoch489  loss:1.1232899144291877  accuracy:61.75302614678252\n",
      "Epoch490  loss:1.1210189841687679  accuracy:62.17314911682148\n",
      "Epoch491  loss:1.1339697953313586  accuracy:62.55655230387422\n",
      "Epoch492  loss:1.1247478943318128  accuracy:62.988500302853225\n",
      "Epoch493  loss:1.1651183664798737  accuracy:61.197433638998206\n",
      "Epoch494  loss:1.205556723475456  accuracy:60.93840791418141\n",
      "Epoch495  loss:1.1926101669669151  accuracy:59.89261771805424\n",
      "Epoch496  loss:1.1481035119853913  accuracy:62.449269993784846\n",
      "Epoch497  loss:1.0863330455496907  accuracy:63.374371008559805\n",
      "Epoch498  loss:1.162312363088131  accuracy:61.69354979921524\n",
      "Epoch499  loss:1.149730411171913  accuracy:61.292585208873454\n",
      "Epoch500  loss:1.146501038223505  accuracy:61.682480837156476\n"
     ]
    }
   ],
   "source": [
    "server = Server()\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset)\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "acc_valid = []\n",
    "loss_valid = []\n",
    "\n",
    "early_stopping = Early_Stopping(args.partience)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(args.global_epochs):\n",
    "  sample_worker = server.sample_worker(workers)\n",
    "  server.send_model(sample_worker)\n",
    "\n",
    "  acc_train_avg = 0.0\n",
    "  loss_train_avg = 0.0\n",
    "  acc_valid_avg = 0.0\n",
    "  loss_valid_avg = 0.0\n",
    "  for worker in sample_worker:\n",
    "    worker.local_train()\n",
    "    acc_train_tmp,loss_train_tmp,acc_valid_tmp,loss_valid_tmp = worker.global_train()\n",
    "    acc_train_avg += acc_train_tmp/len(sample_worker)\n",
    "    loss_train_avg += loss_train_tmp/len(sample_worker)\n",
    "    acc_valid_avg += acc_valid_tmp/len(sample_worker)\n",
    "    loss_valid_avg += loss_valid_tmp/len(sample_worker)\n",
    "  server.aggregate_model(sample_worker)\n",
    "  '''\n",
    "  server.model.to(args.device)\n",
    "  for worker in workers:\n",
    "    acc_valid_tmp,loss_valid_tmp = test(server.model,args.criterion,worker.valloader)\n",
    "    acc_valid_avg += acc_valid_tmp/len(workers)\n",
    "    loss_valid_avg += loss_valid_tmp/len(workers)\n",
    "  server.model.to('cpu')\n",
    "  '''\n",
    "  print('Epoch{}  loss:{}  accuracy:{}'.format(epoch+1,loss_valid_avg,acc_valid_avg))\n",
    "  acc_train.append(acc_train_avg)\n",
    "  loss_train.append(loss_train_avg)\n",
    "  acc_valid.append(acc_valid_avg)\n",
    "  loss_valid.append(loss_valid_avg)\n",
    "\n",
    "  if early_stopping.validate(loss_valid_avg):\n",
    "    print('Early Stop')\n",
    "    break\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "mi_uceyoptLP",
    "outputId": "bc067e09-01bc-4e65-daf9-ac2f42373cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 accuracy:36.712749615975426  loss:1.7518707513809204\n",
      "Worker2 accuracy:70.28688524590164  loss:1.11521577835083\n",
      "Worker3 accuracy:62.285714285714285  loss:1.193273901939392\n",
      "Worker4 accuracy:75.7936507936508  loss:1.715030312538147\n",
      "Worker5 accuracy:72.78048780487805  loss:0.9932506382465363\n",
      "Worker6 accuracy:50.97493036211699  loss:2.169666051864624\n",
      "Worker7 accuracy:50.10526315789474  loss:0.9831329584121704\n",
      "Worker8 accuracy:77.46650426309378  loss:1.119786024093628\n",
      "Worker9 accuracy:81.65605095541402  loss:0.9980876445770264\n",
      "Worker10 accuracy:79.76190476190476  loss:0.4624582529067993\n",
      "Worker11 accuracy:65.71428571428571  loss:1.5716962814331055\n",
      "Worker12 accuracy:49.49640287769784  loss:1.4958406686782837\n",
      "Worker13 accuracy:76.93877551020408  loss:0.7057133913040161\n",
      "Worker14 accuracy:58.96551724137931  loss:1.7432740926742554\n",
      "Worker15 accuracy:51.851851851851855  loss:1.675270915031433\n",
      "Worker16 accuracy:41.048034934497814  loss:1.6348832845687866\n",
      "Worker17 accuracy:51.41509433962264  loss:1.4256001710891724\n",
      "Worker18 accuracy:48.62888482632541  loss:2.1748626232147217\n",
      "Worker19 accuracy:55.056179775280896  loss:1.9753824472427368\n",
      "Worker20 accuracy:50.37593984962406  loss:1.7635935544967651\n",
      "Test  loss:1.4333944872021676  accuracy:60.3657554083657\n"
     ]
    }
   ],
   "source": [
    "acc_test = []\n",
    "loss_test = []\n",
    "\n",
    "nums = 0\n",
    "for worker in workers:\n",
    "  nums += worker.test_data_num\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.aggregation_weight = 1.0*worker.test_data_num/nums\n",
    "  worker.global_model = copy.deepcopy(server.global_model)\n",
    "  worker.local_train()\n",
    "  acc_tmp,loss_tmp = test(worker.mix_model,args.criterion,worker.testloader)\n",
    "  acc_test.append(acc_tmp)\n",
    "  loss_test.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "  worker.mix_model = worker.mix_model.to('cpu')\n",
    "  del worker.mix_model,worker.global_model\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_test_avg = sum(acc_test)/len(acc_test)\n",
    "loss_test_avg = sum(loss_test)/len(loss_test)\n",
    "print('Test  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
