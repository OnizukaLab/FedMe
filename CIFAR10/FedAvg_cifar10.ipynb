{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    # random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 42\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O0TfzOhU-QlG"
   },
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 40\n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 500\n",
    "    self.local_epochs = 2\n",
    "    self.lr = None\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 500\n",
    "    self.worker_num = 20\n",
    "    self.sample_num = 20\n",
    "    self.unlabeleddata_size = 1000\n",
    "    self.device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    self.alpha_label = 0.5\n",
    "    self.alpha_size = 10\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = []\n",
    "lr_list.append(10**-3.0)\n",
    "lr_list.append(10**-2.5)\n",
    "lr_list.append(10**-2.0)\n",
    "lr_list.append(10**-1.5)\n",
    "lr_list.append(10**-1.0)\n",
    "lr_list.append(10**-0.5)\n",
    "lr_list.append(10**0.0)\n",
    "lr_list.append(10**0.5)\n",
    "\n",
    "args.lr = lr_list[lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r5PuCcqmJNUQ"
   },
   "outputs": [],
   "source": [
    "class LocalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.label[idx]\n",
    "        if self.transform:\n",
    "            out_data = self.transform(out_data)\n",
    "        return out_data, out_label\n",
    "    \n",
    "class DatasetFromSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.subset[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    \n",
    "class GlobalDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,federated_dataset,transform=None):\n",
    "    self.transform = transform\n",
    "    self.data = []\n",
    "    self.label = []\n",
    "    for dataset in federated_dataset:\n",
    "      for (data,label) in dataset:\n",
    "        self.data.append(data)\n",
    "        self.label.append(label)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    out_data = self.data[idx]\n",
    "    out_label = self.label[idx]\n",
    "    if self.transform:\n",
    "        out_data = self.transform(out_data)\n",
    "    return out_data, out_label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "class UnlabeledDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,transform=None):\n",
    "    self.transform = transform\n",
    "    self.data = []\n",
    "    self.target = None\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    out_data = self.data[idx]\n",
    "    out_label = 'unlabeled'\n",
    "    if self.transform:\n",
    "        out_data = self.transform(out_data)\n",
    "    return out_data, out_label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(Centralized=False,unlabeled_data=False):\n",
    "    \n",
    "    transform_train = transforms.Compose([transforms.ToPILImage(),\n",
    "                                    transforms.RandomCrop(32, padding=2),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.491372549, 0.482352941, 0.446666667), (0.247058824, 0.243529412, 0.261568627))])\n",
    "    transform_test = transforms.Compose([transforms.ToPILImage(),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.491372549, 0.482352941, 0.446666667), (0.247058824, 0.243529412, 0.261568627))])\n",
    "\n",
    "    # download train data\n",
    "    all_trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True)\n",
    "    #trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "    # download test data\n",
    "    all_testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True)\n",
    "    #testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "    \n",
    "    ## get unlabeled dataset\n",
    "    if unlabeled_data:\n",
    "        unlabeled_dataset = UnlabeledDataset(transform_test)\n",
    "        idx = sorted(random.sample(range(len(all_trainset)),args.unlabeleddata_size))\n",
    "        unlabeled_dataset.data = np.array([all_trainset.data[i]  for i in idx])\n",
    "        all_trainset.data = np.delete(all_trainset.data,idx,0)\n",
    "        all_trainset.targets = np.delete(all_trainset.targets,idx,0)\n",
    "    all_train_data = np.array(all_trainset.data)\n",
    "    all_train_label = np.array(all_trainset.targets)\n",
    "    all_test_data = np.array(all_testset.data)\n",
    "    all_test_label = np.array(all_testset.targets)\n",
    "    print('Train:{} Test:{}'.format(len(all_train_data),len(all_test_data)))\n",
    "\n",
    "\n",
    "    ## Data size heterogeneity\n",
    "    data_proportions = np.random.dirichlet(np.repeat(args.alpha_size, args.worker_num))\n",
    "    train_data_proportions = np.array([0 for _ in range(args.worker_num)])\n",
    "    test_data_proportions = np.array([0 for _ in range(args.worker_num)])\n",
    "    for i in range(len(data_proportions)):\n",
    "        if i==(len(data_proportions)-1):\n",
    "            train_data_proportions = train_data_proportions.astype('int64')\n",
    "            test_data_proportions = test_data_proportions.astype('int64')\n",
    "            train_data_proportions[-1] = len(all_train_data) - np.sum(train_data_proportions[:-1])\n",
    "            test_data_proportions[-1] = len(all_test_data) - np.sum(test_data_proportions[:-1])\n",
    "        else:\n",
    "            train_data_proportions[i] = (data_proportions[i] * len(all_train_data))\n",
    "            test_data_proportions[i] = (data_proportions[i] * len(all_test_data))\n",
    "    min_size = 0\n",
    "    K = 10\n",
    "\n",
    "    '''\n",
    "    label_list = np.arange(10)\n",
    "    np.random.shuffle(label_list)\n",
    "    '''\n",
    "    label_list = list(range(K))\n",
    "\n",
    "\n",
    "    ## Data distribution heterogeneity\n",
    "    while min_size<10:\n",
    "        idx_train_batch = [[] for _ in range(args.worker_num)]\n",
    "        idx_test_batch = [[] for _ in range(args.worker_num)]\n",
    "        for k in label_list:\n",
    "            proportions_train = np.random.dirichlet(np.repeat(args.alpha_label, args.worker_num))\n",
    "            proportions_test = copy.deepcopy(proportions_train)\n",
    "            idx_k_train = np.where(all_train_label == k)[0]\n",
    "            idx_k_test = np.where(all_test_label == k)[0]\n",
    "            np.random.shuffle(idx_k_train)\n",
    "            np.random.shuffle(idx_k_test)\n",
    "            ## Balance (train)\n",
    "            proportions_train = np.array([p*(len(idx_j)<train_data_proportions[i]) for i,(p,idx_j) in enumerate(zip(proportions_train,idx_train_batch))])\n",
    "            proportions_train = proportions_train/proportions_train.sum()\n",
    "            proportions_train = (np.cumsum(proportions_train)*len(idx_k_train)).astype(int)[:-1]\n",
    "            idx_train_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_train_batch,np.split(idx_k_train,proportions_train))]\n",
    "\n",
    "            ## Balance (test)\n",
    "            proportions_test = np.array([p*(len(idx_j)<test_data_proportions[i]) for i,(p,idx_j) in enumerate(zip(proportions_test,idx_test_batch))])\n",
    "            proportions_test = proportions_test/proportions_test.sum()\n",
    "            proportions_test = (np.cumsum(proportions_test)*len(idx_k_test)).astype(int)[:-1]\n",
    "            idx_test_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_test_batch,np.split(idx_k_test,proportions_test))]\n",
    "\n",
    "            min_size = min([len(idx_j) for idx_j in idx_train_batch])\n",
    "\n",
    "    federated_trainset = []\n",
    "    federated_testset = []\n",
    "    for i in range(args.worker_num):\n",
    "        ## create trainset\n",
    "        data = [all_train_data[idx] for idx in idx_train_batch[i]]\n",
    "        label = [all_train_label[idx] for idx in idx_train_batch[i]]\n",
    "        federated_trainset.append(LocalDataset())\n",
    "        federated_trainset[-1].data = data\n",
    "        federated_trainset[-1].label = label\n",
    "\n",
    "        ## create testset\n",
    "        data = [all_test_data[idx] for idx in idx_test_batch[i]]\n",
    "        label = [all_test_label[idx] for idx in idx_test_batch[i]]\n",
    "        federated_testset.append(LocalDataset())\n",
    "        federated_testset[-1].data = data\n",
    "        federated_testset[-1].label = label\n",
    "\n",
    "        \n",
    "    ## split trainset\n",
    "    federated_valset = [None]*args.worker_num\n",
    "    for i in range(args.worker_num):\n",
    "        n_samples = len(federated_trainset[i])\n",
    "        if n_samples==1:\n",
    "            train_subset = federated_trainset[i]\n",
    "            val_subset = copy.deepcopy(federated_trainset[i])\n",
    "        else:\n",
    "            train_size = int(len(federated_trainset[i]) * 0.8) \n",
    "            val_size = n_samples - train_size \n",
    "            train_subset,val_subset = torch.utils.data.random_split(federated_trainset[i], [train_size, val_size])\n",
    "\n",
    "        federated_trainset[i] = DatasetFromSubset(train_subset)\n",
    "        federated_valset[i] = DatasetFromSubset(val_subset)\n",
    "\n",
    "    ## show data distribution\n",
    "    H = 4\n",
    "    W = 5\n",
    "    fig, axs = plt.subplots(H, W, figsize=(20, 5))\n",
    "    x = np.arange(1,11)\n",
    "    for i, (trainset,valset,testset) in enumerate(zip(federated_trainset,federated_valset,federated_testset)):\n",
    "        bottom = [0]*10\n",
    "        count = [0]*10\n",
    "        for _,label in trainset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        for j in range(len(count)):\n",
    "            bottom[j]+=count[j]\n",
    "        count = [0]*10\n",
    "        for _,label in valset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        for j in range(len(count)):\n",
    "            bottom[j]+=count[j]\n",
    "        count = [0]*10\n",
    "        for _,label in testset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        #axs[int(i/W), i%W].title(\"worker{}\".format(i+1), fontsize=12, color = \"green\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ## get global dataset\n",
    "    if Centralized:\n",
    "        global_trainset = GlobalDataset(federated_trainset)\n",
    "        global_valset = GlobalDataset(federated_valset)\n",
    "        global_testset =  GlobalDataset(federated_testset)\n",
    "        \n",
    "        #show_cifer(global_trainset.data,global_testset.label, cifar10_labels)\n",
    "\n",
    "        global_trainset.transform = transform_train\n",
    "        global_valset.transform = transform_test\n",
    "        global_testset.transform = transform_test\n",
    "\n",
    "        global_trainloader = torch.utils.data.DataLoader(global_trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "        global_valloader = torch.utils.data.DataLoader(global_valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "        global_testloader = torch.utils.data.DataLoader(global_testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "\n",
    "    ## set transform\n",
    "    for i in range(args.worker_num):\n",
    "        federated_trainset[i].transform = transform_train\n",
    "        federated_valset[i].transform = transform_test\n",
    "        federated_testset[i].transform = transform_test\n",
    "    \n",
    "    if Centralized and unlabeled_data:\n",
    "        return federated_trainset,federated_valset,federated_testset,global_trainloader,global_valloader,global_testloader,unlabeled_dataset\n",
    "    if Centralized:\n",
    "        return federated_trainset,federated_valset,federated_testset,global_trainloader,global_valloader,global_testloader\n",
    "    elif unlabeled_data:\n",
    "        return federated_trainset,federated_valset,federated_testset,unlabeled_dataset\n",
    "    else:\n",
    "        return federated_trainset,federated_valset,federated_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train:49000 Test:10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "federated_trainset,federated_valset,federated_testset,unlabeled_dataset = get_dataset(unlabeled_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39191, 9809, 10000]\n"
     ]
    }
   ],
   "source": [
    "total = [0,0,0]\n",
    "for i in range(args.worker_num):\n",
    "    total[0]+=len(federated_trainset[i])\n",
    "    total[1]+=len(federated_valset[i])\n",
    "    total[2]+=len(federated_testset[i])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZU3vAAb9-6SD"
   },
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG model \n",
    "    '''\n",
    "    def __init__(self, features, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            #print(\"in_channels: {}, v: {}\".format(in_channels, v))\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
    "          512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGGConvBlocks(nn.Module):\n",
    "    '''\n",
    "    VGG containers that only contains the conv layers \n",
    "    '''\n",
    "    def __init__(self, features, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class VGGContainer(nn.Module):\n",
    "    '''\n",
    "    VGG model \n",
    "    '''\n",
    "    def __init__(self, features, input_dim, hidden_dims, num_classes=10):\n",
    "        super(VGGContainer, self).__init__()\n",
    "        self.features = features\n",
    "        # note: we hard coded here a bit by assuming we only have two hidden layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dims[1], num_classes),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def matched_vgg11(matched_shapes):\n",
    "    # [(67, 27), (67,), (132, 603), (132,), (260, 1188), (260,), (261, 2340), (261,), (516, 2349), (516,), (517, 4644), (517,), \n",
    "    # (516, 4653), (516,), (516, 4644), (516,), (516, 515), (515,), (515, 515), (515,), (515, 10), (10,)]\n",
    "    processed_matched_shape = [matched_shapes[0][0], \n",
    "                                'M', \n",
    "                                matched_shapes[2][0], \n",
    "                                'M', \n",
    "                                matched_shapes[4][0], \n",
    "                                matched_shapes[6][0], \n",
    "                                'M', \n",
    "                                matched_shapes[8][0], \n",
    "                                matched_shapes[10][0], \n",
    "                                'M', \n",
    "                                matched_shapes[12][0], \n",
    "                                matched_shapes[14][0], \n",
    "                                'M']\n",
    "    return VGGContainer(make_layers(processed_matched_shape), input_dim=matched_shapes[16][0], \n",
    "            hidden_dims=[matched_shapes[16][1], matched_shapes[18][1]], num_classes=10)\n",
    "\n",
    "\n",
    "def vgg11():\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
    "    return VGG(make_layers(cfg['A']))\n",
    "\n",
    "\n",
    "def vgg11_bn(num_classes=10):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['A'], batch_norm=True), num_classes=num_classes)\n",
    "\n",
    "\n",
    "def vgg13():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n",
    "    return VGG(make_layers(cfg['B']))\n",
    "\n",
    "\n",
    "def vgg13_bn():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg16():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n",
    "    return VGG(make_layers(cfg['D']))\n",
    "\n",
    "\n",
    "def vgg16_bn():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n",
    "    return VGG(make_layers(cfg['E']))\n",
    "\n",
    "\n",
    "def vgg19_bn():\n",
    "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['E'], batch_norm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self):\n",
    "    self.model = vgg13()\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(federated_trainset[i],federated_valset[i],federated_testset[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "\n",
    "  def send_model(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "\n",
    "    for worker in workers:\n",
    "      worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "      worker.model = copy.deepcopy(self.model)\n",
    "      worker.model = worker.model.to(args.device)\n",
    "\n",
    "  def aggregate_model(self,workers):   \n",
    "    new_params = OrderedDict()\n",
    "    for i,worker in enumerate(workers):\n",
    "      worker_state = worker.model.state_dict()\n",
    "      for key in worker_state.keys():\n",
    "        if i==0:\n",
    "          new_params[key] = worker_state[key]*worker.aggregation_weight\n",
    "        else:\n",
    "          new_params[key] += worker_state[key]*worker.aggregation_weight\n",
    "      worker.model = worker.model.to('cpu')\n",
    "      del worker.model\n",
    "    self.model.load_state_dict(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,trainset,valset,testset):\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.model = None\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "    self.aggregation_weight = None\n",
    "\n",
    "  def local_train(self):\n",
    "    acc_train,loss_train = train(self.model,args.criterion,self.trainloader,args.local_epochs)\n",
    "    acc_valid,loss_valid = test(self.model,args.criterion,self.valloader)\n",
    "    return acc_train,loss_train,acc_valid,loss_valid\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7-GY66gROuEU"
   },
   "outputs": [],
   "source": [
    "def train(model,criterion,trainloader,epochs):\n",
    "  optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for (data,labels) in trainloader:\n",
    "      data,labels = Variable(data),Variable(labels)\n",
    "      data,labels = data.to(args.device),labels.to(args.device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(data)\n",
    "      loss = criterion(outputs,labels)\n",
    "      running_loss += loss.item()\n",
    "      predicted = torch.argmax(outputs,dim=1)\n",
    "      correct += (predicted==labels).sum().item()\n",
    "      count += len(labels)\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "      optimizer.step()\n",
    "\n",
    "  return 100.0*correct/count,running_loss/len(trainloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  count = 0\n",
    "  for (data,labels) in testloader:\n",
    "    data,labels = data.to(args.device),labels.to(args.device)\n",
    "    outputs = model(data)\n",
    "    running_loss += criterion(outputs,labels).item()\n",
    "    predicted = torch.argmax(outputs,dim=1)\n",
    "    correct += (predicted==labels).sum().item()\n",
    "    count += len(labels)\n",
    "\n",
    "  accuracy = 100.0*correct/count\n",
    "  loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "  return accuracy,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WMO7_WSLHeGl"
   },
   "outputs": [],
   "source": [
    "class Early_Stopping():\n",
    "  def __init__(self,partience):\n",
    "    self.step = 0\n",
    "    self.loss = float('inf')\n",
    "    self.partience = partience\n",
    "\n",
    "  def validate(self,loss):\n",
    "    if self.loss<loss:\n",
    "      self.step += 1\n",
    "      if self.step>self.partience:\n",
    "        return True\n",
    "    else:\n",
    "      self.step = 0\n",
    "      self.loss = loss\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "-noG_98IR-nZ",
    "outputId": "78a6ebe2-854a-4f83-dc45-5c4ac35b69e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1  loss:1.633384159207344  accuracy:43.38342688191277\n",
      "Epoch2  loss:1.4930766344070436  accuracy:43.82718161572266\n",
      "Epoch3  loss:1.480247959494591  accuracy:42.858551986235156\n",
      "Epoch4  loss:1.4766689777374267  accuracy:43.04309314379103\n",
      "Epoch5  loss:1.4640475332736969  accuracy:44.10558747240963\n",
      "Epoch6  loss:1.452163538336754  accuracy:44.13413078035386\n",
      "Epoch7  loss:1.4299251973629001  accuracy:45.544231823382596\n",
      "Epoch8  loss:1.3824883431196213  accuracy:49.887734600964336\n",
      "Epoch9  loss:1.3104586631059645  accuracy:52.942558772617254\n",
      "Epoch10  loss:1.2677022218704221  accuracy:53.71332201611844\n",
      "Epoch11  loss:1.2553341686725619  accuracy:54.42367849526681\n",
      "Epoch12  loss:1.2256717711687086  accuracy:54.46792277433714\n",
      "Epoch13  loss:1.193456906080246  accuracy:56.17340841584055\n",
      "Epoch14  loss:1.174005052447319  accuracy:56.159464337523374\n",
      "Epoch15  loss:1.1834368079900741  accuracy:56.40479237576704\n",
      "Epoch16  loss:1.1620809316635132  accuracy:57.50562703852613\n",
      "Epoch17  loss:1.1603864997625353  accuracy:57.284573325943995\n",
      "Epoch18  loss:1.148585271835327  accuracy:57.81865424353249\n",
      "Epoch19  loss:1.1574978232383728  accuracy:57.164110764886786\n",
      "Epoch20  loss:1.1352645456790926  accuracy:58.28543823785787\n",
      "Epoch21  loss:1.1413338020443917  accuracy:58.25636839155264\n",
      "Epoch22  loss:1.1356174290180208  accuracy:58.33862954729521\n",
      "Epoch23  loss:1.1191964760422703  accuracy:58.682994952831436\n",
      "Epoch24  loss:1.1109010338783265  accuracy:58.67888082842581\n",
      "Epoch25  loss:1.1199482887983323  accuracy:58.414205255986076\n",
      "Epoch26  loss:1.1039311066269875  accuracy:58.77735139672079\n",
      "Epoch27  loss:1.0997098207473752  accuracy:58.776829423622424\n",
      "Epoch28  loss:1.1119387239217757  accuracy:59.26067979721249\n",
      "Epoch29  loss:1.085734660923481  accuracy:59.40983300816097\n",
      "Epoch30  loss:1.0740551874041557  accuracy:59.525274585756264\n",
      "Epoch31  loss:1.0787647187709808  accuracy:60.254466096394886\n",
      "Epoch32  loss:1.0539446786046027  accuracy:60.68833630179278\n",
      "Epoch33  loss:1.040292704105377  accuracy:61.80109086128899\n",
      "Epoch34  loss:1.0349712014198302  accuracy:61.86050931045743\n",
      "Epoch35  loss:1.045443630218506  accuracy:61.85263965067321\n",
      "Epoch36  loss:1.0195016160607338  accuracy:62.52115151012386\n",
      "Epoch37  loss:1.0309993520379066  accuracy:62.384607283662824\n",
      "Epoch38  loss:1.0224025279283524  accuracy:62.83129690592156\n",
      "Epoch39  loss:0.9977404534816741  accuracy:63.64132643838993\n",
      "Epoch40  loss:1.0020169317722318  accuracy:63.19137480711121\n",
      "Epoch41  loss:0.9924362510442731  accuracy:64.22501177365812\n",
      "Epoch42  loss:0.9899113923311234  accuracy:63.69546966924331\n",
      "Epoch43  loss:0.979241520166397  accuracy:64.54968035583532\n",
      "Epoch44  loss:0.9802671745419504  accuracy:64.31341165023663\n",
      "Epoch45  loss:0.9729239970445632  accuracy:64.95267932010185\n",
      "Epoch46  loss:0.9906254634261131  accuracy:64.04821350609605\n",
      "Epoch47  loss:0.9544869929552079  accuracy:65.20167699635655\n",
      "Epoch48  loss:0.9432530090212822  accuracy:65.96593712587251\n",
      "Epoch49  loss:0.9543819442391395  accuracy:65.19896810271352\n",
      "Epoch50  loss:0.9416362524032591  accuracy:65.64878344506165\n",
      "Epoch51  loss:0.9154793992638589  accuracy:66.21722357284533\n",
      "Epoch52  loss:0.9206478402018548  accuracy:66.77090393367854\n",
      "Epoch53  loss:0.9238994494080546  accuracy:66.42243384432663\n",
      "Epoch54  loss:0.9165057986974716  accuracy:66.93275105292292\n",
      "Epoch55  loss:0.9042842715978624  accuracy:66.98577681888001\n",
      "Epoch56  loss:0.9007660046219826  accuracy:67.5560307570608\n",
      "Epoch57  loss:0.8903778225183486  accuracy:67.44858728934796\n",
      "Epoch58  loss:0.8769645854830742  accuracy:68.17382152230714\n",
      "Epoch59  loss:0.8820693939924242  accuracy:67.83584756055986\n",
      "Epoch60  loss:0.866421015560627  accuracy:68.52398728901095\n",
      "Epoch61  loss:0.873388786613941  accuracy:68.5431012264423\n",
      "Epoch62  loss:0.8719371870160103  accuracy:68.13591244806848\n",
      "Epoch63  loss:0.8447085916996001  accuracy:69.1639318789514\n",
      "Epoch64  loss:0.8693094864487648  accuracy:68.36231230832112\n",
      "Epoch65  loss:0.8461108267307281  accuracy:69.72797854101839\n",
      "Epoch66  loss:0.8305614084005356  accuracy:70.10254906851851\n",
      "Epoch67  loss:0.8287470817565917  accuracy:70.33766088340354\n",
      "Epoch68  loss:0.8270189881324769  accuracy:70.69885165051907\n",
      "Epoch69  loss:0.8255817875266074  accuracy:70.52650242019689\n",
      "Epoch70  loss:0.8055975407361985  accuracy:71.32205366076023\n",
      "Epoch71  loss:0.827171452343464  accuracy:70.90393007709947\n",
      "Epoch72  loss:0.8079069316387176  accuracy:71.09554356314698\n",
      "Epoch73  loss:0.7937344387173652  accuracy:70.94764384534763\n",
      "Epoch74  loss:0.7889776036143302  accuracy:71.893837751776\n",
      "Epoch75  loss:0.7924935713410377  accuracy:71.59857827317481\n",
      "Epoch76  loss:0.7967088714241982  accuracy:71.29017163026256\n",
      "Epoch77  loss:0.7757503025233747  accuracy:71.92356558793345\n",
      "Epoch78  loss:0.8000269681215286  accuracy:72.22570854369084\n",
      "Epoch79  loss:0.7877519145607947  accuracy:72.72712628987789\n",
      "Epoch80  loss:0.7512624904513359  accuracy:73.24122556414817\n",
      "Epoch81  loss:0.7606424525380137  accuracy:73.1900260495459\n",
      "Epoch82  loss:0.7638148963451385  accuracy:72.82218768688536\n",
      "Epoch83  loss:0.7313037857413291  accuracy:73.59008970564207\n",
      "Epoch84  loss:0.7389268234372139  accuracy:73.81927942034801\n",
      "Epoch85  loss:0.7482886701822282  accuracy:73.88964895634507\n",
      "Epoch86  loss:0.7249081850051878  accuracy:74.6324313667851\n",
      "Epoch87  loss:0.7278963379561902  accuracy:74.37076113341969\n",
      "Epoch88  loss:0.7310369297862053  accuracy:74.36349853498893\n",
      "Epoch89  loss:0.722172312438488  accuracy:74.58586533277297\n",
      "Epoch90  loss:0.7100733667612076  accuracy:74.78504323951849\n",
      "Epoch91  loss:0.7088936701416968  accuracy:75.47464422747903\n",
      "Epoch92  loss:0.6963466808199883  accuracy:75.58982056703277\n",
      "Epoch93  loss:0.7041711911559106  accuracy:75.42261536279831\n",
      "Epoch94  loss:0.6795417070388793  accuracy:75.82489760164079\n",
      "Epoch95  loss:0.7122069612145423  accuracy:75.5711649311466\n",
      "Epoch96  loss:0.6895207285881043  accuracy:75.72128555072904\n",
      "Epoch97  loss:0.6871846169233322  accuracy:76.53627319889388\n",
      "Epoch98  loss:0.706340779364109  accuracy:76.10580509512121\n",
      "Epoch99  loss:0.6937997177243234  accuracy:75.73159926624179\n",
      "Epoch100  loss:0.6639166295528411  accuracy:76.6329511366632\n",
      "Epoch101  loss:0.6877037763595581  accuracy:75.64845594889591\n",
      "Epoch102  loss:0.6854077927768231  accuracy:75.8039080761236\n",
      "Epoch103  loss:0.6642685636878015  accuracy:76.4950576417204\n",
      "Epoch104  loss:0.6562739476561548  accuracy:76.545135501076\n",
      "Epoch105  loss:0.6669880941510199  accuracy:76.93661345372051\n",
      "Epoch106  loss:0.6554938375949858  accuracy:77.14126435000976\n",
      "Epoch107  loss:0.6472177438437939  accuracy:77.53437088945515\n",
      "Epoch108  loss:0.640034618228674  accuracy:77.68761805437124\n",
      "Epoch109  loss:0.6606072127819062  accuracy:77.06974984900366\n",
      "Epoch110  loss:0.6395943686366081  accuracy:77.7054281188283\n",
      "Epoch111  loss:0.635047423839569  accuracy:77.63821641721275\n",
      "Epoch112  loss:0.6386456966400148  accuracy:77.95184912620006\n",
      "Epoch113  loss:0.6219871167093516  accuracy:78.1063151880812\n",
      "Epoch114  loss:0.6314159505069257  accuracy:77.7626112786959\n",
      "Epoch115  loss:0.6266303673386573  accuracy:77.86865806818972\n",
      "Epoch116  loss:0.6094473250210285  accuracy:78.87050990249959\n",
      "Epoch117  loss:0.6424614943563939  accuracy:78.07956730326795\n",
      "Epoch118  loss:0.6128683447837829  accuracy:77.98109683007017\n",
      "Epoch119  loss:0.613257860392332  accuracy:78.65827967905669\n",
      "Epoch120  loss:0.6165264911949634  accuracy:78.700085539757\n",
      "Epoch121  loss:0.6145638853311539  accuracy:78.98262402979921\n",
      "Epoch122  loss:0.6336985684931279  accuracy:77.81992766736896\n",
      "Epoch123  loss:0.6026013366878031  accuracy:78.7135340584195\n",
      "Epoch124  loss:0.6048591937869787  accuracy:78.98815111001754\n",
      "Epoch125  loss:0.6013324245810507  accuracy:79.10938283377986\n",
      "Epoch126  loss:0.5945565693080426  accuracy:79.38924264140603\n",
      "Epoch127  loss:0.6102159135043621  accuracy:79.0922751730937\n",
      "Epoch128  loss:0.6062446184456348  accuracy:79.21227531382436\n",
      "Epoch129  loss:0.6019020341336727  accuracy:79.340237541947\n",
      "Epoch130  loss:0.5984291970729828  accuracy:78.90578139786545\n",
      "Epoch131  loss:0.5891099803149701  accuracy:79.87451407541685\n",
      "Epoch132  loss:0.5996723435819149  accuracy:79.07642273026876\n",
      "Epoch133  loss:0.5674070328474045  accuracy:80.20915812733418\n",
      "Epoch134  loss:0.5703518092632295  accuracy:80.40400511710416\n",
      "Epoch135  loss:0.5696474436670541  accuracy:80.51489634624741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch136  loss:0.5796826533973217  accuracy:80.53007859339654\n",
      "Epoch137  loss:0.5856906823813914  accuracy:79.65144840765711\n",
      "Epoch138  loss:0.5534596875309944  accuracy:80.74866704596833\n",
      "Epoch139  loss:0.5669529892504214  accuracy:80.49892323090884\n",
      "Epoch140  loss:0.5609595589339734  accuracy:81.45782055394514\n",
      "Epoch141  loss:0.5606307744979858  accuracy:80.76451924095167\n",
      "Epoch142  loss:0.5634394310414791  accuracy:80.82244028038639\n",
      "Epoch143  loss:0.5642015404999255  accuracy:80.54957102188199\n",
      "Epoch144  loss:0.5471700139343739  accuracy:80.8183892142976\n",
      "Epoch145  loss:0.546561662107706  accuracy:81.59185369558156\n",
      "Epoch146  loss:0.5405227988958359  accuracy:82.00984964203249\n",
      "Epoch147  loss:0.5351891789585351  accuracy:81.57637262222866\n",
      "Epoch148  loss:0.5323561072349549  accuracy:81.54828398452685\n",
      "Epoch149  loss:0.5498816054314375  accuracy:81.17140969468868\n",
      "Epoch150  loss:0.543246153742075  accuracy:81.85195039393878\n",
      "Epoch151  loss:0.536090199649334  accuracy:81.37601483994256\n",
      "Epoch152  loss:0.5397442515939475  accuracy:81.81906277261304\n",
      "Epoch153  loss:0.5412884809076787  accuracy:81.9459292650779\n",
      "Epoch154  loss:0.540543071180582  accuracy:81.58327254203606\n",
      "Epoch155  loss:0.538144912943244  accuracy:81.87897878511134\n",
      "Epoch156  loss:0.5314601324498652  accuracy:82.29288426175751\n",
      "Epoch157  loss:0.511294472962618  accuracy:82.60905739293075\n",
      "Epoch158  loss:0.5218807440251112  accuracy:81.92929298511233\n",
      "Epoch159  loss:0.534728578478098  accuracy:82.54788800137597\n",
      "Epoch160  loss:0.5324648324400186  accuracy:81.99623635547715\n",
      "Epoch161  loss:0.5199066899716854  accuracy:82.30436612374135\n",
      "Epoch162  loss:0.5154178611934185  accuracy:82.44644040356208\n",
      "Epoch163  loss:0.5104463685303926  accuracy:82.72240536624713\n",
      "Epoch164  loss:0.5144842308014631  accuracy:82.5956871441191\n",
      "Epoch165  loss:0.5028781805187464  accuracy:83.01995340995104\n",
      "Epoch166  loss:0.5028314281255005  accuracy:83.09640009694411\n",
      "Epoch167  loss:0.5076414484530688  accuracy:82.83623453341505\n",
      "Epoch168  loss:0.5146439278498292  accuracy:82.35498809744793\n",
      "Epoch169  loss:0.5073189668357372  accuracy:82.90865594328524\n",
      "Epoch170  loss:0.5117787666618824  accuracy:83.13358318263931\n",
      "Epoch171  loss:0.5054816978052258  accuracy:82.99482884287673\n",
      "Epoch172  loss:0.4922658775001765  accuracy:83.56455358404142\n",
      "Epoch173  loss:0.49304494969546786  accuracy:83.74695010731406\n",
      "Epoch174  loss:0.5051521755754949  accuracy:83.01399634413109\n",
      "Epoch175  loss:0.4854938773438335  accuracy:83.69742372971842\n",
      "Epoch176  loss:0.486751814931631  accuracy:83.89482959286899\n",
      "Epoch177  loss:0.49648885913193225  accuracy:83.4194147881566\n",
      "Epoch178  loss:0.49332521781325345  accuracy:83.67060419367976\n",
      "Epoch179  loss:0.4914056282490492  accuracy:83.83040338486681\n",
      "Epoch180  loss:0.5006103225052356  accuracy:83.61832091800514\n",
      "Epoch181  loss:0.49791409857571123  accuracy:83.62846542776447\n",
      "Epoch182  loss:0.4855664558708667  accuracy:83.39935082851326\n",
      "Epoch183  loss:0.48427927084267136  accuracy:83.89903915294381\n",
      "Epoch184  loss:0.4870031701400877  accuracy:83.47159816904193\n",
      "Epoch185  loss:0.47656920105218886  accuracy:84.01478102673924\n",
      "Epoch186  loss:0.482147237379104  accuracy:83.98395216041477\n",
      "Epoch187  loss:0.47945913579314947  accuracy:84.13616937633427\n",
      "Epoch188  loss:0.47016188921406865  accuracy:84.21811401580257\n",
      "Epoch189  loss:0.48029008843004706  accuracy:83.69300209135321\n",
      "Epoch190  loss:0.4861991558223963  accuracy:84.22900560373971\n",
      "Epoch191  loss:0.4723567822948098  accuracy:84.2715104642386\n",
      "Epoch192  loss:0.4776231601834297  accuracy:84.35415473403228\n",
      "Epoch193  loss:0.4707754550501704  accuracy:84.52437197855194\n",
      "Epoch194  loss:0.49194614496082073  accuracy:83.78323858726942\n",
      "Epoch195  loss:0.47644669841974985  accuracy:84.54952348469288\n",
      "Epoch196  loss:0.47708551054820414  accuracy:84.3671575193449\n",
      "Epoch197  loss:0.46435740566812456  accuracy:84.42597325764578\n",
      "Epoch198  loss:0.46329705696553  accuracy:84.51293486364854\n",
      "Epoch199  loss:0.4706812674179673  accuracy:84.3305036995114\n",
      "Epoch200  loss:0.465438111498952  accuracy:84.89609303740545\n",
      "Epoch201  loss:0.46997352242469786  accuracy:84.76585199157775\n",
      "Epoch202  loss:0.4592797180637718  accuracy:84.96374050904312\n",
      "Epoch204  loss:0.4624552838504314  accuracy:85.07894663082625\n",
      "Epoch205  loss:0.4753018692135811  accuracy:84.70718131462779\n",
      "Epoch206  loss:0.4834814041852951  accuracy:84.61378087115338\n",
      "Epoch207  loss:0.4599652714096009  accuracy:84.65230696094099\n",
      "Epoch208  loss:0.4742705618031324  accuracy:84.12658807660407\n",
      "Epoch209  loss:0.4773358642123639  accuracy:84.91771142324417\n",
      "Epoch210  loss:0.4819772569462657  accuracy:84.23996151070989\n",
      "Epoch211  loss:0.45147116146981714  accuracy:85.29166871352263\n",
      "Epoch212  loss:0.45640045097097753  accuracy:84.93162991197458\n",
      "Epoch213  loss:0.47405747743323445  accuracy:84.4578391312178\n",
      "Epoch214  loss:0.4804250612854958  accuracy:84.18818358752566\n",
      "Epoch215  loss:0.4571012052707375  accuracy:85.02350603513293\n",
      "Epoch216  loss:0.44764833264052867  accuracy:85.21210756471223\n",
      "Epoch217  loss:0.4515264329034835  accuracy:85.29979846119103\n",
      "Epoch218  loss:0.4528380274772645  accuracy:85.27248066129653\n",
      "Epoch219  loss:0.45490379957482213  accuracy:85.40001289910042\n",
      "Epoch220  loss:0.46259298061486326  accuracy:84.76863905565256\n",
      "Epoch221  loss:0.4738765422254801  accuracy:84.66873608291097\n",
      "Epoch222  loss:0.46204400081187486  accuracy:84.64359885306533\n",
      "Epoch223  loss:0.46723108603619035  accuracy:84.97091316765099\n",
      "Epoch224  loss:0.45971536403521895  accuracy:84.96795417892199\n",
      "Epoch225  loss:0.46739611799130204  accuracy:84.84291984070587\n",
      "Epoch226  loss:0.4644261337583884  accuracy:85.01033364860143\n",
      "Epoch227  loss:0.456115815625526  accuracy:85.13134833297383\n",
      "Epoch228  loss:0.46468927116366104  accuracy:85.33545799952171\n",
      "Epoch229  loss:0.4800174094736577  accuracy:84.9734665254447\n",
      "Epoch230  loss:0.4513189420918933  accuracy:85.424976900427\n",
      "Epoch231  loss:0.4723574473522603  accuracy:85.14899601915536\n",
      "Epoch232  loss:0.4503699788256199  accuracy:85.57156775131658\n",
      "Epoch233  loss:0.4758605245966464  accuracy:84.90067242345253\n",
      "Epoch234  loss:0.4706163251074031  accuracy:85.3781543266772\n",
      "Epoch235  loss:0.46131222784169956  accuracy:85.36357249248255\n",
      "Epoch236  loss:0.4582228825427592  accuracy:85.54090192418961\n",
      "Epoch237  loss:0.46996763087227017  accuracy:85.621971831617\n",
      "Epoch238  loss:0.46758796387002804  accuracy:85.31814721873164\n",
      "Epoch239  loss:0.459401940740645  accuracy:85.4014899574423\n",
      "Epoch240  loss:0.4596990822057706  accuracy:85.48705136157268\n",
      "Epoch241  loss:0.46857665546704075  accuracy:85.50056804487168\n",
      "Epoch242  loss:0.4448358849040232  accuracy:85.83722105984104\n",
      "Epoch243  loss:0.4604564720648341  accuracy:85.602275830987\n",
      "Epoch244  loss:0.4597049123840406  accuracy:85.3989891773731\n",
      "Epoch245  loss:0.4568882484600181  accuracy:85.46885942583775\n",
      "Epoch246  loss:0.46040915577177666  accuracy:85.59862248324397\n",
      "Epoch247  loss:0.4551987808430567  accuracy:85.53969739241725\n",
      "Epoch248  loss:0.46166569526540113  accuracy:85.9039015245352\n",
      "Epoch249  loss:0.45964500075206155  accuracy:85.52686397719029\n",
      "Epoch250  loss:0.47623257865197954  accuracy:84.88230691102277\n",
      "Epoch251  loss:0.46643415773287417  accuracy:85.42789296481048\n",
      "Epoch252  loss:0.4506124377367087  accuracy:86.10644659499509\n",
      "Epoch253  loss:0.47031744748819626  accuracy:85.32978526689772\n",
      "Epoch254  loss:0.4482301251380705  accuracy:85.89097669660674\n",
      "Epoch255  loss:0.4575929723345326  accuracy:85.89926393225868\n",
      "Epoch256  loss:0.4487873346428387  accuracy:85.84508576189803\n",
      "Epoch257  loss:0.46039100627531293  accuracy:86.29784127011993\n",
      "Epoch258  loss:0.45844507029396475  accuracy:85.72782473160031\n",
      "Epoch259  loss:0.45506950309500094  accuracy:85.70922411996526\n",
      "Epoch260  loss:0.45921387108974154  accuracy:86.0719487093417\n",
      "Epoch261  loss:0.47460558414459236  accuracy:85.27642268203687\n",
      "Epoch262  loss:0.4616024005110375  accuracy:85.47203341022797\n",
      "Epoch263  loss:0.4662460166233359  accuracy:85.8171922808615\n",
      "Epoch264  loss:0.47723456021485616  accuracy:85.63378833304475\n",
      "Epoch265  loss:0.4625054145697504  accuracy:85.67440006724814\n",
      "Epoch266  loss:0.46635549854254343  accuracy:85.99234186124032\n",
      "Epoch267  loss:0.4620407911075745  accuracy:86.30927097006845\n",
      "Epoch268  loss:0.4645543488528347  accuracy:85.72132646865835\n",
      "Epoch269  loss:0.4615827330737374  accuracy:86.29804562340861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch270  loss:0.44437605340499436  accuracy:86.47847570900414\n",
      "Epoch271  loss:0.45535097531974317  accuracy:86.02835335391607\n",
      "Epoch272  loss:0.47154198766220357  accuracy:86.11849676444882\n",
      "Epoch273  loss:0.4520058317109942  accuracy:86.39187155921127\n",
      "Epoch274  loss:0.47542398951482034  accuracy:86.01126743207797\n",
      "Epoch275  loss:0.4732007674814667  accuracy:85.77336743107176\n",
      "Epoch276  loss:0.5008549077203498  accuracy:85.17629887110404\n",
      "Epoch277  loss:0.4731228875461964  accuracy:86.14723478790411\n",
      "Epoch278  loss:0.4815013334387914  accuracy:85.75785603110766\n",
      "Epoch279  loss:0.4560053731780499  accuracy:86.21004392283096\n",
      "Epoch280  loss:0.48315956395235843  accuracy:85.72769051274933\n",
      "Epoch281  loss:0.4670293254985154  accuracy:86.238997816253\n",
      "Epoch282  loss:0.47970369779504834  accuracy:85.61932564882473\n",
      "Epoch283  loss:0.4832969518029131  accuracy:85.91820075542509\n",
      "Epoch284  loss:0.4801658133044839  accuracy:85.95005221653949\n",
      "Epoch285  loss:0.4727035462856292  accuracy:86.4800760640816\n",
      "Epoch286  loss:0.4653440713474993  accuracy:86.14339817518966\n",
      "Epoch287  loss:0.47608853115234523  accuracy:86.2605298422337\n",
      "Epoch288  loss:0.4602707912912592  accuracy:86.26591428730472\n",
      "Epoch289  loss:0.4839554832346039  accuracy:85.72839732662835\n",
      "Epoch290  loss:0.4875848034673254  accuracy:86.24841780007795\n",
      "Epoch291  loss:0.4697926545493829  accuracy:86.56932992198205\n",
      "Epoch292  loss:0.4800100599211874  accuracy:86.10802612507175\n",
      "Epoch293  loss:0.470942975631624  accuracy:86.67209613376465\n",
      "Epoch294  loss:0.49821516603697086  accuracy:85.71494128726054\n",
      "Epoch295  loss:0.48489533276733704  accuracy:86.17092780901308\n",
      "Epoch296  loss:0.4818908618195564  accuracy:86.56598744925603\n",
      "Epoch297  loss:0.4885576912402028  accuracy:86.44286285184202\n",
      "Epoch298  loss:0.47744683865457777  accuracy:86.5146466016389\n",
      "Epoch299  loss:0.4797766202262209  accuracy:86.69844036958295\n",
      "Epoch300  loss:0.48007595657836644  accuracy:86.2087537930941\n",
      "Epoch301  loss:0.4841589520336129  accuracy:86.33547757909642\n",
      "Epoch302  loss:0.47728213284717635  accuracy:86.46409301362796\n",
      "Epoch303  loss:0.503299254726153  accuracy:86.12440883346859\n",
      "Epoch304  loss:0.48453621617518366  accuracy:86.32105447164784\n",
      "Epoch305  loss:0.5007039943469863  accuracy:86.116145372338\n",
      "Epoch306  loss:0.5026448097487446  accuracy:85.8866618703608\n",
      "Epoch307  loss:0.492595619126223  accuracy:86.28817009526597\n",
      "Epoch308  loss:0.49682634758064514  accuracy:86.58784662625965\n",
      "Epoch309  loss:0.47279844903241613  accuracy:86.8899867219561\n",
      "Epoch310  loss:0.49162302975128114  accuracy:86.47012166307101\n",
      "Epoch311  loss:0.49618046570058144  accuracy:86.25169468419291\n",
      "Epoch312  loss:0.4936577735003084  accuracy:86.33117086235212\n",
      "Epoch313  loss:0.4899835806394549  accuracy:86.79362367971117\n",
      "Epoch314  loss:0.4934280332639901  accuracy:86.21810340502984\n",
      "Epoch315  loss:0.48004397814074756  accuracy:86.9512351535027\n",
      "Epoch316  loss:0.49306202768348145  accuracy:86.44241024787814\n",
      "Epoch317  loss:0.48029471920017386  accuracy:86.60209567400085\n",
      "Epoch318  loss:0.5008860500995069  accuracy:86.2552884752223\n",
      "Epoch319  loss:0.504588907235302  accuracy:86.27474851653508\n",
      "Epoch320  loss:0.48649437131825835  accuracy:86.8415293237968\n",
      "Epoch321  loss:0.49430955860298126  accuracy:86.36930078687732\n",
      "Epoch322  loss:0.49231518122833223  accuracy:86.79812321934149\n",
      "Epoch323  loss:0.5155701222654897  accuracy:86.45990139104813\n",
      "Epoch324  loss:0.5017626042943448  accuracy:86.47704876970023\n",
      "Epoch325  loss:0.5009852013761701  accuracy:86.39747412282902\n",
      "Epoch326  loss:0.5043901931632718  accuracy:86.49519063363104\n",
      "Epoch327  loss:0.5220948034606409  accuracy:86.59949509765352\n",
      "Epoch328  loss:0.5022692190483213  accuracy:86.63700593145593\n",
      "Epoch329  loss:0.5234910510444478  accuracy:86.5053762915974\n",
      "Epoch330  loss:0.5328594032984257  accuracy:86.21951063646496\n",
      "Epoch331  loss:0.5047182148249704  accuracy:86.96380626399758\n",
      "Epoch332  loss:0.5157324785424863  accuracy:86.15071250416635\n",
      "Epoch333  loss:0.49931297258081025  accuracy:86.68857364435873\n",
      "Epoch334  loss:0.5160728807964915  accuracy:86.34874449387063\n",
      "Epoch335  loss:0.5003981634902402  accuracy:86.91041983084389\n",
      "Epoch336  loss:0.5218590810892465  accuracy:86.5071523071636\n",
      "Epoch337  loss:0.4975136681994172  accuracy:86.81106887051148\n",
      "Epoch338  loss:0.5188624767120927  accuracy:86.73232315352305\n",
      "Epoch339  loss:0.4903698303129204  accuracy:87.10887055430446\n",
      "Epoch340  loss:0.5322371087386273  accuracy:86.41350803247069\n",
      "Epoch341  loss:0.533202648954466  accuracy:86.10361554229847\n",
      "Epoch342  loss:0.5245182568185556  accuracy:86.55449209265305\n",
      "Epoch343  loss:0.5175579539150931  accuracy:87.02073168554907\n",
      "Epoch344  loss:0.5167861219242695  accuracy:86.56932489238436\n",
      "Epoch345  loss:0.4990972239174879  accuracy:87.42356267053405\n",
      "Epoch346  loss:0.5256490044390376  accuracy:86.79832005736922\n",
      "Epoch347  loss:0.5484881272539498  accuracy:86.09224801060785\n",
      "Epoch348  loss:0.5143761879147859  accuracy:86.83027831783997\n",
      "Epoch349  loss:0.5139019198981258  accuracy:86.84695664245115\n",
      "Epoch350  loss:0.5121249995342623  accuracy:87.00281652778324\n",
      "Epoch351  loss:0.5485567489638925  accuracy:86.81285866479253\n",
      "Epoch352  loss:0.5223830797942356  accuracy:86.50152499412965\n",
      "Epoch353  loss:0.5233893885610086  accuracy:86.80180381551489\n",
      "Epoch354  loss:0.5312165774359529  accuracy:86.64287846748707\n",
      "Epoch355  loss:0.5204625397291238  accuracy:86.69013598236134\n",
      "Epoch356  loss:0.5183418940752745  accuracy:87.30385010234764\n",
      "Epoch357  loss:0.5243194093844067  accuracy:86.94054929126027\n",
      "Epoch358  loss:0.5374593321310386  accuracy:86.62705529168848\n",
      "Epoch359  loss:0.5107968577936844  accuracy:87.04653611381906\n",
      "Epoch360  loss:0.5436737659294522  accuracy:86.80624461510146\n",
      "Epoch361  loss:0.5363047032523355  accuracy:86.92414457749615\n",
      "Epoch362  loss:0.5380460808984935  accuracy:86.78774475037628\n",
      "Epoch363  loss:0.5566700987205422  accuracy:86.50161777688338\n",
      "Epoch364  loss:0.5518523360449762  accuracy:86.45679845464755\n",
      "Epoch365  loss:0.5672652777284384  accuracy:86.46593956542073\n",
      "Epoch366  loss:0.5379820588088476  accuracy:87.1629324218256\n",
      "Epoch367  loss:0.5209368773867027  accuracy:86.89855421092544\n",
      "Epoch368  loss:0.5623905707150698  accuracy:86.0617964795307\n",
      "Epoch369  loss:0.5266002863606675  accuracy:87.00924640434432\n",
      "Epoch370  loss:0.5512601800250536  accuracy:86.54173232417398\n",
      "Epoch371  loss:0.5539149112991254  accuracy:86.61890066132293\n",
      "Epoch372  loss:0.5482136799939326  accuracy:86.92756829359726\n",
      "Epoch373  loss:0.5222965361414026  accuracy:87.00396501741622\n",
      "Epoch374  loss:0.5484384922136087  accuracy:86.55914731603909\n",
      "Epoch375  loss:0.5406203089747579  accuracy:86.84091792353641\n",
      "Epoch376  loss:0.546143932035193  accuracy:86.26314283395541\n",
      "Epoch377  loss:0.5587004713004717  accuracy:86.48713026529387\n",
      "Epoch378  loss:0.5623646869673393  accuracy:86.71893319411637\n",
      "Epoch379  loss:0.5556156514328904  accuracy:86.43622811117095\n",
      "Epoch380  loss:0.5508834686941555  accuracy:86.9990200933635\n",
      "Epoch381  loss:0.5805035493535797  accuracy:86.42053223000269\n",
      "Epoch382  loss:0.5612895132973791  accuracy:86.72280365836464\n",
      "Epoch383  loss:0.5345584699132815  accuracy:87.0699132783955\n",
      "Epoch384  loss:0.5663605079054832  accuracy:87.09549878474263\n",
      "Epoch385  loss:0.5332675203972029  accuracy:86.7043082473008\n",
      "Epoch386  loss:0.5565201848744379  accuracy:86.87672176055628\n",
      "Epoch387  loss:0.5605368152225765  accuracy:86.87543234971699\n",
      "Epoch388  loss:0.5677770846174097  accuracy:86.84659581164979\n",
      "Epoch389  loss:0.5737170137441922  accuracy:86.89698469482157\n",
      "Epoch390  loss:0.5560045785648982  accuracy:86.94442341532479\n",
      "Epoch391  loss:0.5680476047028606  accuracy:86.60882178616926\n",
      "Epoch392  loss:0.557200800391729  accuracy:87.25823051395548\n",
      "Epoch393  loss:0.5638849228468189  accuracy:87.11938203341953\n",
      "Epoch394  loss:0.5605984206544234  accuracy:87.09243813572027\n",
      "Epoch395  loss:0.5469412598031339  accuracy:87.31525385089634\n",
      "Epoch396  loss:0.5868378820232465  accuracy:86.43749314076997\n",
      "Epoch397  loss:0.5705937003018333  accuracy:86.99993978613823\n",
      "Epoch398  loss:0.5632795085810357  accuracy:86.76040672781771\n",
      "Epoch399  loss:0.5858347207305316  accuracy:86.71643606408303\n",
      "Epoch400  loss:0.5669342246348607  accuracy:86.7227399387714\n",
      "Epoch401  loss:0.5596300601710027  accuracy:87.29499694735605\n",
      "Epoch402  loss:0.5961995139834472  accuracy:86.51591030610719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch403  loss:0.5868104180935916  accuracy:87.02132015836742\n",
      "Epoch404  loss:0.5608155220726303  accuracy:86.95229877850024\n",
      "Epoch405  loss:0.5801585741326108  accuracy:86.81905584016566\n",
      "Epoch406  loss:0.5710652507838575  accuracy:86.84129113910335\n",
      "Epoch407  loss:0.5697285419359104  accuracy:87.01810092529102\n",
      "Epoch408  loss:0.5879724373866339  accuracy:86.31863040867283\n",
      "Epoch409  loss:0.5800958991044637  accuracy:86.82628398159076\n",
      "Epoch410  loss:0.5792410135054524  accuracy:86.56429133962605\n",
      "Epoch411  loss:0.5939625043916751  accuracy:86.89536547077265\n",
      "Epoch412  loss:0.5771984368558776  accuracy:86.78703160129477\n",
      "Epoch413  loss:0.5960204810899085  accuracy:86.29602764110963\n",
      "Epoch414  loss:0.576089938640598  accuracy:87.00854597038676\n",
      "Epoch415  loss:0.5751138768967395  accuracy:86.86047550721054\n",
      "Epoch416  loss:0.5990292664659138  accuracy:87.03866149371643\n",
      "Epoch417  loss:0.5537410266637608  accuracy:87.52204429783774\n",
      "Epoch418  loss:0.5974583872895891  accuracy:86.74954380339076\n",
      "Epoch419  loss:0.5734751433111341  accuracy:87.54690715968293\n",
      "Epoch420  loss:0.5925052638383932  accuracy:87.26185286764769\n",
      "Epoch421  loss:0.5807579305826949  accuracy:87.09305279675128\n",
      "Epoch422  loss:0.6021691746544092  accuracy:86.93928886044503\n",
      "Epoch423  loss:0.6046287812292277  accuracy:86.80213236768572\n",
      "Epoch424  loss:0.5934538930648329  accuracy:86.92183783214976\n",
      "Epoch425  loss:0.5955956219731888  accuracy:86.895180112509\n",
      "Epoch426  loss:0.5829136189859128  accuracy:87.31664358420733\n",
      "Epoch427  loss:0.5845977811717603  accuracy:87.05846413810772\n",
      "Epoch428  loss:0.5998218784050551  accuracy:86.55115952524282\n",
      "Epoch429  loss:0.6131247594944852  accuracy:87.193091923225\n",
      "Epoch430  loss:0.5630788084548841  accuracy:87.33425412067503\n",
      "Epoch431  loss:0.6046670242678372  accuracy:86.89458322458908\n",
      "Epoch432  loss:0.5826863592055815  accuracy:86.79112921883086\n",
      "Epoch433  loss:0.6065092146796814  accuracy:87.28607293385795\n",
      "Epoch434  loss:0.5983512083443203  accuracy:86.89695728732256\n",
      "Epoch435  loss:0.6104818247258293  accuracy:87.2942931194444\n",
      "Epoch436  loss:0.6017464876174629  accuracy:86.92811875296609\n",
      "Epoch437  loss:0.6073168569710106  accuracy:86.79810911941966\n",
      "Epoch438  loss:0.6068183852359652  accuracy:86.7814574980205\n",
      "Epoch439  loss:0.5930152617343537  accuracy:87.35087431869827\n",
      "Epoch440  loss:0.6162026062587984  accuracy:87.41502631467839\n",
      "Epoch441  loss:0.6318826980869061  accuracy:86.45156391987723\n",
      "Epoch442  loss:0.6318682275712412  accuracy:86.83336229141717\n",
      "Epoch443  loss:0.6098351947938455  accuracy:86.91419951989769\n",
      "Epoch444  loss:0.5931953876679472  accuracy:87.2743398834327\n",
      "Epoch445  loss:0.5869662279008482  accuracy:87.75192037259039\n",
      "Epoch446  loss:0.5927053600466479  accuracy:87.54597012536271\n",
      "Epoch447  loss:0.6209794506222807  accuracy:87.38099094593616\n",
      "Epoch448  loss:0.6062607407484394  accuracy:87.51558319639193\n",
      "Epoch449  loss:0.6041249856352566  accuracy:87.33862608887476\n",
      "Epoch450  loss:0.6323151670396292  accuracy:87.19648128887489\n",
      "Epoch451  loss:0.6209070033705757  accuracy:87.26453393420482\n",
      "Epoch452  loss:0.6098201095937383  accuracy:86.97591002995411\n",
      "Epoch453  loss:0.6126787163295149  accuracy:87.30186479749705\n",
      "Epoch454  loss:0.6429769389329465  accuracy:87.29283349772918\n",
      "Epoch455  loss:0.6128702912596056  accuracy:87.18859015439445\n",
      "Epoch456  loss:0.6217695437371019  accuracy:86.96576706252749\n",
      "Epoch457  loss:0.5992803059517512  accuracy:87.58502486943803\n",
      "Epoch458  loss:0.6321629971265794  accuracy:86.86734702887824\n",
      "Epoch459  loss:0.637733209138969  accuracy:86.86834605405177\n",
      "Epoch460  loss:0.6321111298987944  accuracy:86.79309332425046\n",
      "Epoch461  loss:0.6320221301844866  accuracy:87.13174918574715\n",
      "Epoch462  loss:0.6180688243421173  accuracy:87.26435826020449\n",
      "Epoch463  loss:0.6363213978703016  accuracy:86.91629660993374\n",
      "Epoch464  loss:0.6251628167345188  accuracy:87.12650359059742\n",
      "Epoch465  loss:0.6390105992547659  accuracy:87.04435164527163\n",
      "Epoch466  loss:0.6180008240043962  accuracy:87.05073553805246\n",
      "Epoch467  loss:0.5834478430151192  accuracy:87.61286002883423\n",
      "Epoch468  loss:0.6264197016949765  accuracy:87.10824197590472\n",
      "Epoch469  loss:0.6247858349292075  accuracy:86.94681536804066\n",
      "Epoch470  loss:0.6257167136471253  accuracy:87.3072256854787\n",
      "Epoch471  loss:0.622876195603567  accuracy:87.35519708475498\n",
      "Epoch472  loss:0.6170749925076251  accuracy:87.44823215131396\n",
      "Epoch473  loss:0.6294099664681199  accuracy:87.49165791078696\n",
      "Epoch474  loss:0.638559205084985  accuracy:87.143909548198\n",
      "Epoch475  loss:0.6510218991463261  accuracy:87.38642227343794\n",
      "Epoch476  loss:0.6434501700016199  accuracy:86.96777726735885\n",
      "Epoch477  loss:0.6284263223404879  accuracy:87.14014174741192\n",
      "Epoch478  loss:0.6409538835120656  accuracy:87.40173360294823\n",
      "Epoch479  loss:0.6215081329690294  accuracy:87.23832691178305\n",
      "Epoch480  loss:0.6224473215639378  accuracy:87.47244533529273\n",
      "Epoch481  loss:0.6414741703760227  accuracy:87.19260740265292\n",
      "Epoch482  loss:0.6419550314544294  accuracy:87.46242753758901\n",
      "Epoch483  loss:0.6224339559591497  accuracy:87.62857215843616\n",
      "Epoch484  loss:0.6551404493729934  accuracy:87.08866453572608\n",
      "Epoch485  loss:0.6210350289906955  accuracy:87.31814687382787\n",
      "Epoch486  loss:0.6244429118860579  accuracy:87.40449811520338\n",
      "Epoch487  loss:0.6135939269281152  accuracy:87.99074530016036\n",
      "Epoch488  loss:0.6465890884338023  accuracy:87.38822675353093\n",
      "Epoch489  loss:0.6227144080738072  accuracy:87.67697288871355\n",
      "Epoch490  loss:0.6410648427903112  accuracy:87.42929920666181\n",
      "Epoch491  loss:0.642724316567174  accuracy:87.669082676336\n",
      "Epoch492  loss:0.6444925837684423  accuracy:87.44231934674032\n",
      "Epoch493  loss:0.6511644490061997  accuracy:87.33282449181576\n",
      "Epoch494  loss:0.6349433395080268  accuracy:87.95323691495553\n",
      "Epoch495  loss:0.6439277186914296  accuracy:87.20429606086478\n",
      "Epoch496  loss:0.6550635381338907  accuracy:87.42942890779884\n",
      "Epoch497  loss:0.6342849794287758  accuracy:87.70258982190286\n",
      "Epoch498  loss:0.6361996253195684  accuracy:87.87951090243651\n",
      "Epoch499  loss:0.6527372971176107  accuracy:87.65030530309595\n",
      "Epoch500  loss:0.6510340667678974  accuracy:87.30619215111598\n"
     ]
    }
   ],
   "source": [
    "server = Server()\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset)\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "acc_valid = []\n",
    "loss_valid = []\n",
    "\n",
    "early_stopping = Early_Stopping(args.partience)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(args.global_epochs):\n",
    "  sample_worker = server.sample_worker(workers)\n",
    "  server.send_model(sample_worker)\n",
    "\n",
    "  acc_train_avg = 0.0\n",
    "  loss_train_avg = 0.0\n",
    "  acc_valid_avg = 0.0\n",
    "  loss_valid_avg = 0.0\n",
    "  for worker in sample_worker:\n",
    "    acc_train_tmp,loss_train_tmp,acc_valid_tmp,loss_valid_tmp = worker.local_train()\n",
    "    acc_train_avg += acc_train_tmp/len(sample_worker)\n",
    "    loss_train_avg += loss_train_tmp/len(sample_worker)\n",
    "    acc_valid_avg += acc_valid_tmp/len(sample_worker)\n",
    "    loss_valid_avg += loss_valid_tmp/len(sample_worker)\n",
    "  server.aggregate_model(sample_worker)\n",
    "  '''\n",
    "  server.model.to(args.device)\n",
    "  for worker in workers:\n",
    "    acc_valid_tmp,loss_valid_tmp = test(server.model,args.criterion,worker.valloader)\n",
    "    acc_valid_avg += acc_valid_tmp/len(workers)\n",
    "    loss_valid_avg += loss_valid_tmp/len(workers)\n",
    "  server.model.to('cpu')\n",
    "  '''\n",
    "  print('Epoch{}  loss:{}  accuracy:{}'.format(epoch+1,loss_valid_avg,acc_valid_avg))\n",
    "  acc_train.append(acc_train_avg)\n",
    "  loss_train.append(loss_train_avg)\n",
    "  acc_valid.append(acc_valid_avg)\n",
    "  loss_valid.append(loss_valid_avg)\n",
    "\n",
    "  if early_stopping.validate(loss_valid_avg):\n",
    "    print('Early Stop')\n",
    "    break\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "mi_uceyoptLP",
    "outputId": "bc067e09-01bc-4e65-daf9-ac2f42373cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 accuracy:77.11213517665131  loss:1.0882105827331543\n",
      "Worker2 accuracy:80.32786885245902  loss:0.9302712678909302\n",
      "Worker3 accuracy:81.9047619047619  loss:0.9404745101928711\n",
      "Worker4 accuracy:85.11904761904762  loss:0.7348741292953491\n",
      "Worker5 accuracy:83.70731707317073  loss:0.7287482619285583\n",
      "Worker6 accuracy:74.93036211699165  loss:1.183293104171753\n",
      "Worker7 accuracy:78.10526315789474  loss:1.059178352355957\n",
      "Worker8 accuracy:86.35809987819732  loss:0.6604049205780029\n",
      "Worker9 accuracy:83.31210191082802  loss:0.855284571647644\n",
      "Worker10 accuracy:79.76190476190476  loss:0.9293153285980225\n",
      "Worker11 accuracy:83.80952380952381  loss:0.7457153797149658\n",
      "Worker12 accuracy:83.30935251798562  loss:0.744534432888031\n",
      "Worker13 accuracy:89.38775510204081  loss:0.4858578145503998\n",
      "Worker14 accuracy:76.89655172413794  loss:0.977077841758728\n",
      "Worker15 accuracy:77.5599128540305  loss:1.0345617532730103\n",
      "Worker16 accuracy:82.53275109170306  loss:0.8765638470649719\n",
      "Worker17 accuracy:83.01886792452831  loss:0.8060707449913025\n",
      "Worker18 accuracy:80.62157221206581  loss:0.8694987893104553\n",
      "Worker19 accuracy:77.90262172284645  loss:1.0862919092178345\n",
      "Worker20 accuracy:81.70426065162907  loss:0.8041856288909912\n",
      "Test  loss:0.8770206585526467  accuracy:81.36910160311992\n"
     ]
    }
   ],
   "source": [
    "acc_test = []\n",
    "loss_test = []\n",
    "\n",
    "server.model.to(args.device)\n",
    "\n",
    "nums = 0\n",
    "for worker in workers:\n",
    "  nums += worker.test_data_num\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.aggregation_weight = 1.0*worker.test_data_num/nums\n",
    "  acc_tmp,loss_tmp = test(server.model,args.criterion,worker.testloader)\n",
    "  acc_test.append(acc_tmp)\n",
    "  loss_test.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_test_avg = sum(acc_test)/len(acc_test)\n",
    "loss_test_avg = sum(loss_test)/len(loss_test)\n",
    "print('Test  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 Valid accuracy:82.99531981279252  loss:1.0305582284927368\n",
      "Worker1 Test accuracy:82.33486943164363  loss:0.9284118413925171\n",
      "Worker2 Valid accuracy:91.28630705394191  loss:0.5307278633117676\n",
      "Worker2 Test accuracy:90.77868852459017  loss:0.49214184284210205\n",
      "Worker3 Valid accuracy:82.6171875  loss:0.9291512370109558\n",
      "Worker3 Test accuracy:84.0  loss:0.9188185930252075\n",
      "Worker4 Valid accuracy:89.4949494949495  loss:0.4722346067428589\n",
      "Worker4 Test accuracy:92.06349206349206  loss:0.42056524753570557\n",
      "Worker5 Valid accuracy:86.65338645418326  loss:0.453025996633869\n",
      "Worker5 Test accuracy:85.95121951219512  loss:0.5730805546045303\n",
      "Worker6 Valid accuracy:81.25  loss:0.9119798541069031\n",
      "Worker6 Test accuracy:81.05849582172702  loss:1.0125120878219604\n",
      "Worker7 Valid accuracy:89.07922912205568  loss:0.5649206638336182\n",
      "Worker7 Test accuracy:85.26315789473684  loss:0.8916639685630798\n",
      "Worker8 Valid accuracy:92.92803970223325  loss:0.3894939422607422\n",
      "Worker8 Test accuracy:91.10840438489647  loss:0.6179893612861633\n",
      "Worker9 Valid accuracy:93.50649350649351  loss:0.37821853160858154\n",
      "Worker9 Test accuracy:93.2484076433121  loss:0.37815770506858826\n",
      "Worker10 Valid accuracy:94.33198380566802  loss:0.20242498815059662\n",
      "Worker10 Test accuracy:92.46031746031746  loss:0.22912834584712982\n",
      "Worker11 Valid accuracy:87.70226537216828  loss:0.5597181916236877\n",
      "Worker11 Test accuracy:88.88888888888889  loss:0.5921677947044373\n",
      "Worker12 Valid accuracy:85.31571218795888  loss:0.9434713125228882\n",
      "Worker12 Test accuracy:87.05035971223022  loss:0.7284011244773865\n",
      "Worker13 Valid accuracy:93.7888198757764  loss:0.32848209142684937\n",
      "Worker13 Test accuracy:96.3265306122449  loss:0.29934194684028625\n",
      "Worker14 Valid accuracy:88.65248226950355  loss:0.48585569858551025\n",
      "Worker14 Test accuracy:84.82758620689656  loss:0.9441671371459961\n",
      "Worker15 Valid accuracy:89.3569844789357  loss:0.6518839001655579\n",
      "Worker15 Test accuracy:85.40305010893246  loss:0.7545905113220215\n",
      "Worker16 Valid accuracy:88.54625550660793  loss:0.542198121547699\n",
      "Worker16 Test accuracy:84.27947598253274  loss:0.8439626693725586\n",
      "Worker17 Valid accuracy:85.99033816425121  loss:0.7407289743423462\n",
      "Worker17 Test accuracy:85.61320754716981  loss:0.7334834933280945\n",
      "Worker18 Valid accuracy:80.66914498141264  loss:1.1559230089187622\n",
      "Worker18 Test accuracy:79.89031078610603  loss:1.1461684703826904\n",
      "Worker19 Valid accuracy:83.9080459770115  loss:0.7896878719329834\n",
      "Worker19 Test accuracy:80.1498127340824  loss:0.8762527108192444\n",
      "Worker20 Valid accuracy:88.37209302325581  loss:0.6372092366218567\n",
      "Worker20 Test accuracy:86.46616541353383  loss:0.7416951656341553\n",
      "Validation(tune)  loss:0.6348947159920385  accuracy:87.82225191445997\n",
      "Test(tune)  loss:0.7061350286006928  accuracy:86.85812203647643\n"
     ]
    }
   ],
   "source": [
    "acc_tune_test = []\n",
    "loss_tune_test = []\n",
    "acc_tune_valid = []\n",
    "loss_tune_valid = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "    worker.model = copy.deepcopy(server.model)\n",
    "    worker.model = worker.model.to(args.device)\n",
    "    _,_,acc_tmp,loss_tmp = worker.local_train()\n",
    "    acc_tune_valid.append(acc_tmp)\n",
    "    loss_tune_valid.append(loss_tmp)\n",
    "    print('Worker{} Valid accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    \n",
    "    acc_tmp,loss_tmp = test(worker.model,args.criterion,worker.testloader)\n",
    "    acc_tune_test.append(acc_tmp)\n",
    "    loss_tune_test.append(loss_tmp)\n",
    "    print('Worker{} Test accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    worker.model = worker.model.to('cpu')\n",
    "    del worker.model\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_valid_avg = sum(acc_tune_valid)/len(acc_tune_valid)\n",
    "loss_valid_avg = sum(loss_tune_valid)/len(loss_tune_valid)\n",
    "print('Validation(tune)  loss:{}  accuracy:{}'.format(loss_valid_avg,acc_valid_avg))\n",
    "acc_test_avg = sum(acc_tune_test)/len(acc_tune_test)\n",
    "loss_test_avg = sum(loss_tune_test)/len(loss_tune_test)\n",
    "print('Test(tune)  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
