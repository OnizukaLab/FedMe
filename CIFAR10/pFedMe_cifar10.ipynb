{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    # random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 42\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 40\n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 500\n",
    "    self.local_epochs = 2\n",
    "    self.lamda = 15\n",
    "    self.K = 5\n",
    "    self.lr = None\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 500\n",
    "    self.worker_num = 20\n",
    "    self.sample_num = 20\n",
    "    self.unlabeleddata_size = 1000\n",
    "    self.device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    self.alpha_label = 0.5\n",
    "    self.alpha_size = 10\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned value\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = []\n",
    "lr_list.append(10**-3.0)\n",
    "lr_list.append(10**-2.5)\n",
    "lr_list.append(10**-2.0)\n",
    "lr_list.append(10**-1.5)\n",
    "lr_list.append(10**-1.0)\n",
    "lr_list.append(10**-0.5)\n",
    "lr_list.append(10**0.0)\n",
    "lr_list.append(10**0.5)\n",
    "\n",
    "args.lr = lr_list[lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.label[idx]\n",
    "        if self.transform:\n",
    "            out_data = self.transform(out_data)\n",
    "        return out_data, out_label\n",
    "    \n",
    "class DatasetFromSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.subset[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    \n",
    "class GlobalDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,federated_dataset,transform=None):\n",
    "    self.transform = transform\n",
    "    self.data = []\n",
    "    self.label = []\n",
    "    for dataset in federated_dataset:\n",
    "      for (data,label) in dataset:\n",
    "        self.data.append(data)\n",
    "        self.label.append(label)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    out_data = self.data[idx]\n",
    "    out_label = self.label[idx]\n",
    "    if self.transform:\n",
    "        out_data = self.transform(out_data)\n",
    "    return out_data, out_label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "class UnlabeledDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,transform=None):\n",
    "    self.transform = transform\n",
    "    self.data = []\n",
    "    self.target = None\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    out_data = self.data[idx]\n",
    "    out_label = 'unlabeled'\n",
    "    if self.transform:\n",
    "        out_data = self.transform(out_data)\n",
    "    return out_data, out_label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(Centralized=False,unlabeled_data=False):\n",
    "    \n",
    "    transform_train = transforms.Compose([transforms.ToPILImage(),\n",
    "                                    transforms.RandomCrop(32, padding=2),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.491372549, 0.482352941, 0.446666667), (0.247058824, 0.243529412, 0.261568627))])\n",
    "    transform_test = transforms.Compose([transforms.ToPILImage(),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.491372549, 0.482352941, 0.446666667), (0.247058824, 0.243529412, 0.261568627))])\n",
    "\n",
    "    # download train data\n",
    "    all_trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True)\n",
    "    #trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "    # download test data\n",
    "    all_testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True)\n",
    "    #testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "    \n",
    "    ## get unlabeled dataset\n",
    "    if unlabeled_data:\n",
    "        unlabeled_dataset = UnlabeledDataset(transform_test)\n",
    "        idx = sorted(random.sample(range(len(all_trainset)),args.unlabeleddata_size))\n",
    "        unlabeled_dataset.data = np.array([all_trainset.data[i]  for i in idx])\n",
    "        all_trainset.data = np.delete(all_trainset.data,idx,0)\n",
    "        all_trainset.targets = np.delete(all_trainset.targets,idx,0)\n",
    "    all_train_data = np.array(all_trainset.data)\n",
    "    all_train_label = np.array(all_trainset.targets)\n",
    "    all_test_data = np.array(all_testset.data)\n",
    "    all_test_label = np.array(all_testset.targets)\n",
    "    print('Train:{} Test:{}'.format(len(all_train_data),len(all_test_data)))\n",
    "\n",
    "\n",
    "    ## Data size heterogeneity\n",
    "    data_proportions = np.random.dirichlet(np.repeat(args.alpha_size, args.worker_num))\n",
    "    train_data_proportions = np.array([0 for _ in range(args.worker_num)])\n",
    "    test_data_proportions = np.array([0 for _ in range(args.worker_num)])\n",
    "    for i in range(len(data_proportions)):\n",
    "        if i==(len(data_proportions)-1):\n",
    "            train_data_proportions = train_data_proportions.astype('int64')\n",
    "            test_data_proportions = test_data_proportions.astype('int64')\n",
    "            train_data_proportions[-1] = len(all_train_data) - np.sum(train_data_proportions[:-1])\n",
    "            test_data_proportions[-1] = len(all_test_data) - np.sum(test_data_proportions[:-1])\n",
    "        else:\n",
    "            train_data_proportions[i] = (data_proportions[i] * len(all_train_data))\n",
    "            test_data_proportions[i] = (data_proportions[i] * len(all_test_data))\n",
    "    min_size = 0\n",
    "    K = 10\n",
    "\n",
    "    '''\n",
    "    label_list = np.arange(10)\n",
    "    np.random.shuffle(label_list)\n",
    "    '''\n",
    "    label_list = list(range(K))\n",
    "\n",
    "\n",
    "    ## Data distribution heterogeneity\n",
    "    while min_size<10:\n",
    "        idx_train_batch = [[] for _ in range(args.worker_num)]\n",
    "        idx_test_batch = [[] for _ in range(args.worker_num)]\n",
    "        for k in label_list:\n",
    "            proportions_train = np.random.dirichlet(np.repeat(args.alpha_label, args.worker_num))\n",
    "            proportions_test = copy.deepcopy(proportions_train)\n",
    "            idx_k_train = np.where(all_train_label == k)[0]\n",
    "            idx_k_test = np.where(all_test_label == k)[0]\n",
    "            np.random.shuffle(idx_k_train)\n",
    "            np.random.shuffle(idx_k_test)\n",
    "            ## Balance (train)\n",
    "            proportions_train = np.array([p*(len(idx_j)<train_data_proportions[i]) for i,(p,idx_j) in enumerate(zip(proportions_train,idx_train_batch))])\n",
    "            proportions_train = proportions_train/proportions_train.sum()\n",
    "            proportions_train = (np.cumsum(proportions_train)*len(idx_k_train)).astype(int)[:-1]\n",
    "            idx_train_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_train_batch,np.split(idx_k_train,proportions_train))]\n",
    "\n",
    "            ## Balance (test)\n",
    "            proportions_test = np.array([p*(len(idx_j)<test_data_proportions[i]) for i,(p,idx_j) in enumerate(zip(proportions_test,idx_test_batch))])\n",
    "            proportions_test = proportions_test/proportions_test.sum()\n",
    "            proportions_test = (np.cumsum(proportions_test)*len(idx_k_test)).astype(int)[:-1]\n",
    "            idx_test_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_test_batch,np.split(idx_k_test,proportions_test))]\n",
    "\n",
    "            min_size = min([len(idx_j) for idx_j in idx_train_batch])\n",
    "\n",
    "    federated_trainset = []\n",
    "    federated_testset = []\n",
    "    for i in range(args.worker_num):\n",
    "        ## create trainset\n",
    "        data = [all_train_data[idx] for idx in idx_train_batch[i]]\n",
    "        label = [all_train_label[idx] for idx in idx_train_batch[i]]\n",
    "        federated_trainset.append(LocalDataset())\n",
    "        federated_trainset[-1].data = data\n",
    "        federated_trainset[-1].label = label\n",
    "\n",
    "        ## create testset\n",
    "        data = [all_test_data[idx] for idx in idx_test_batch[i]]\n",
    "        label = [all_test_label[idx] for idx in idx_test_batch[i]]\n",
    "        federated_testset.append(LocalDataset())\n",
    "        federated_testset[-1].data = data\n",
    "        federated_testset[-1].label = label\n",
    "\n",
    "        \n",
    "    ## split trainset\n",
    "    federated_valset = [None]*args.worker_num\n",
    "    for i in range(args.worker_num):\n",
    "        n_samples = len(federated_trainset[i])\n",
    "        if n_samples==1:\n",
    "            train_subset = federated_trainset[i]\n",
    "            val_subset = copy.deepcopy(federated_trainset[i])\n",
    "        else:\n",
    "            train_size = int(len(federated_trainset[i]) * 0.8) \n",
    "            val_size = n_samples - train_size \n",
    "            train_subset,val_subset = torch.utils.data.random_split(federated_trainset[i], [train_size, val_size])\n",
    "\n",
    "        federated_trainset[i] = DatasetFromSubset(train_subset)\n",
    "        federated_valset[i] = DatasetFromSubset(val_subset)\n",
    "\n",
    "    ## show data distribution\n",
    "    H = 4\n",
    "    W = 5\n",
    "    fig, axs = plt.subplots(H, W, figsize=(20, 5))\n",
    "    x = np.arange(1,11)\n",
    "    for i, (trainset,valset,testset) in enumerate(zip(federated_trainset,federated_valset,federated_testset)):\n",
    "        bottom = [0]*10\n",
    "        count = [0]*10\n",
    "        for _,label in trainset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        for j in range(len(count)):\n",
    "            bottom[j]+=count[j]\n",
    "        count = [0]*10\n",
    "        for _,label in valset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        for j in range(len(count)):\n",
    "            bottom[j]+=count[j]\n",
    "        count = [0]*10\n",
    "        for _,label in testset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        #axs[int(i/W), i%W].title(\"worker{}\".format(i+1), fontsize=12, color = \"green\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ## get global dataset\n",
    "    if Centralized:\n",
    "        global_trainset = GlobalDataset(federated_trainset)\n",
    "        global_valset = GlobalDataset(federated_valset)\n",
    "        global_testset =  GlobalDataset(federated_testset)\n",
    "        \n",
    "        #show_cifer(global_trainset.data,global_testset.label, cifar10_labels)\n",
    "\n",
    "        global_trainset.transform = transform_train\n",
    "        global_valset.transform = transform_test\n",
    "        global_testset.transform = transform_test\n",
    "\n",
    "        global_trainloader = torch.utils.data.DataLoader(global_trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "        global_valloader = torch.utils.data.DataLoader(global_valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "        global_testloader = torch.utils.data.DataLoader(global_testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "\n",
    "    ## set transform\n",
    "    for i in range(args.worker_num):\n",
    "        federated_trainset[i].transform = transform_train\n",
    "        federated_valset[i].transform = transform_test\n",
    "        federated_testset[i].transform = transform_test\n",
    "    \n",
    "    if Centralized and unlabeled_data:\n",
    "        return federated_trainset,federated_valset,federated_testset,global_trainloader,global_valloader,global_testloader,unlabeled_dataset\n",
    "    if Centralized:\n",
    "        return federated_trainset,federated_valset,federated_testset,global_trainloader,global_valloader,global_testloader\n",
    "    elif unlabeled_data:\n",
    "        return federated_trainset,federated_valset,federated_testset,unlabeled_dataset\n",
    "    else:\n",
    "        return federated_trainset,federated_valset,federated_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train:49000 Test:10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "federated_trainset,federated_valset,federated_testset,unlabeled_dataset = get_dataset(unlabeled_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39191, 9809, 10000]\n"
     ]
    }
   ],
   "source": [
    "total = [0,0,0]\n",
    "for i in range(args.worker_num):\n",
    "    total[0]+=len(federated_trainset[i])\n",
    "    total[1]+=len(federated_valset[i])\n",
    "    total[2]+=len(federated_testset[i])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG model \n",
    "    '''\n",
    "    def __init__(self, features, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            #print(\"in_channels: {}, v: {}\".format(in_channels, v))\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
    "          512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGGConvBlocks(nn.Module):\n",
    "    '''\n",
    "    VGG containers that only contains the conv layers \n",
    "    '''\n",
    "    def __init__(self, features, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class VGGContainer(nn.Module):\n",
    "    '''\n",
    "    VGG model \n",
    "    '''\n",
    "    def __init__(self, features, input_dim, hidden_dims, num_classes=10):\n",
    "        super(VGGContainer, self).__init__()\n",
    "        self.features = features\n",
    "        # note: we hard coded here a bit by assuming we only have two hidden layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dims[1], num_classes),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def matched_vgg11(matched_shapes):\n",
    "    # [(67, 27), (67,), (132, 603), (132,), (260, 1188), (260,), (261, 2340), (261,), (516, 2349), (516,), (517, 4644), (517,), \n",
    "    # (516, 4653), (516,), (516, 4644), (516,), (516, 515), (515,), (515, 515), (515,), (515, 10), (10,)]\n",
    "    processed_matched_shape = [matched_shapes[0][0], \n",
    "                                'M', \n",
    "                                matched_shapes[2][0], \n",
    "                                'M', \n",
    "                                matched_shapes[4][0], \n",
    "                                matched_shapes[6][0], \n",
    "                                'M', \n",
    "                                matched_shapes[8][0], \n",
    "                                matched_shapes[10][0], \n",
    "                                'M', \n",
    "                                matched_shapes[12][0], \n",
    "                                matched_shapes[14][0], \n",
    "                                'M']\n",
    "    return VGGContainer(make_layers(processed_matched_shape), input_dim=matched_shapes[16][0], \n",
    "            hidden_dims=[matched_shapes[16][1], matched_shapes[18][1]], num_classes=10)\n",
    "\n",
    "\n",
    "def vgg11():\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
    "    return VGG(make_layers(cfg['A']))\n",
    "\n",
    "\n",
    "def vgg11_bn(num_classes=10):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['A'], batch_norm=True), num_classes=num_classes)\n",
    "\n",
    "\n",
    "def vgg13():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n",
    "    return VGG(make_layers(cfg['B']))\n",
    "\n",
    "\n",
    "def vgg13_bn():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg16():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n",
    "    return VGG(make_layers(cfg['D']))\n",
    "\n",
    "\n",
    "def vgg16_bn():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n",
    "    return VGG(make_layers(cfg['E']))\n",
    "\n",
    "\n",
    "def vgg19_bn():\n",
    "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['E'], batch_norm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pFedMeOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, lamda=0.1 , mu = 0.001):\n",
    "        #self.local_weight_updated = local_weight # w_i,K\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        defaults = dict(lr=lr, lamda=lamda, mu = mu)\n",
    "        super(pFedMeOptimizer, self).__init__(params, defaults)\n",
    "    \n",
    "    def step(self, local_weight_updated, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure\n",
    "        weight_update = local_weight_updated.copy()\n",
    "        for group in self.param_groups:\n",
    "            for p, localweight in zip( group['params'], weight_update):\n",
    "                localweight.data = localweight.data.to(args.device)\n",
    "                p.data = p.data - group['lr'] * (p.grad.data + group['lamda'] * (p.data - localweight.data) + group['mu']*p.data)\n",
    "                localweight.data = localweight.data.to('cpu')\n",
    "        return  group['params'], loss\n",
    "    \n",
    "    def update_param(self, local_weight_updated, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure\n",
    "        weight_update = local_weight_updated.copy()\n",
    "        for group in self.param_groups:\n",
    "            for p, localweight in zip( group['params'], weight_update):\n",
    "                p.data = localweight.data\n",
    "        #return  p.data\n",
    "        return  group['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self):\n",
    "    self.model = vgg13()\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(federated_trainset[i],federated_valset[i],federated_testset[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "\n",
    "  def send_model(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "\n",
    "    for worker in workers:\n",
    "      worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "      worker.model = copy.deepcopy(self.model)\n",
    "      worker.personalized_model = copy.deepcopy(self.model)\n",
    "      worker.local_model = copy.deepcopy(self.model)\n",
    "\n",
    "  def aggregate_model(self,workers):   \n",
    "    new_params = OrderedDict()\n",
    "    for i,worker in enumerate(workers):\n",
    "      worker_state = worker.model.state_dict()\n",
    "      for key in worker_state.keys():\n",
    "        if i==0:\n",
    "          new_params[key] = worker_state[key]*worker.aggregation_weight\n",
    "        else:\n",
    "          new_params[key] += worker_state[key]*worker.aggregation_weight\n",
    "    self.model.load_state_dict(new_params)\n",
    "    \n",
    "  def send_parameters(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "    for worker in workers:\n",
    "        worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "        worker.set_parameters(self.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,trainset,valset,testset):\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    #self.iter_trainloader = iter(self.trainloader)\n",
    "    self.model = vgg13()\n",
    "    self.local_model = copy.deepcopy(list(self.model.parameters()))\n",
    "    self.persionalized_model = copy.deepcopy(list(self.model.parameters()))\n",
    "    self.persionalized_model_bar = copy.deepcopy(list(self.model.parameters()))\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "    \n",
    "    self.optimizer = pFedMeOptimizer(self.model.parameters(),lr=args.lr,lamda=args.lamda)\n",
    "    \n",
    "  def set_parameters(self, model):\n",
    "    for old_param, new_param, local_param in zip(self.model.parameters(), model.parameters(), self.local_model):\n",
    "        old_param.data = new_param.data.clone()\n",
    "        local_param.data = new_param.data.clone()\n",
    "    #self.local_weight_updated = copy.deepcopy(self.optimizer.param_groups[0]['params'])\n",
    "    \n",
    "  def get_next_train_batch(self):\n",
    "    try:\n",
    "        # Samples a new batch for persionalizing\n",
    "        (X, y) = next(self.iter_trainloader)\n",
    "    except StopIteration:\n",
    "        # restart the generator if the previous generator is exhausted.\n",
    "        self.iter_trainloader = iter(self.trainloader)\n",
    "        (X, y) = next(self.iter_trainloader)\n",
    "    return (X.to(args.device), y.to(args.device))    \n",
    "  '''\n",
    "  def local_train(self):\n",
    "    self.model.train()\n",
    "    optimizer = pFedMeOptimizer(self.model.parameters(),lr=args.lr,lamda=args.lamda)\n",
    "    for epoch in range(args.local_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        data,labels = self.get_next_train_batch()\n",
    "        for i in range(args.K):\n",
    "            self.model = self.model.to(args.device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(data)\n",
    "            loss = args.criterion(outputs,labels)\n",
    "            running_loss += loss.item()\n",
    "            predicted = torch.argmax(outputs,dim=1)\n",
    "            correct += (predicted==labels).sum().item()\n",
    "            count += len(labels)\n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clip)\n",
    "            self.model = self.model.to('cpu')\n",
    "            personal_model,_ = optimizer.step(self.local_model)\n",
    "        \n",
    "        for new_param,localweight in zip(personal_model,self.local_model.parameters()):\n",
    "            localweight.data = localweight.data - args.lamda * args.lr * (localweight.data - new_param.data)    \n",
    "    \n",
    "    del self.model\n",
    "    \n",
    "    update_parameters(self.personalized_model,personal_model)\n",
    "    return 100.0*correct/count,running_loss/len(self.trainloader)\n",
    "  '''\n",
    "\n",
    "  def local_train(self):\n",
    "    self.model.train() \n",
    "    self.model = self.model.to(args.device)       \n",
    "    for epoch in range(args.local_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for (data,labels) in self.trainloader:\n",
    "            self.model.train()\n",
    "            data,labels = Variable(data),Variable(labels)\n",
    "            data,labels = data.to(args.device),labels.to(args.device)\n",
    "            for k in range(args.K):\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(data)\n",
    "                loss = args.criterion(outputs,labels)\n",
    "                if k==(args.K-1):\n",
    "                    running_loss += loss.item()\n",
    "                    predicted = torch.argmax(outputs,dim=1)\n",
    "                    correct += (predicted==labels).sum().item()\n",
    "                    count += len(labels)\n",
    "                loss.backward()\n",
    "                #torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clip)\n",
    "                self.persionalized_model_bar,_ = self.optimizer.step(self.local_model)\n",
    "        \n",
    "            for new_param, localweight in zip(self.persionalized_model_bar, self.local_model):\n",
    "                localweight.data = localweight.data.to(args.device)\n",
    "                localweight.data = localweight.data - args.lamda* args.lr * (localweight.data - new_param.data)\n",
    "                localweight.data = localweight.data.to('cpu')\n",
    "    \n",
    "    self.update_parameters(self.local_model)\n",
    "    \n",
    "    self.model = self.model.to('cpu')\n",
    "    for personal_weight, localweight in zip(self.persionalized_model_bar, self.local_model):\n",
    "        personal_weight.data = personal_weight.data.to('cpu')\n",
    "        localweight.data = localweight.data.to('cpu')\n",
    "        \n",
    "    return 100.0*correct/count,running_loss/len(self.trainloader)\n",
    "\n",
    "\n",
    "  def validate(self):\n",
    "    self.model.eval()\n",
    "    self.update_parameters(self.persionalized_model_bar)\n",
    "    self.model = self.model.to(args.device)\n",
    "    acc,loss = test(self.model,args.criterion,self.valloader)\n",
    "    self.model = self.model.to('cpu')\n",
    "    self.update_parameters(self.local_model)\n",
    "    return acc,loss\n",
    "\n",
    "\n",
    "  def test(self):\n",
    "    self.model.eval()\n",
    "    self.update_parameters(self.persionalized_model_bar)\n",
    "    self.model = self.model.to(args.device)\n",
    "    acc,loss = test(self.model,args.criterion,self.testloader)\n",
    "    self.model = self.model.to('cpu')\n",
    "    self.update_parameters(self.local_model)\n",
    "    return acc,loss\n",
    "\n",
    "\n",
    "  def update_parameters(self, new_params):\n",
    "    for param , new_param in zip(self.model.parameters(), new_params):\n",
    "      param.data = new_param.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7-GY66gROuEU"
   },
   "outputs": [],
   "source": [
    "def train(model,criterion,trainloader,epochs):\n",
    "  optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for (data,labels) in trainloader:\n",
    "      data,labels = Variable(data),Variable(labels)\n",
    "      data,labels = data.to(args.device),labels.to(args.device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(data)\n",
    "      loss = criterion(outputs,labels)\n",
    "      running_loss += loss.item()\n",
    "      predicted = torch.argmax(outputs,dim=1)\n",
    "      correct += (predicted==labels).sum().item()\n",
    "      count += len(labels)\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "      optimizer.step()\n",
    "\n",
    "  return 100.0*correct/count,running_loss/len(trainloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  count = 0\n",
    "  for (data,labels) in testloader:\n",
    "    data,labels = data.to(args.device),labels.to(args.device)\n",
    "    outputs = model(data)\n",
    "    running_loss += criterion(outputs,labels).item()\n",
    "    predicted = torch.argmax(outputs,dim=1)\n",
    "    correct += (predicted==labels).sum().item()\n",
    "    count += len(labels)\n",
    "\n",
    "  accuracy = 100.0*correct/count\n",
    "  loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "  return accuracy,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WMO7_WSLHeGl"
   },
   "outputs": [],
   "source": [
    "class Early_Stopping():\n",
    "  def __init__(self,partience):\n",
    "    self.step = 0\n",
    "    self.loss = float('inf')\n",
    "    self.partience = partience\n",
    "\n",
    "  def validate(self,loss):\n",
    "    if self.loss<loss:\n",
    "      self.step += 1\n",
    "      if self.step>self.partience:\n",
    "        return True\n",
    "    else:\n",
    "      self.step = 0\n",
    "      self.loss = loss\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "-noG_98IR-nZ",
    "outputId": "78a6ebe2-854a-4f83-dc45-5c4ac35b69e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1  loss:2.2669762432575227  accuracy:30.424147730920353\n",
      "Epoch2  loss:2.258585387468338  accuracy:32.07310632810449\n",
      "Epoch3  loss:2.233982270956039  accuracy:34.55147124926346\n",
      "Epoch4  loss:2.1721201121807097  accuracy:35.7454358078988\n",
      "Epoch5  loss:2.041137325763702  accuracy:39.33147824159092\n",
      "Epoch6  loss:1.8701604098081586  accuracy:41.21868811101514\n",
      "Epoch7  loss:1.7521153569221493  accuracy:43.395750375021706\n",
      "Epoch8  loss:1.689305254817009  accuracy:43.87413773360097\n",
      "Epoch9  loss:1.656920477747917  accuracy:43.54160812464672\n",
      "Epoch10  loss:1.6404186487197874  accuracy:43.31466199786938\n",
      "Epoch11  loss:1.6297234863042833  accuracy:43.35899999310768\n",
      "Epoch12  loss:1.623941025137901  accuracy:43.376619583084675\n",
      "Epoch13  loss:1.6193630337715152  accuracy:43.5856079143862\n",
      "Epoch14  loss:1.6158465892076497  accuracy:43.269126420065994\n",
      "Epoch15  loss:1.61292579472065  accuracy:43.355250114576755\n",
      "Epoch16  loss:1.6108795374631881  accuracy:43.725783478368804\n",
      "Epoch17  loss:1.6096218496561048  accuracy:43.1257359248221\n",
      "Epoch18  loss:1.6071079462766646  accuracy:43.6219627976641\n",
      "Epoch19  loss:1.6052593439817429  accuracy:43.38382129805172\n",
      "Epoch20  loss:1.6028792411088943  accuracy:43.00824562717002\n",
      "Epoch21  loss:1.6013312488794327  accuracy:43.446079094384025\n",
      "Epoch22  loss:1.6008541613817213  accuracy:43.24258107667588\n",
      "Epoch23  loss:1.5995372653007507  accuracy:43.09196050745694\n",
      "Epoch24  loss:1.5979185074567794  accuracy:43.16873312897351\n",
      "Epoch25  loss:1.5964944660663607  accuracy:43.03366463888549\n",
      "Epoch26  loss:1.595262134075165  accuracy:43.116389165472626\n",
      "Epoch27  loss:1.5950438320636748  accuracy:43.33507765013345\n",
      "Epoch28  loss:1.5943781435489652  accuracy:43.34899022140184\n",
      "Epoch29  loss:1.5920345693826674  accuracy:43.48162222059924\n",
      "Epoch30  loss:1.5920323163270953  accuracy:43.41780876464681\n",
      "Epoch31  loss:1.591477847099304  accuracy:43.25160680938796\n",
      "Epoch32  loss:1.5894010126590734  accuracy:43.595435995512844\n",
      "Epoch33  loss:1.5900083184242249  accuracy:43.229209767383125\n",
      "Epoch34  loss:1.5893199414014816  accuracy:43.452367989187636\n",
      "Epoch35  loss:1.5871724307537076  accuracy:43.36805326516717\n",
      "Epoch36  loss:1.5860939025878906  accuracy:43.55517413492849\n",
      "Epoch37  loss:1.5866386324167252  accuracy:43.672499147696705\n",
      "Epoch38  loss:1.5836668789386752  accuracy:43.380158360286686\n",
      "Epoch39  loss:1.5838029056787486  accuracy:43.67880403765608\n",
      "Epoch40  loss:1.5823091447353366  accuracy:43.32289006521215\n",
      "Epoch41  loss:1.5823241502046588  accuracy:42.87466256691187\n",
      "Epoch42  loss:1.5814846813678742  accuracy:42.6280504547567\n",
      "Epoch43  loss:1.580441468954086  accuracy:43.01272507682354\n",
      "Epoch44  loss:1.5802887141704558  accuracy:42.74288967833918\n",
      "Epoch45  loss:1.5809260010719302  accuracy:42.72417169302189\n",
      "Epoch46  loss:1.5798102200031279  accuracy:43.17097286976453\n",
      "Epoch47  loss:1.5789670497179034  accuracy:43.10554903134502\n",
      "Epoch48  loss:1.5777514100074768  accuracy:42.776671943298105\n",
      "Epoch49  loss:1.5774983614683153  accuracy:43.08662587007828\n",
      "Epoch50  loss:1.5767455935478212  accuracy:43.045473485639626\n",
      "Epoch51  loss:1.577908989787102  accuracy:43.23768666881363\n",
      "Epoch52  loss:1.5769065380096434  accuracy:42.94613909580882\n",
      "Epoch53  loss:1.5761631071567537  accuracy:43.156997662761064\n",
      "Epoch54  loss:1.576077052950859  accuracy:43.26382882654997\n",
      "Epoch55  loss:1.5751143515110015  accuracy:43.10523483795442\n",
      "Epoch56  loss:1.5744760394096375  accuracy:43.018583947474625\n",
      "Epoch57  loss:1.5739566236734388  accuracy:43.091808864355855\n",
      "Epoch58  loss:1.5741886913776395  accuracy:43.438921231021304\n",
      "Epoch59  loss:1.5727185487747193  accuracy:43.320108711420815\n",
      "Epoch60  loss:1.572383353114128  accuracy:43.52394649689468\n",
      "Epoch61  loss:1.5722926348447803  accuracy:43.55784598435319\n",
      "Epoch62  loss:1.5716965943574903  accuracy:43.548334678789296\n",
      "Epoch63  loss:1.5698699533939358  accuracy:43.26725306386059\n",
      "Epoch64  loss:1.5702596813440326  accuracy:43.00742479432595\n",
      "Epoch65  loss:1.5699981063604358  accuracy:43.31205730935315\n",
      "Epoch66  loss:1.5689505547285079  accuracy:43.565215745942204\n",
      "Epoch67  loss:1.56817906498909  accuracy:43.37432188925698\n",
      "Epoch68  loss:1.5667873948812485  accuracy:43.62813317454022\n",
      "Epoch69  loss:1.566757306456566  accuracy:43.28729128441195\n",
      "Epoch70  loss:1.5664611756801605  accuracy:43.40210121282997\n",
      "Epoch71  loss:1.5666215986013412  accuracy:43.685340461167804\n",
      "Epoch72  loss:1.5649656146764754  accuracy:43.48892113151935\n",
      "Epoch73  loss:1.5646087616682052  accuracy:43.73592102480711\n",
      "Epoch74  loss:1.5626922190189363  accuracy:43.36186628722844\n",
      "Epoch75  loss:1.5627107322216032  accuracy:43.59681980907711\n",
      "Epoch76  loss:1.5619382351636886  accuracy:43.369283116673586\n",
      "Epoch77  loss:1.5596335738897322  accuracy:43.28643687296918\n",
      "Epoch78  loss:1.5585852354764937  accuracy:43.72683059968837\n",
      "Epoch79  loss:1.5576357275247577  accuracy:43.304616520464386\n",
      "Epoch80  loss:1.5567584812641146  accuracy:43.868164816495664\n",
      "Epoch81  loss:1.5563818991184235  accuracy:43.72867942201216\n",
      "Epoch82  loss:1.553692278265953  accuracy:43.62419595293132\n",
      "Epoch83  loss:1.5535402089357377  accuracy:43.2180487676049\n",
      "Epoch84  loss:1.5509809404611588  accuracy:43.49917078638257\n",
      "Epoch85  loss:1.5486148566007611  accuracy:43.51018418969238\n",
      "Epoch86  loss:1.5471526771783826  accuracy:43.97631429889342\n",
      "Epoch87  loss:1.5432687282562252  accuracy:44.43968937033157\n",
      "Epoch88  loss:1.5427552074193955  accuracy:44.44068647805105\n",
      "Epoch89  loss:1.5389502316713335  accuracy:44.759016229856606\n",
      "Epoch90  loss:1.5352835029363634  accuracy:44.971211747702725\n",
      "Epoch91  loss:1.5307243168354034  accuracy:45.34916514583932\n",
      "Epoch92  loss:1.5259812146425247  accuracy:45.59608506762436\n",
      "Epoch93  loss:1.51973178088665  accuracy:46.71999451998178\n",
      "Epoch94  loss:1.5133213996887207  accuracy:47.28840738463574\n",
      "Epoch95  loss:1.5055391848087312  accuracy:47.481076913370686\n",
      "Epoch96  loss:1.4976382344961165  accuracy:48.316142945282024\n",
      "Epoch97  loss:1.488310545682907  accuracy:48.76689276548231\n",
      "Epoch98  loss:1.4787665963172911  accuracy:49.29633803275222\n",
      "Epoch99  loss:1.4682922512292864  accuracy:49.662964015410616\n",
      "Epoch100  loss:1.4581811100244524  accuracy:49.769696782930716\n",
      "Epoch101  loss:1.4490030020475386  accuracy:50.25846862648106\n",
      "Epoch102  loss:1.4373846650123598  accuracy:50.49595112205123\n",
      "Epoch103  loss:1.428614977002144  accuracy:50.58152530216522\n",
      "Epoch104  loss:1.4188513666391374  accuracy:50.806273251823654\n",
      "Epoch105  loss:1.411287498474121  accuracy:50.96302383851557\n",
      "Epoch106  loss:1.4023694962263105  accuracy:51.10103874564118\n",
      "Epoch107  loss:1.3955341517925262  accuracy:51.345513654152924\n",
      "Epoch108  loss:1.3873596966266633  accuracy:51.67093367125587\n",
      "Epoch109  loss:1.379982912540436  accuracy:51.55892344353151\n",
      "Epoch110  loss:1.3763675183057784  accuracy:51.81877512861982\n",
      "Epoch111  loss:1.3704430103302003  accuracy:51.940250110114185\n",
      "Epoch112  loss:1.3648713350296025  accuracy:52.10281986904846\n",
      "Epoch113  loss:1.3613536655902865  accuracy:52.17177451438554\n",
      "Epoch114  loss:1.356015756726265  accuracy:52.50632327595533\n",
      "Epoch115  loss:1.3521965354681018  accuracy:52.54997303500786\n",
      "Epoch116  loss:1.3489459991455077  accuracy:52.7001587465373\n",
      "Epoch117  loss:1.3447309732437134  accuracy:52.725395913225334\n",
      "Epoch118  loss:1.3415655970573428  accuracy:52.90728375129428\n",
      "Epoch119  loss:1.3398273676633836  accuracy:53.08615930278212\n",
      "Epoch120  loss:1.3360674977302551  accuracy:53.156877031373135\n",
      "Epoch121  loss:1.3333337634801863  accuracy:53.24435340633458\n",
      "Epoch122  loss:1.3302288830280304  accuracy:53.4318091605531\n",
      "Epoch123  loss:1.3265942364931107  accuracy:53.579156252182315\n",
      "Epoch124  loss:1.323191490769386  accuracy:53.60594726663858\n",
      "Epoch125  loss:1.3210999310016633  accuracy:53.801603369389625\n",
      "Epoch126  loss:1.3190560430288314  accuracy:53.70010228296091\n",
      "Epoch127  loss:1.315034383535385  accuracy:53.97886484385886\n",
      "Epoch128  loss:1.3121726840734482  accuracy:53.930455115939814\n",
      "Epoch129  loss:1.3079113125801085  accuracy:54.04102684526696\n",
      "Epoch130  loss:1.304589319229126  accuracy:54.40111847953014\n",
      "Epoch131  loss:1.300969034433365  accuracy:54.2743071772076\n",
      "Epoch132  loss:1.2976948082447053  accuracy:54.599305329463085\n",
      "Epoch133  loss:1.2944817930459978  accuracy:54.51681427774628\n",
      "Epoch134  loss:1.2897422790527342  accuracy:54.81817535813081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch135  loss:1.2863140106201172  accuracy:54.918846975321486\n",
      "Epoch136  loss:1.2820320576429365  accuracy:55.2302437936765\n",
      "Epoch138  loss:1.2756465971469881  accuracy:55.22020397212153\n",
      "Epoch139  loss:1.2723021030426025  accuracy:55.61015125336412\n",
      "Epoch140  loss:1.2691176980733871  accuracy:55.57624899466325\n",
      "Epoch141  loss:1.2652113527059552  accuracy:55.678000946302326\n",
      "Epoch142  loss:1.2632801324129102  accuracy:55.640777547526305\n",
      "Epoch143  loss:1.2603009551763535  accuracy:55.85855936496518\n",
      "Epoch144  loss:1.2574342191219332  accuracy:56.016434466735504\n",
      "Epoch145  loss:1.2544078528881075  accuracy:56.009614629868715\n",
      "Epoch146  loss:1.2509074449539184  accuracy:56.078548920046494\n",
      "Epoch147  loss:1.247482618689537  accuracy:56.369700462122495\n",
      "Epoch148  loss:1.2461976021528245  accuracy:56.42295099846345\n",
      "Epoch149  loss:1.2444923698902128  accuracy:56.36471212966443\n",
      "Epoch150  loss:1.2413586020469665  accuracy:56.35530348054809\n",
      "Epoch151  loss:1.2387248933315278  accuracy:56.66409569823263\n",
      "Epoch152  loss:1.23575037419796  accuracy:56.63890346472556\n",
      "Epoch153  loss:1.2334951281547548  accuracy:56.605678453269924\n",
      "Epoch154  loss:1.2317146003246306  accuracy:56.71335826880973\n",
      "Epoch155  loss:1.2297348380088804  accuracy:56.61871951108239\n",
      "Epoch156  loss:1.2279136657714842  accuracy:56.80992418432908\n",
      "Epoch157  loss:1.2269437402486802  accuracy:57.04214951713967\n",
      "Epoch158  loss:1.2230486929416655  accuracy:57.175245024735496\n",
      "Epoch159  loss:1.224160623550415  accuracy:57.0110451765611\n",
      "Epoch160  loss:1.221178764104843  accuracy:57.02878211872044\n",
      "Epoch161  loss:1.2207204759120942  accuracy:57.13675405419707\n",
      "Epoch162  loss:1.2194370120763778  accuracy:57.08102240829017\n",
      "Epoch163  loss:1.215849328041077  accuracy:57.21618108256065\n",
      "Epoch164  loss:1.214280825853348  accuracy:57.26189884663826\n",
      "Epoch165  loss:1.2138605892658234  accuracy:57.20880556682191\n",
      "Epoch166  loss:1.2120673000812532  accuracy:57.3303697779784\n",
      "Epoch167  loss:1.2107929199934009  accuracy:57.37994925115981\n",
      "Epoch168  loss:1.2094034373760223  accuracy:57.47271012943878\n",
      "Epoch169  loss:1.209023556113243  accuracy:57.38933562221641\n",
      "Epoch170  loss:1.2060156732797622  accuracy:57.474779875362785\n",
      "Epoch171  loss:1.2061128169298172  accuracy:57.40598558217632\n",
      "Epoch172  loss:1.2034711807966234  accuracy:57.43639718420226\n",
      "Epoch173  loss:1.2036268651485444  accuracy:57.362860388113276\n",
      "Epoch174  loss:1.2013964146375657  accuracy:57.306497951296755\n",
      "Epoch175  loss:1.2000591903924942  accuracy:57.47897873760295\n",
      "Epoch176  loss:1.1986708879470827  accuracy:57.53315550605675\n",
      "Epoch177  loss:1.1996088206768036  accuracy:57.385767874223276\n",
      "Epoch178  loss:1.1978518664836886  accuracy:57.378231342756266\n",
      "Epoch179  loss:1.196130922436714  accuracy:57.620380250420624\n",
      "Epoch180  loss:1.19690158367157  accuracy:57.44626389655816\n",
      "Epoch181  loss:1.1947176933288572  accuracy:57.42947873569658\n",
      "Epoch182  loss:1.19355137348175  accuracy:57.54466369771407\n",
      "Epoch183  loss:1.1926608264446257  accuracy:57.51334812214176\n",
      "Epoch184  loss:1.1918376833200452  accuracy:57.516211736570504\n",
      "Epoch185  loss:1.1908126026391983  accuracy:57.50340277863948\n",
      "Epoch186  loss:1.1887438684701919  accuracy:57.640722693027975\n",
      "Epoch187  loss:1.1886587053537365  accuracy:57.51046907021347\n",
      "Epoch188  loss:1.1869146674871445  accuracy:57.54451836150208\n",
      "Epoch189  loss:1.186294388771057  accuracy:57.75994656636132\n",
      "Epoch190  loss:1.1858234286308287  accuracy:57.680396501746344\n",
      "Epoch191  loss:1.1840179860591886  accuracy:57.404294463686895\n",
      "Epoch192  loss:1.1845414280891418  accuracy:57.60498181955804\n",
      "Epoch193  loss:1.181459406018257  accuracy:57.6033081940782\n",
      "Epoch194  loss:1.1817320257425308  accuracy:57.8433839812253\n",
      "Epoch195  loss:1.1828071504831312  accuracy:57.61975762787886\n",
      "Epoch196  loss:1.1801748126745224  accuracy:57.650668436999815\n",
      "Epoch197  loss:1.1800822645425797  accuracy:57.748114519616806\n",
      "Epoch198  loss:1.1786584824323656  accuracy:57.70496774787938\n",
      "Epoch199  loss:1.1790382385253906  accuracy:57.9497625423609\n",
      "Epoch200  loss:1.1770658522844315  accuracy:57.927325140109204\n",
      "Epoch201  loss:1.1755591601133348  accuracy:57.662927623370514\n",
      "Epoch202  loss:1.1775917768478394  accuracy:57.73132684271187\n",
      "Epoch203  loss:1.175419384241104  accuracy:57.835129651395874\n",
      "Epoch204  loss:1.1735493332147597  accuracy:57.863402683191886\n",
      "Epoch205  loss:1.1729096204042435  accuracy:57.85351927013244\n",
      "Epoch206  loss:1.1731740146875382  accuracy:57.8095041037433\n",
      "Epoch207  loss:1.1713037848472596  accuracy:57.88653551714467\n",
      "Epoch208  loss:1.1705334901809692  accuracy:58.00685998835289\n",
      "Epoch209  loss:1.169910329580307  accuracy:57.993336368548825\n",
      "Epoch210  loss:1.1692682653665543  accuracy:58.03175127088951\n",
      "Epoch211  loss:1.1684130489826203  accuracy:58.070781608158434\n",
      "Epoch212  loss:1.168468949198723  accuracy:58.01917140758437\n",
      "Epoch213  loss:1.165923738479614  accuracy:57.93850446192801\n",
      "Epoch214  loss:1.1666327446699145  accuracy:58.10851959091174\n",
      "Epoch215  loss:1.164662107825279  accuracy:58.15665764218656\n",
      "Epoch216  loss:1.1638089209795  accuracy:58.23951158910135\n",
      "Epoch217  loss:1.163266307115555  accuracy:58.281173325137026\n",
      "Epoch218  loss:1.1620343297719953  accuracy:58.252261796530526\n",
      "Epoch219  loss:1.161575490236282  accuracy:58.065542009956744\n",
      "Epoch220  loss:1.1629692018032076  accuracy:58.10949425544169\n",
      "Epoch221  loss:1.1609990149736407  accuracy:58.16803555199213\n",
      "Epoch222  loss:1.159714177250862  accuracy:58.16661410169697\n",
      "Epoch223  loss:1.1588217854499818  accuracy:58.27795296659162\n",
      "Epoch224  loss:1.1599530041217805  accuracy:58.28914544277161\n",
      "Epoch225  loss:1.1573954939842224  accuracy:58.41509390884386\n",
      "Epoch226  loss:1.1576187610626223  accuracy:58.42351170426522\n",
      "Epoch227  loss:1.1565494000911711  accuracy:58.26504380145074\n",
      "Epoch228  loss:1.1558217942714692  accuracy:58.338981159267064\n",
      "Epoch229  loss:1.1550861924886702  accuracy:58.22839886096346\n",
      "Epoch230  loss:1.154368871450424  accuracy:58.38589099029961\n",
      "Epoch231  loss:1.153894194960594  accuracy:58.49233677092326\n",
      "Epoch232  loss:1.1523247808218002  accuracy:58.56232970324142\n",
      "Epoch233  loss:1.1531619876623156  accuracy:58.51117366101934\n",
      "Epoch234  loss:1.151312670111656  accuracy:58.52507528209849\n",
      "Epoch235  loss:1.1503092408180238  accuracy:58.3761577369032\n",
      "Epoch236  loss:1.1497725695371628  accuracy:58.394254232362584\n",
      "Epoch237  loss:1.149795600771904  accuracy:58.68145201791364\n",
      "Epoch238  loss:1.1487916529178621  accuracy:58.51563050276926\n",
      "Epoch239  loss:1.1478686064481733  accuracy:58.68743973531959\n",
      "Epoch240  loss:1.1471258699893954  accuracy:58.6856697869095\n",
      "Epoch241  loss:1.1482771575450899  accuracy:58.665181666417055\n",
      "Epoch242  loss:1.1454103440046313  accuracy:58.55109410259805\n",
      "Epoch243  loss:1.1451095998287202  accuracy:58.70563518275819\n",
      "Epoch244  loss:1.1457449197769163  accuracy:58.59107615470427\n",
      "Epoch245  loss:1.1433476358652117  accuracy:58.82464033283536\n",
      "Epoch246  loss:1.1426720321178434  accuracy:58.790690118580095\n",
      "Epoch247  loss:1.1424117654561996  accuracy:58.82667536276332\n",
      "Epoch248  loss:1.1406293213367462  accuracy:58.804222549056014\n",
      "Epoch249  loss:1.140983673930168  accuracy:58.82114729773065\n",
      "Epoch250  loss:1.1397482335567473  accuracy:58.732908706547406\n",
      "Epoch251  loss:1.1388257056474687  accuracy:58.805868037113164\n",
      "Epoch252  loss:1.1388794958591464  accuracy:59.11736072881167\n",
      "Epoch253  loss:1.1369475454092026  accuracy:58.8514307872521\n",
      "Epoch254  loss:1.137250304222107  accuracy:58.807244453352915\n",
      "Epoch255  loss:1.1362399697303769  accuracy:59.00110064880081\n",
      "Epoch256  loss:1.1357243239879609  accuracy:58.98253896084284\n",
      "Epoch257  loss:1.134593912959099  accuracy:58.991794257503635\n",
      "Epoch258  loss:1.1326794713735582  accuracy:59.06403273364656\n",
      "Epoch259  loss:1.1337334275245665  accuracy:59.00499401510269\n",
      "Epoch260  loss:1.1321891069412229  accuracy:59.082925616018905\n",
      "Epoch261  loss:1.1316645681858064  accuracy:59.09649978382563\n",
      "Epoch262  loss:1.1310820549726486  accuracy:58.99701112714668\n",
      "Epoch263  loss:1.1296178579330447  accuracy:59.23049916705079\n",
      "Epoch264  loss:1.1291608959436414  accuracy:59.159980834044354\n",
      "Epoch265  loss:1.1285739302635192  accuracy:59.00899167922796\n",
      "Epoch266  loss:1.1282496482133866  accuracy:59.30545302249807\n",
      "Epoch267  loss:1.1265525728464127  accuracy:59.25120444920941\n",
      "Epoch268  loss:1.1272161662578584  accuracy:59.258756155748834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch269  loss:1.125470557808876  accuracy:59.23186444321928\n",
      "Epoch270  loss:1.1244302630424499  accuracy:59.313534698896426\n",
      "Epoch271  loss:1.1235691696405412  accuracy:59.35571950639217\n",
      "Epoch272  loss:1.1221039295196533  accuracy:59.31990724476689\n",
      "Epoch273  loss:1.1237026125192644  accuracy:59.25724221275077\n",
      "Epoch274  loss:1.1223169147968293  accuracy:59.41206363174539\n",
      "Epoch275  loss:1.1198663502931596  accuracy:59.40872986198689\n",
      "Epoch276  loss:1.1195699542760846  accuracy:59.43294210708748\n",
      "Epoch277  loss:1.118581011891365  accuracy:59.34126093377737\n",
      "Epoch278  loss:1.1180954396724703  accuracy:59.38381159724902\n",
      "Epoch279  loss:1.1170776814222336  accuracy:59.51399393711164\n",
      "Epoch280  loss:1.1155057400465014  accuracy:59.58357039392591\n",
      "Epoch281  loss:1.114788395166397  accuracy:59.41848758751586\n",
      "Epoch282  loss:1.116053432226181  accuracy:59.61496386977812\n",
      "Epoch283  loss:1.1133778125047686  accuracy:59.69712491426598\n",
      "Epoch284  loss:1.1135085999965668  accuracy:59.65714044215953\n",
      "Epoch285  loss:1.111387598514557  accuracy:59.75615709997649\n",
      "Epoch286  loss:1.1096345841884612  accuracy:59.69562892399097\n",
      "Epoch287  loss:1.1092232465744019  accuracy:59.979351788029\n",
      "Epoch288  loss:1.1085539460182192  accuracy:59.90396279806148\n",
      "Epoch289  loss:1.1080674588680268  accuracy:59.85395809769653\n",
      "Epoch290  loss:1.1054463297128678  accuracy:59.93264201629415\n",
      "Epoch291  loss:1.1042899727821351  accuracy:59.95286708195386\n",
      "Epoch292  loss:1.1039005964994428  accuracy:60.01429874169517\n",
      "Epoch293  loss:1.1036102682352065  accuracy:60.10245383168752\n",
      "Epoch294  loss:1.1028846621513366  accuracy:60.00403906047959\n",
      "Epoch295  loss:1.100856328010559  accuracy:60.35084143449453\n",
      "Epoch296  loss:1.1002247035503387  accuracy:60.10820109873798\n",
      "Epoch297  loss:1.0984690874814989  accuracy:60.20642149841785\n",
      "Epoch298  loss:1.0976477324962617  accuracy:60.37692221266491\n",
      "Epoch299  loss:1.0971153557300568  accuracy:60.46181705407785\n",
      "Epoch300  loss:1.0947539269924167  accuracy:60.718874482038935\n",
      "Epoch301  loss:1.0940380126237867  accuracy:60.28633988041888\n",
      "Epoch302  loss:1.0925964891910553  accuracy:60.59359732795668\n",
      "Epoch303  loss:1.0912525296211242  accuracy:60.53177386459887\n",
      "Epoch304  loss:1.0908179819583892  accuracy:60.69554813267334\n",
      "Epoch305  loss:1.089310708642006  accuracy:60.639135155867436\n",
      "Epoch306  loss:1.0879158139228822  accuracy:60.754648666937456\n",
      "Epoch307  loss:1.0863037407398226  accuracy:60.905746202708926\n",
      "Epoch308  loss:1.0861136287450792  accuracy:60.677227200581505\n",
      "Epoch309  loss:1.0855033606290816  accuracy:60.76848421002095\n",
      "Epoch310  loss:1.0840269416570663  accuracy:60.882696122027646\n",
      "Epoch311  loss:1.0832208514213562  accuracy:60.860887688172795\n",
      "Epoch312  loss:1.0817345231771471  accuracy:60.79588843682334\n",
      "Epoch313  loss:1.081784126162529  accuracy:60.89141457147568\n",
      "Epoch314  loss:1.0797664165496825  accuracy:61.02256919222505\n",
      "Epoch315  loss:1.078171607851982  accuracy:60.79933259897103\n",
      "Epoch316  loss:1.0777625590562818  accuracy:60.90281562675948\n",
      "Epoch317  loss:1.0758009612560273  accuracy:61.24979097823676\n",
      "Epoch318  loss:1.0756435722112656  accuracy:61.008561651420926\n",
      "Epoch319  loss:1.0744670540094376  accuracy:61.28536936426926\n",
      "Epoch320  loss:1.0734294593334197  accuracy:61.231228372497746\n",
      "Epoch321  loss:1.0739089995622633  accuracy:61.15845647075269\n",
      "Epoch322  loss:1.070944181084633  accuracy:61.351975725276496\n",
      "Epoch323  loss:1.069015523791313  accuracy:61.29714410746403\n",
      "Epoch324  loss:1.0675794273614883  accuracy:61.36051625642897\n",
      "Epoch325  loss:1.0668141931295396  accuracy:61.4337569755773\n",
      "Epoch326  loss:1.0673258274793622  accuracy:61.393671153811695\n",
      "Epoch327  loss:1.0652315735816955  accuracy:61.42931102168693\n",
      "Epoch328  loss:1.0639840602874757  accuracy:61.581009536030166\n",
      "Epoch329  loss:1.06308097243309  accuracy:61.7207122972359\n",
      "Epoch330  loss:1.0631113022565841  accuracy:61.50389919655073\n",
      "Epoch331  loss:1.0615028291940691  accuracy:61.70531197317446\n",
      "Epoch332  loss:1.0603693664073943  accuracy:61.84094122216183\n",
      "Epoch333  loss:1.0599548280239106  accuracy:61.631670557623586\n",
      "Epoch334  loss:1.0593839928507804  accuracy:62.01616514602348\n",
      "Epoch335  loss:1.0578339546918867  accuracy:61.840626014717174\n",
      "Epoch336  loss:1.0553889125585556  accuracy:61.941736404448626\n",
      "Epoch337  loss:1.0557862401008606  accuracy:61.94645208891037\n",
      "Epoch338  loss:1.0540974348783492  accuracy:61.93302930311753\n",
      "Epoch339  loss:1.053081774711609  accuracy:61.989250414056\n",
      "Epoch340  loss:1.0527253389358522  accuracy:62.01670002352831\n",
      "Epoch341  loss:1.05273300409317  accuracy:61.906883457831135\n",
      "Epoch342  loss:1.0503971219062804  accuracy:62.18572799999452\n",
      "Epoch343  loss:1.050172393023968  accuracy:62.17554067273395\n",
      "Epoch344  loss:1.0484886914491653  accuracy:62.21778600289255\n",
      "Epoch345  loss:1.0483311712741852  accuracy:62.161432037539434\n",
      "Epoch346  loss:1.0472243309020997  accuracy:62.24560585499621\n",
      "Epoch347  loss:1.046025650203228  accuracy:62.28832595483113\n",
      "Epoch348  loss:1.0449752807617188  accuracy:62.39501603693779\n",
      "Epoch349  loss:1.0438882425427438  accuracy:62.480837093770454\n",
      "Epoch350  loss:1.04323268532753  accuracy:62.51264069360306\n",
      "Epoch351  loss:1.043941041827202  accuracy:62.55122851632608\n",
      "Epoch352  loss:1.0413198247551918  accuracy:62.60974637609756\n",
      "Epoch353  loss:1.040648880600929  accuracy:62.65365478474385\n",
      "Epoch354  loss:1.039033003151417  accuracy:62.77429981544164\n",
      "Epoch355  loss:1.0393257856369018  accuracy:62.6908020390755\n",
      "Epoch356  loss:1.0379823669791224  accuracy:62.64576985667304\n",
      "Epoch357  loss:1.0375663995742797  accuracy:62.8269227364642\n",
      "Epoch358  loss:1.0360719680786132  accuracy:62.87906208475645\n",
      "Epoch359  loss:1.0365397170186041  accuracy:62.60316248872471\n",
      "Epoch360  loss:1.0345983877778056  accuracy:62.981805587178904\n",
      "Epoch361  loss:1.033563232421875  accuracy:62.944684122727516\n",
      "Epoch362  loss:1.0327143341302871  accuracy:62.84028379928686\n",
      "Epoch363  loss:1.0325614258646962  accuracy:62.991575947353226\n",
      "Epoch364  loss:1.03103791475296  accuracy:63.08312532039005\n",
      "Epoch365  loss:1.0308455258607865  accuracy:62.972244575766645\n",
      "Epoch366  loss:1.0294279888272286  accuracy:63.127680088343446\n",
      "Epoch367  loss:1.028509336709976  accuracy:63.268525878065226\n",
      "Epoch368  loss:1.028524598479271  accuracy:63.22309015162435\n",
      "Epoch369  loss:1.02572800219059  accuracy:63.264571259871786\n",
      "Epoch370  loss:1.026565134525299  accuracy:63.35594220631379\n",
      "Epoch371  loss:1.0265027582645414  accuracy:63.368592217721634\n",
      "Epoch372  loss:1.0250501915812493  accuracy:63.33197891552017\n",
      "Epoch373  loss:1.0241697981953624  accuracy:63.37413876357268\n",
      "Epoch374  loss:1.0231359779834748  accuracy:63.323765410107754\n",
      "Epoch375  loss:1.0239452645182612  accuracy:63.48229926466865\n",
      "Epoch376  loss:1.022598360478878  accuracy:63.44635840751635\n",
      "Epoch377  loss:1.0215256050229071  accuracy:63.5814481163856\n",
      "Epoch378  loss:1.0208642899990084  accuracy:63.518986676960665\n",
      "Epoch379  loss:1.0195381700992585  accuracy:63.66672902787426\n",
      "Epoch380  loss:1.01847922205925  accuracy:63.48101563941078\n",
      "Epoch381  loss:1.0165764182806016  accuracy:63.68300702376574\n",
      "Epoch382  loss:1.0176308631896973  accuracy:63.56798012531628\n",
      "Epoch383  loss:1.0167993992567061  accuracy:63.63957459652689\n",
      "Epoch384  loss:1.0157627731561663  accuracy:63.646245629219685\n",
      "Epoch385  loss:1.0144006595015527  accuracy:63.707917945459826\n",
      "Epoch386  loss:1.0139779925346373  accuracy:63.8306193225306\n",
      "Epoch387  loss:1.013707357645035  accuracy:63.872712379860175\n",
      "Epoch388  loss:1.011294150352478  accuracy:63.91289840146687\n",
      "Epoch389  loss:1.0121767058968543  accuracy:63.90354307804147\n",
      "Epoch390  loss:1.011320497095585  accuracy:63.98838097766058\n",
      "Epoch391  loss:1.0094255536794663  accuracy:63.90901473967993\n",
      "Epoch392  loss:1.0090266197919844  accuracy:63.923138471885665\n",
      "Epoch393  loss:1.0088988229632376  accuracy:64.04239704423415\n",
      "Epoch394  loss:1.008124414086342  accuracy:64.05120111774066\n",
      "Epoch395  loss:1.0072191923856735  accuracy:63.927935221752186\n",
      "Epoch396  loss:1.0072151839733126  accuracy:63.958994976908954\n",
      "Epoch397  loss:1.0058317184448242  accuracy:64.08721824034092\n",
      "Epoch398  loss:1.0048587381839753  accuracy:63.97960109531545\n",
      "Epoch399  loss:1.0051174551248547  accuracy:64.07584501364188\n",
      "Epoch400  loss:1.0035583317279817  accuracy:64.19284767623624\n",
      "Epoch401  loss:1.0030648335814476  accuracy:64.35854762379189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch402  loss:1.0029850110411642  accuracy:64.03129967321098\n",
      "Epoch403  loss:1.001377236843109  accuracy:64.20645470086342\n",
      "Epoch404  loss:1.0005022525787353  accuracy:64.16986038084079\n",
      "Epoch405  loss:1.0005341500043867  accuracy:64.21467235972781\n",
      "Epoch406  loss:0.9984227538108824  accuracy:64.33326800971798\n",
      "Epoch407  loss:0.9989158809185027  accuracy:64.20579531593442\n",
      "Epoch408  loss:0.9987529724836347  accuracy:64.40299005309785\n",
      "Epoch409  loss:0.9981018021702767  accuracy:64.40884225060388\n",
      "Epoch410  loss:0.9971411272883417  accuracy:64.56643583798082\n",
      "Epoch411  loss:0.9952945753931997  accuracy:64.39173394353008\n",
      "Epoch412  loss:0.9960926234722138  accuracy:64.46425418999561\n",
      "Epoch413  loss:0.9955910876393317  accuracy:64.49023436237371\n",
      "Epoch414  loss:0.9943896532058717  accuracy:64.41367440372733\n",
      "Epoch415  loss:0.9931569069623946  accuracy:64.5525287911447\n",
      "Epoch416  loss:0.9932740673422814  accuracy:64.44196138243757\n",
      "Epoch417  loss:0.9932487785816193  accuracy:64.64094646119833\n",
      "Epoch418  loss:0.9907567828893662  accuracy:64.65999398368075\n",
      "Epoch419  loss:0.9917023807764054  accuracy:64.55042490397571\n",
      "Epoch420  loss:0.988339075446129  accuracy:64.68626152940993\n",
      "Epoch421  loss:0.9892662689089772  accuracy:64.63229756038238\n",
      "Epoch422  loss:0.9882497698068619  accuracy:64.62304082777067\n",
      "Epoch423  loss:0.9868548661470414  accuracy:64.78658651808277\n",
      "Epoch424  loss:0.9863605633378029  accuracy:64.74071063382758\n",
      "Epoch425  loss:0.9856191635131837  accuracy:64.73145050484914\n",
      "Epoch426  loss:0.9862334489822387  accuracy:64.71987733581433\n",
      "Epoch427  loss:0.9849416166543961  accuracy:64.71157729419099\n",
      "Epoch428  loss:0.9836954474449159  accuracy:64.91961687307767\n",
      "Epoch429  loss:0.9849413678050041  accuracy:64.90224131862796\n",
      "Epoch430  loss:0.9829957365989687  accuracy:65.04288298432114\n",
      "Epoch431  loss:0.9821283742785455  accuracy:64.87060585913976\n",
      "Epoch432  loss:0.9830534011125563  accuracy:64.81769784208903\n",
      "Epoch433  loss:0.9824915185570715  accuracy:64.9883333279398\n",
      "Epoch434  loss:0.9797144666314125  accuracy:64.96574235025574\n",
      "Epoch435  loss:0.9793302044272422  accuracy:64.97537395718378\n",
      "Epoch436  loss:0.9781210243701935  accuracy:65.04014391853954\n",
      "Epoch437  loss:0.9788286179304122  accuracy:65.20747778857357\n",
      "Epoch438  loss:0.9788156747817993  accuracy:65.23109190182609\n",
      "Epoch439  loss:0.9776071593165397  accuracy:65.08493263395924\n",
      "Epoch440  loss:0.9765198230743409  accuracy:65.34754571162551\n",
      "Epoch441  loss:0.9754456117749215  accuracy:65.25004745240503\n",
      "Epoch442  loss:0.9747091606259346  accuracy:65.14630988972714\n",
      "Epoch443  loss:0.9739447280764582  accuracy:65.01362042411719\n",
      "Epoch444  loss:0.9722588747739792  accuracy:65.21869066920476\n",
      "Epoch445  loss:0.9739991366863251  accuracy:65.42699402622809\n",
      "Epoch446  loss:0.9735486894845964  accuracy:65.22037688572495\n",
      "Epoch447  loss:0.9705887287855148  accuracy:65.21061938392617\n",
      "Epoch448  loss:0.9719204232096672  accuracy:65.26472485503209\n",
      "Epoch449  loss:0.971390874683857  accuracy:65.2371379282623\n",
      "Epoch450  loss:0.9699554339051246  accuracy:65.50291140434271\n",
      "Epoch451  loss:0.9693533673882482  accuracy:65.40316470659997\n",
      "Epoch452  loss:0.9680100321769716  accuracy:65.58797699644815\n",
      "Epoch453  loss:0.9693614542484282  accuracy:65.63799158230843\n",
      "Epoch454  loss:0.9664334744215013  accuracy:65.55570892059217\n",
      "Epoch455  loss:0.9677646085619926  accuracy:65.44614410378453\n",
      "Epoch456  loss:0.9670850753784178  accuracy:65.49104114821469\n",
      "Epoch457  loss:0.9673147946596145  accuracy:65.60354143325733\n",
      "Epoch458  loss:0.9644189268350601  accuracy:65.72278573266196\n",
      "Epoch459  loss:0.9648352220654488  accuracy:65.81692147958303\n",
      "Epoch460  loss:0.964839243888855  accuracy:65.67550087586035\n",
      "Epoch461  loss:0.96360884308815  accuracy:65.80331487632024\n",
      "Epoch462  loss:0.9628980204463007  accuracy:65.83692285827253\n",
      "Epoch463  loss:0.962145271897316  accuracy:65.78217924574456\n",
      "Epoch464  loss:0.9614978417754174  accuracy:65.8306231981525\n",
      "Epoch465  loss:0.9599987521767618  accuracy:65.75422560736166\n",
      "Epoch466  loss:0.9600700303912163  accuracy:65.86096313023161\n",
      "Epoch467  loss:0.9597968459129335  accuracy:65.84230303217016\n",
      "Epoch468  loss:0.9587347939610481  accuracy:65.84695914505203\n",
      "Epoch469  loss:0.9592066794633867  accuracy:65.94569066846098\n",
      "Epoch470  loss:0.9602922603487969  accuracy:65.82590687765513\n",
      "Epoch471  loss:0.9579722434282303  accuracy:65.75579881532944\n",
      "Epoch472  loss:0.958145271241665  accuracy:65.93744066385169\n",
      "Epoch473  loss:0.9588164567947387  accuracy:66.04808745802195\n",
      "Epoch474  loss:0.956530922651291  accuracy:65.94894846645677\n",
      "Epoch475  loss:0.9575855657458305  accuracy:65.74287806668126\n",
      "Epoch476  loss:0.9554190829396247  accuracy:65.93144269220848\n",
      "Epoch477  loss:0.9550381526350976  accuracy:65.93437153775596\n",
      "Epoch478  loss:0.9574617013335229  accuracy:65.90827259874942\n",
      "Epoch479  loss:0.9553368374705313  accuracy:66.05444034254356\n",
      "Epoch480  loss:0.9544148743152617  accuracy:65.97868250633749\n",
      "Epoch481  loss:0.9538019269704818  accuracy:66.16246236218538\n",
      "Epoch482  loss:0.9516474038362502  accuracy:66.04212411771863\n",
      "Epoch483  loss:0.9535142585635186  accuracy:66.03498488271366\n",
      "Epoch484  loss:0.9513935267925262  accuracy:66.10876925102829\n",
      "Epoch485  loss:0.9529210522770882  accuracy:66.13390899873257\n",
      "Epoch486  loss:0.9507427245378495  accuracy:66.26887820769932\n",
      "Epoch487  loss:0.951002761721611  accuracy:66.25274353737755\n",
      "Epoch488  loss:0.9514906868338587  accuracy:66.19677629150304\n",
      "Epoch489  loss:0.9501393213868141  accuracy:66.36045574081689\n",
      "Epoch490  loss:0.9488787963986396  accuracy:66.19709496164522\n",
      "Epoch491  loss:0.9490643054246903  accuracy:66.3424154728708\n",
      "Epoch492  loss:0.9471143141388895  accuracy:66.2876940887953\n",
      "Epoch493  loss:0.9475844293832779  accuracy:66.20566679682548\n",
      "Epoch494  loss:0.9482239827513694  accuracy:66.37315777552796\n",
      "Epoch495  loss:0.9464838624000549  accuracy:66.15775218179994\n",
      "Epoch496  loss:0.947049307823181  accuracy:66.30143959269598\n",
      "Epoch497  loss:0.9467384859919549  accuracy:66.24460086290995\n",
      "Epoch498  loss:0.9454807862639427  accuracy:66.41231347782583\n",
      "Epoch499  loss:0.944835688173771  accuracy:66.5244948852827\n",
      "Epoch500  loss:0.9433330148458482  accuracy:66.49010310828477\n"
     ]
    }
   ],
   "source": [
    "server = Server()\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset)\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "acc_valid = []\n",
    "loss_valid = []\n",
    "\n",
    "early_stopping = Early_Stopping(args.partience)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(args.global_epochs):\n",
    "  sample_worker = server.sample_worker(workers)\n",
    "  server.send_parameters(sample_worker)\n",
    "\n",
    "  acc_train_avg = 0.0\n",
    "  loss_train_avg = 0.0\n",
    "  acc_valid_avg = 0.0\n",
    "  loss_valid_avg = 0.0\n",
    "  for worker in sample_worker:\n",
    "    acc_train_tmp,loss_train_tmp = worker.local_train()\n",
    "    acc_valid_tmp,loss_valid_tmp = worker.validate()\n",
    "    acc_train_avg += acc_train_tmp/len(sample_worker)\n",
    "    loss_train_avg += loss_train_tmp/len(sample_worker)\n",
    "    acc_valid_avg += acc_valid_tmp/len(sample_worker)\n",
    "    loss_valid_avg += loss_valid_tmp/len(sample_worker)\n",
    "  server.aggregate_model(sample_worker)\n",
    "  '''\n",
    "  server.model.to(args.device)\n",
    "  for worker in workers:\n",
    "    acc_valid_tmp,loss_valid_tmp = test(server.model,args.criterion,worker.valloader)\n",
    "    acc_valid_avg += acc_valid_tmp/len(workers)\n",
    "    loss_valid_avg += loss_valid_tmp/len(workers)\n",
    "  server.model.to('cpu')\n",
    "  '''\n",
    "  print('Epoch{}  loss:{}  accuracy:{}'.format(epoch+1,loss_valid_avg,acc_valid_avg))\n",
    "  acc_train.append(acc_train_avg)\n",
    "  loss_train.append(loss_train_avg)\n",
    "  acc_valid.append(acc_valid_avg)\n",
    "  loss_valid.append(loss_valid_avg)\n",
    "\n",
    "  if early_stopping.validate(loss_valid_avg):\n",
    "    print('Early Stop')\n",
    "    break\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 accuracy:62.672811059907836  loss:1.0829381942749023\n",
      "Worker2 accuracy:72.3360655737705  loss:0.8391865491867065\n",
      "Worker3 accuracy:58.666666666666664  loss:1.0808207988739014\n",
      "Worker4 accuracy:72.42063492063492  loss:0.7067062854766846\n",
      "Worker5 accuracy:64.39024390243902  loss:0.7300734519958496\n",
      "Worker6 accuracy:58.77437325905292  loss:1.2279852628707886\n",
      "Worker7 accuracy:54.10526315789474  loss:0.9303566813468933\n",
      "Worker8 accuracy:69.67113276492083  loss:0.7424317598342896\n",
      "Worker9 accuracy:80.0  loss:0.6182482242584229\n",
      "Worker10 accuracy:84.52380952380952  loss:0.5809528231620789\n",
      "Worker11 accuracy:65.07936507936508  loss:1.0720680952072144\n",
      "Worker12 accuracy:57.84172661870504  loss:1.1396509408950806\n",
      "Worker13 accuracy:80.81632653061224  loss:0.549914538860321\n",
      "Worker14 accuracy:67.58620689655173  loss:1.1400705575942993\n",
      "Worker15 accuracy:69.71677559912854  loss:0.8641899228096008\n",
      "Worker16 accuracy:62.44541484716157  loss:1.2679274082183838\n",
      "Worker17 accuracy:53.77358490566038  loss:1.1436052322387695\n",
      "Worker18 accuracy:59.41499085923218  loss:1.2717682123184204\n",
      "Worker19 accuracy:63.670411985018724  loss:1.08306884765625\n",
      "Worker20 accuracy:68.92230576441102  loss:0.9012196660041809\n",
      "Test(personalized)  loss:0.9486591726541519  accuracy:66.34140549574718\n"
     ]
    }
   ],
   "source": [
    "acc_test_personalized = []\n",
    "loss_test_personalized = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  acc_tmp,loss_tmp = worker.test()\n",
    "  acc_test_personalized.append(acc_tmp)\n",
    "  loss_test_personalized.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_test_personalized_avg = sum(acc_test_personalized)/len(acc_test_personalized)\n",
    "loss_test_personalized_avg = sum(loss_test_personalized)/len(loss_test_personalized)\n",
    "print('Test(personalized)  loss:{}  accuracy:{}'.format(loss_test_personalized_avg,acc_test_personalized_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 accuracy:28.725038402457756  loss:1.6173909902572632\n",
      "Worker2 accuracy:31.9672131147541  loss:1.8891551494598389\n",
      "Worker3 accuracy:24.952380952380953  loss:1.7458717823028564\n",
      "Worker4 accuracy:59.12698412698413  loss:1.3219714164733887\n",
      "Worker5 accuracy:51.1219512195122  loss:1.2964500188827515\n",
      "Worker6 accuracy:32.590529247910865  loss:1.6340245008468628\n",
      "Worker7 accuracy:25.05263157894737  loss:1.6831095218658447\n",
      "Worker8 accuracy:55.29841656516443  loss:1.2373273372650146\n",
      "Worker9 accuracy:21.910828025477706  loss:1.8746966123580933\n",
      "Worker10 accuracy:30.555555555555557  loss:1.6341320276260376\n",
      "Worker11 accuracy:23.49206349206349  loss:1.7839469909667969\n",
      "Worker12 accuracy:42.15827338129496  loss:1.6651012897491455\n",
      "Worker13 accuracy:47.142857142857146  loss:1.4543452262878418\n",
      "Worker14 accuracy:36.55172413793103  loss:1.575492262840271\n",
      "Worker15 accuracy:54.68409586056645  loss:1.442461371421814\n",
      "Worker16 accuracy:43.66812227074236  loss:1.6733001470565796\n",
      "Worker17 accuracy:38.443396226415096  loss:1.5150423049926758\n",
      "Worker18 accuracy:31.627056672760514  loss:1.6775181293487549\n",
      "Worker19 accuracy:50.18726591760299  loss:1.3968698978424072\n",
      "Worker20 accuracy:55.88972431077694  loss:1.4067604541778564\n",
      "Test(global)  loss:1.5762483716011046  accuracy:39.257305410107804\n"
     ]
    }
   ],
   "source": [
    "acc_test_global = []\n",
    "loss_test_global = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  server.model = server.model.to(args.device)\n",
    "  acc_tmp,loss_tmp = test(server.model,args.criterion,worker.testloader)\n",
    "  acc_test_global.append(acc_tmp)\n",
    "  loss_test_global.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "\n",
    "server.model = server.model.to('cpu')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_test_global_avg = sum(acc_test_global)/len(acc_test_global)\n",
    "loss_test_global_avg = sum(loss_test_global)/len(loss_test_global)\n",
    "print('Test(global)  loss:{}  accuracy:{}'.format(loss_test_global_avg,acc_test_global_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 accuracy:61.904761904761905  loss:1.195131778717041\n",
      "Worker2 accuracy:70.28688524590164  loss:0.8767534494400024\n",
      "Worker3 accuracy:52.38095238095238  loss:1.3708988428115845\n",
      "Worker4 accuracy:67.85714285714286  loss:0.8437215685844421\n",
      "Worker5 accuracy:61.951219512195124  loss:0.6862875670194626\n",
      "Worker6 accuracy:55.710306406685234  loss:1.2814489603042603\n",
      "Worker7 accuracy:50.73684210526316  loss:0.9857251048088074\n",
      "Worker8 accuracy:67.35688185140073  loss:0.8147603273391724\n",
      "Worker9 accuracy:78.72611464968153  loss:0.6814498901367188\n",
      "Worker10 accuracy:86.5079365079365  loss:0.39040589332580566\n",
      "Worker11 accuracy:64.12698412698413  loss:1.0425533056259155\n",
      "Worker12 accuracy:58.1294964028777  loss:1.1936753988265991\n",
      "Worker13 accuracy:77.55102040816327  loss:0.6484477519989014\n",
      "Worker14 accuracy:60.3448275862069  loss:1.1827634572982788\n",
      "Worker15 accuracy:67.3202614379085  loss:0.8784492015838623\n",
      "Worker16 accuracy:53.275109170305676  loss:1.326694130897522\n",
      "Worker17 accuracy:52.594339622641506  loss:1.2098561525344849\n",
      "Worker18 accuracy:57.40402193784278  loss:1.3329648971557617\n",
      "Worker19 accuracy:63.670411985018724  loss:1.1339802742004395\n",
      "Worker20 accuracy:68.67167919799499  loss:0.9213424921035767\n",
      "Test_fine-tune(global)  loss:0.999865522235632  accuracy:63.82535976489326\n"
     ]
    }
   ],
   "source": [
    "acc_tune_test_global = []\n",
    "loss_tune_test_global = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.model = copy.deepcopy(server.model)\n",
    "  worker.model = worker.model.to(args.device)\n",
    "  _,_ = train(worker.model,args.criterion,worker.trainloader,args.local_epochs)\n",
    "  acc_tmp,loss_tmp = test(worker.model,args.criterion,worker.testloader)\n",
    "  acc_tune_test_global.append(acc_tmp)\n",
    "  loss_tune_test_global.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "  worker.model = worker.model.to('cpu')\n",
    "  del worker.model\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_tune_test_global_avg = sum(acc_tune_test_global)/len(acc_tune_test_global)\n",
    "loss_tune_test_global_avg = sum(loss_tune_test_global)/len(loss_tune_test_global)\n",
    "print('Test_fine-tune(global)  loss:{}  accuracy:{}'.format(loss_tune_test_global_avg,acc_tune_test_global_avg))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
