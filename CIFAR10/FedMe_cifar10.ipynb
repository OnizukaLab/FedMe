{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    # random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 42\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O0TfzOhU-QlG"
   },
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 40\n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 500\n",
    "    self.local_epochs = 2\n",
    "    self.lr = None\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 500\n",
    "    self.worker_num = 20\n",
    "    self.sample_num = 20\n",
    "    self.cluster_list = [1,2,3,4]\n",
    "    self.cluster_num = None\n",
    "    self.turn_of_cluster_num = [0,250,375,450]\n",
    "    self.turn_of_replacement_model = list(range(self.global_epochs))\n",
    "    self.unlabeleddata_size = 1000\n",
    "    self.device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion_ce = nn.CrossEntropyLoss()\n",
    "    self.criterion_kl = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    self.alpha_label = 0.5\n",
    "    self.alpha_size = 10\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = []\n",
    "lr_list.append(10**-3.0)\n",
    "lr_list.append(10**-2.5)\n",
    "lr_list.append(10**-2.0)\n",
    "lr_list.append(10**-1.5)\n",
    "lr_list.append(10**-1.0)\n",
    "lr_list.append(10**-0.5)\n",
    "lr_list.append(10**0.0)\n",
    "lr_list.append(10**0.5)\n",
    "\n",
    "args.lr = lr_list[lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r5PuCcqmJNUQ"
   },
   "outputs": [],
   "source": [
    "class LocalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.label[idx]\n",
    "        if self.transform:\n",
    "            out_data = self.transform(out_data)\n",
    "        return out_data, out_label\n",
    "    \n",
    "class DatasetFromSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.subset[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    \n",
    "class GlobalDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,federated_dataset,transform=None):\n",
    "    self.transform = transform\n",
    "    self.data = []\n",
    "    self.label = []\n",
    "    for dataset in federated_dataset:\n",
    "      for (data,label) in dataset:\n",
    "        self.data.append(data)\n",
    "        self.label.append(label)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    out_data = self.data[idx]\n",
    "    out_label = self.label[idx]\n",
    "    if self.transform:\n",
    "        out_data = self.transform(out_data)\n",
    "    return out_data, out_label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "class UnlabeledDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,transform=None):\n",
    "    self.transform = transform\n",
    "    self.data = []\n",
    "    self.target = None\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    out_data = self.data[idx]\n",
    "    out_label = 'unlabeled'\n",
    "    if self.transform:\n",
    "        out_data = self.transform(out_data)\n",
    "    return out_data, out_label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(Centralized=False,unlabeled_data=False):\n",
    "    \n",
    "    transform_train = transforms.Compose([transforms.ToPILImage(),\n",
    "                                    transforms.RandomCrop(32, padding=2),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.491372549, 0.482352941, 0.446666667), (0.247058824, 0.243529412, 0.261568627))])\n",
    "    transform_test = transforms.Compose([transforms.ToPILImage(),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.491372549, 0.482352941, 0.446666667), (0.247058824, 0.243529412, 0.261568627))])\n",
    "\n",
    "    # download train data\n",
    "    all_trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True)\n",
    "    #trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "    # download test data\n",
    "    all_testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True)\n",
    "    #testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "    \n",
    "    ## get unlabeled dataset\n",
    "    if unlabeled_data:\n",
    "        unlabeled_dataset = UnlabeledDataset(transform_test)\n",
    "        idx = sorted(random.sample(range(len(all_trainset)),args.unlabeleddata_size))\n",
    "        unlabeled_dataset.data = np.array([all_trainset.data[i]  for i in idx])\n",
    "        all_trainset.data = np.delete(all_trainset.data,idx,0)\n",
    "        all_trainset.targets = np.delete(all_trainset.targets,idx,0)\n",
    "    all_train_data = np.array(all_trainset.data)\n",
    "    all_train_label = np.array(all_trainset.targets)\n",
    "    all_test_data = np.array(all_testset.data)\n",
    "    all_test_label = np.array(all_testset.targets)\n",
    "    print('Train:{} Test:{}'.format(len(all_train_data),len(all_test_data)))\n",
    "\n",
    "\n",
    "    ## Data size heterogeneity\n",
    "    data_proportions = np.random.dirichlet(np.repeat(args.alpha_size, args.worker_num))\n",
    "    train_data_proportions = np.array([0 for _ in range(args.worker_num)])\n",
    "    test_data_proportions = np.array([0 for _ in range(args.worker_num)])\n",
    "    for i in range(len(data_proportions)):\n",
    "        if i==(len(data_proportions)-1):\n",
    "            train_data_proportions = train_data_proportions.astype('int64')\n",
    "            test_data_proportions = test_data_proportions.astype('int64')\n",
    "            train_data_proportions[-1] = len(all_train_data) - np.sum(train_data_proportions[:-1])\n",
    "            test_data_proportions[-1] = len(all_test_data) - np.sum(test_data_proportions[:-1])\n",
    "        else:\n",
    "            train_data_proportions[i] = (data_proportions[i] * len(all_train_data))\n",
    "            test_data_proportions[i] = (data_proportions[i] * len(all_test_data))\n",
    "    min_size = 0\n",
    "    K = 10\n",
    "\n",
    "    '''\n",
    "    label_list = np.arange(10)\n",
    "    np.random.shuffle(label_list)\n",
    "    '''\n",
    "    label_list = list(range(K))\n",
    "\n",
    "\n",
    "    ## Data distribution heterogeneity\n",
    "    while min_size<10:\n",
    "        idx_train_batch = [[] for _ in range(args.worker_num)]\n",
    "        idx_test_batch = [[] for _ in range(args.worker_num)]\n",
    "        for k in label_list:\n",
    "            proportions_train = np.random.dirichlet(np.repeat(args.alpha_label, args.worker_num))\n",
    "            proportions_test = copy.deepcopy(proportions_train)\n",
    "            idx_k_train = np.where(all_train_label == k)[0]\n",
    "            idx_k_test = np.where(all_test_label == k)[0]\n",
    "            np.random.shuffle(idx_k_train)\n",
    "            np.random.shuffle(idx_k_test)\n",
    "            ## Balance (train)\n",
    "            proportions_train = np.array([p*(len(idx_j)<train_data_proportions[i]) for i,(p,idx_j) in enumerate(zip(proportions_train,idx_train_batch))])\n",
    "            proportions_train = proportions_train/proportions_train.sum()\n",
    "            proportions_train = (np.cumsum(proportions_train)*len(idx_k_train)).astype(int)[:-1]\n",
    "            idx_train_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_train_batch,np.split(idx_k_train,proportions_train))]\n",
    "\n",
    "            ## Balance (test)\n",
    "            proportions_test = np.array([p*(len(idx_j)<test_data_proportions[i]) for i,(p,idx_j) in enumerate(zip(proportions_test,idx_test_batch))])\n",
    "            proportions_test = proportions_test/proportions_test.sum()\n",
    "            proportions_test = (np.cumsum(proportions_test)*len(idx_k_test)).astype(int)[:-1]\n",
    "            idx_test_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_test_batch,np.split(idx_k_test,proportions_test))]\n",
    "\n",
    "            min_size = min([len(idx_j) for idx_j in idx_train_batch])\n",
    "\n",
    "    federated_trainset = []\n",
    "    federated_testset = []\n",
    "    for i in range(args.worker_num):\n",
    "        ## create trainset\n",
    "        data = [all_train_data[idx] for idx in idx_train_batch[i]]\n",
    "        label = [all_train_label[idx] for idx in idx_train_batch[i]]\n",
    "        federated_trainset.append(LocalDataset())\n",
    "        federated_trainset[-1].data = data\n",
    "        federated_trainset[-1].label = label\n",
    "\n",
    "        ## create testset\n",
    "        data = [all_test_data[idx] for idx in idx_test_batch[i]]\n",
    "        label = [all_test_label[idx] for idx in idx_test_batch[i]]\n",
    "        federated_testset.append(LocalDataset())\n",
    "        federated_testset[-1].data = data\n",
    "        federated_testset[-1].label = label\n",
    "\n",
    "        \n",
    "    ## split trainset\n",
    "    federated_valset = [None]*args.worker_num\n",
    "    for i in range(args.worker_num):\n",
    "        n_samples = len(federated_trainset[i])\n",
    "        if n_samples==1:\n",
    "            train_subset = federated_trainset[i]\n",
    "            val_subset = copy.deepcopy(federated_trainset[i])\n",
    "        else:\n",
    "            train_size = int(len(federated_trainset[i]) * 0.8) \n",
    "            val_size = n_samples - train_size \n",
    "            train_subset,val_subset = torch.utils.data.random_split(federated_trainset[i], [train_size, val_size])\n",
    "\n",
    "        federated_trainset[i] = DatasetFromSubset(train_subset)\n",
    "        federated_valset[i] = DatasetFromSubset(val_subset)\n",
    "\n",
    "    ## show data distribution\n",
    "    H = 4\n",
    "    W = 5\n",
    "    fig, axs = plt.subplots(H, W, figsize=(20, 5))\n",
    "    x = np.arange(1,11)\n",
    "    for i, (trainset,valset,testset) in enumerate(zip(federated_trainset,federated_valset,federated_testset)):\n",
    "        bottom = [0]*10\n",
    "        count = [0]*10\n",
    "        for _,label in trainset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        for j in range(len(count)):\n",
    "            bottom[j]+=count[j]\n",
    "        count = [0]*10\n",
    "        for _,label in valset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        for j in range(len(count)):\n",
    "            bottom[j]+=count[j]\n",
    "        count = [0]*10\n",
    "        for _,label in testset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        #axs[int(i/W), i%W].title(\"worker{}\".format(i+1), fontsize=12, color = \"green\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ## get global dataset\n",
    "    if Centralized:\n",
    "        global_trainset = GlobalDataset(federated_trainset)\n",
    "        global_valset = GlobalDataset(federated_valset)\n",
    "        global_testset =  GlobalDataset(federated_testset)\n",
    "        \n",
    "        #show_cifer(global_trainset.data,global_testset.label, cifar10_labels)\n",
    "\n",
    "        global_trainset.transform = transform_train\n",
    "        global_valset.transform = transform_test\n",
    "        global_testset.transform = transform_test\n",
    "\n",
    "        global_trainloader = torch.utils.data.DataLoader(global_trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "        global_valloader = torch.utils.data.DataLoader(global_valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "        global_testloader = torch.utils.data.DataLoader(global_testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "\n",
    "    ## set transform\n",
    "    for i in range(args.worker_num):\n",
    "        federated_trainset[i].transform = transform_train\n",
    "        federated_valset[i].transform = transform_test\n",
    "        federated_testset[i].transform = transform_test\n",
    "    \n",
    "    if Centralized and unlabeled_data:\n",
    "        return federated_trainset,federated_valset,federated_testset,global_trainloader,global_valloader,global_testloader,unlabeled_dataset\n",
    "    if Centralized:\n",
    "        return federated_trainset,federated_valset,federated_testset,global_trainloader,global_valloader,global_testloader\n",
    "    elif unlabeled_data:\n",
    "        return federated_trainset,federated_valset,federated_testset,unlabeled_dataset\n",
    "    else:\n",
    "        return federated_trainset,federated_valset,federated_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train:49000 Test:10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "federated_trainset,federated_valset,federated_testset,unlabeled_dataset = get_dataset(unlabeled_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39191, 9809, 10000]\n"
     ]
    }
   ],
   "source": [
    "total = [0,0,0]\n",
    "for i in range(args.worker_num):\n",
    "    total[0]+=len(federated_trainset[i])\n",
    "    total[1]+=len(federated_valset[i])\n",
    "    total[2]+=len(federated_testset[i])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZU3vAAb9-6SD"
   },
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG model \n",
    "    '''\n",
    "    def __init__(self, features, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            #print(\"in_channels: {}, v: {}\".format(in_channels, v))\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
    "          512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGGConvBlocks(nn.Module):\n",
    "    '''\n",
    "    VGG containers that only contains the conv layers \n",
    "    '''\n",
    "    def __init__(self, features, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class VGGContainer(nn.Module):\n",
    "    '''\n",
    "    VGG model \n",
    "    '''\n",
    "    def __init__(self, features, input_dim, hidden_dims, num_classes=10):\n",
    "        super(VGGContainer, self).__init__()\n",
    "        self.features = features\n",
    "        # note: we hard coded here a bit by assuming we only have two hidden layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dims[1], num_classes),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def matched_vgg11(matched_shapes):\n",
    "    # [(67, 27), (67,), (132, 603), (132,), (260, 1188), (260,), (261, 2340), (261,), (516, 2349), (516,), (517, 4644), (517,), \n",
    "    # (516, 4653), (516,), (516, 4644), (516,), (516, 515), (515,), (515, 515), (515,), (515, 10), (10,)]\n",
    "    processed_matched_shape = [matched_shapes[0][0], \n",
    "                                'M', \n",
    "                                matched_shapes[2][0], \n",
    "                                'M', \n",
    "                                matched_shapes[4][0], \n",
    "                                matched_shapes[6][0], \n",
    "                                'M', \n",
    "                                matched_shapes[8][0], \n",
    "                                matched_shapes[10][0], \n",
    "                                'M', \n",
    "                                matched_shapes[12][0], \n",
    "                                matched_shapes[14][0], \n",
    "                                'M']\n",
    "    return VGGContainer(make_layers(processed_matched_shape), input_dim=matched_shapes[16][0], \n",
    "            hidden_dims=[matched_shapes[16][1], matched_shapes[18][1]], num_classes=10)\n",
    "\n",
    "\n",
    "def vgg11():\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
    "    return VGG(make_layers(cfg['A']))\n",
    "\n",
    "\n",
    "def vgg11_bn(num_classes=10):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['A'], batch_norm=True), num_classes=num_classes)\n",
    "\n",
    "\n",
    "def vgg13():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n",
    "    return VGG(make_layers(cfg['B']))\n",
    "\n",
    "\n",
    "def vgg13_bn():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg16():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n",
    "    return VGG(make_layers(cfg['D']))\n",
    "\n",
    "\n",
    "def vgg16_bn():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n",
    "    return VGG(make_layers(cfg['E']))\n",
    "\n",
    "\n",
    "def vgg19_bn():\n",
    "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['E'], batch_norm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans(object):\n",
    "\n",
    "    def __init__(self, n_clusters=2, max_iter=300):\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        self.cluster_centers_ = None\n",
    "\n",
    "    def fit_predict(self, features):\n",
    "\n",
    "        feature_indexes = np.arange(len(features))\n",
    "        np.random.shuffle(feature_indexes)\n",
    "        initial_centroid_indexes = feature_indexes[:self.n_clusters]\n",
    "        self.cluster_centers_ = features[initial_centroid_indexes]\n",
    "\n",
    "        pred = np.zeros(features.shape)\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "\n",
    "            new_pred = np.array([\n",
    "                np.array([\n",
    "                    self.Euclidean_distance(p, centroid)\n",
    "                    for centroid in self.cluster_centers_\n",
    "                ]).argmin()\n",
    "                for p in features\n",
    "            ])\n",
    "\n",
    "            if np.all(new_pred == pred):\n",
    "                break\n",
    "\n",
    "            pred = new_pred\n",
    "            \n",
    "            self.cluster_centers_ = np.array([features[pred == i].mean(axis=0)\n",
    "                                              for i in range(self.n_clusters)])\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def KLD(self, p0, p1):\n",
    "        P = torch.from_numpy(p0.astype(np.float32)).clone()\n",
    "        Q = torch.from_numpy(p1.astype(np.float32)).clone()\n",
    "        P = F.softmax(Variable(P), dim=1)\n",
    "        Q = F.softmax(Variable(Q), dim=1)\n",
    "        kld = ((P/(P+Q))*(P * (P / ((P/(P+Q))*P + (Q/(P+Q))*Q)).log())).sum() + ((Q/(P+Q))*(Q * (Q / ((P/(P+Q))*P + (Q/(P+Q))*Q)).log())).sum()\n",
    "        return kld\n",
    "    \n",
    "    def Euclidean_distance(self, p0, p1):\n",
    "        return np.sum((p0 - p1) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self,unlabeled_dataset):\n",
    "    self.cluster = None\n",
    "    self.models = None\n",
    "    self.unlabeled_dataloader = torch.utils.data.DataLoader(unlabeled_dataset,batch_size=args.batch_size,shuffle=False,num_workers=2)\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset,client_best_model):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(i,federated_trainset[i],federated_valset[i],federated_testset[i],client_best_model[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "  def collect_model(self,workers):\n",
    "    self.models = [None]*args.worker_num\n",
    "    for worker in workers:\n",
    "      self.models[worker.id] = copy.deepcopy(worker.local_model)\n",
    "\n",
    "  def send_model(self,workers):\n",
    "    for worker in workers:\n",
    "      worker.local_model = copy.deepcopy(self.models[worker.id])\n",
    "      worker.other_model = copy.deepcopy(self.models[worker.other_model_id])\n",
    "        \n",
    "  def return_model(self,workers):\n",
    "    for worker in workers:\n",
    "      worker.local_model = copy.deepcopy(self.models[worker.local_model_id])\n",
    "      worker.local_model_id = worker.id\n",
    "    del self.models\n",
    "    \n",
    "  def aggregate_model(self,workers):   \n",
    "    new_params = []\n",
    "    train_model_id = []\n",
    "    train_model_id_count = []\n",
    "    for worker in workers:\n",
    "      worker_state = worker.local_model.state_dict()\n",
    "      if worker.id in train_model_id:\n",
    "        i = train_model_id.index(worker.id)\n",
    "        for key in worker_state.keys():\n",
    "          new_params[i][key] += worker_state[key]\n",
    "        train_model_id_count[i] += 1\n",
    "      else:\n",
    "        new_params.append(OrderedDict())\n",
    "        train_model_id.append(worker.id)\n",
    "        train_model_id_count.append(1)\n",
    "        i = train_model_id.index(worker.id)\n",
    "        for key in worker_state.keys():\n",
    "          new_params[i][key] = worker_state[key]\n",
    "        \n",
    "      worker_state = worker.other_model.state_dict()\n",
    "      if worker.other_model_id in train_model_id:\n",
    "        i = train_model_id.index(worker.other_model_id)\n",
    "        for key in worker_state.keys():\n",
    "          new_params[i][key] += worker_state[key]\n",
    "        train_model_id_count[i] += 1\n",
    "      else:\n",
    "        new_params.append(OrderedDict())\n",
    "        train_model_id.append(worker.other_model_id)\n",
    "        train_model_id_count.append(1)\n",
    "        i = train_model_id.index(worker.other_model_id)\n",
    "        for key in worker_state.keys():\n",
    "          new_params[i][key] = worker_state[key]\n",
    "        \n",
    "      worker.local_model = worker.local_model.to('cpu')\n",
    "      worker.other_model = worker.other_model.to('cpu')\n",
    "      del worker.local_model,worker.other_model\n",
    "    \n",
    "    for i,model_id in enumerate(train_model_id):\n",
    "      for key in new_params[i].keys():\n",
    "        new_params[i][key] = new_params[i][key]/train_model_id_count[i]\n",
    "      self.models[model_id].load_state_dict(new_params[i])\n",
    "      \n",
    "  '''clustering by kmeans'''  \n",
    "  def clustering(self,workers):\n",
    "    if args.cluster_num==1:\n",
    "        pred = [0]*len(workers)\n",
    "        worker_id_list = []\n",
    "        for worker in workers:\n",
    "            worker_id_list.append(worker.id)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            worker_softmax_targets = [[] for i in range(len(workers))]\n",
    "            worker_id_list = []\n",
    "            count = 0\n",
    "            for i,model in enumerate(self.models):\n",
    "              if model==None:\n",
    "                pass\n",
    "              else:\n",
    "                model = model.to(args.device)\n",
    "                model.eval()\n",
    "                for data,_ in self.unlabeled_dataloader:\n",
    "                  data = data.to(args.device)\n",
    "                  worker_softmax_targets[count].append(model(data).to('cpu').detach().numpy())\n",
    "                worker_softmax_targets[count] = np.array(worker_softmax_targets[count])\n",
    "                model = model.to('cpu')\n",
    "                worker_id_list.append(i)\n",
    "                count += 1\n",
    "            worker_softmax_targets = np.array(worker_softmax_targets)\n",
    "            kmeans = KMeans(n_clusters=args.cluster_num)\n",
    "            pred = kmeans.fit_predict(worker_softmax_targets)\n",
    "    self.cluster = []\n",
    "    for i in range(args.cluster_num):\n",
    "      self.cluster.append([])\n",
    "    for i,cls in enumerate(pred):\n",
    "      self.cluster[cls].append(worker_id_list[i])\n",
    "    for worker in workers:\n",
    "      idx = worker_id_list.index(worker.id)\n",
    "      worker.cluster_num = pred[idx]\n",
    "        \n",
    "  def decide_other_model(self,workers):\n",
    "    for worker in workers:\n",
    "      cls = worker.cluster_num\n",
    "      '''if number of worker in cluster is one, other model is decided by random in all workers. '''\n",
    "      if len(self.cluster[cls])==1:\n",
    "        while True:\n",
    "          other_worker = random.choice(workers)\n",
    "          other_model_id = other_worker.id\n",
    "          if worker.id!=other_model_id:\n",
    "            break\n",
    "      else:\n",
    "        while True:\n",
    "          other_model_id = random.choice(self.cluster[cls])\n",
    "          if worker.id!=other_model_id:\n",
    "            break\n",
    "      worker.other_model_id = other_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,i,trainset,valset,testset,best_model):\n",
    "    self.id = i\n",
    "    self.cluster_num = None\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    if best_model==1:\n",
    "      self.local_model = vgg11()\n",
    "    elif best_model==2:\n",
    "      self.local_model = vgg13()\n",
    "    elif best_model==3:\n",
    "      self.local_model = vgg16()\n",
    "    elif best_model==4:\n",
    "      self.local_model = vgg19()\n",
    "    self.local_model_id = i\n",
    "    self.other_model = None\n",
    "    self.other_model_id = None\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "\n",
    "  def local_train(self):\n",
    "    self.local_model = self.local_model.to(args.device)\n",
    "    self.other_model = self.other_model.to(args.device)\n",
    "    local_optimizer = optim.SGD(self.local_model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "    other_optimizer = optim.SGD(self.other_model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "    self.local_model.train()\n",
    "    self.other_model.train()\n",
    "    for epoch in range(args.local_epochs):\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in self.trainloader:\n",
    "        data,labels = Variable(data),Variable(labels)\n",
    "        data,labels = data.to(args.device),labels.to(args.device)\n",
    "        local_optimizer.zero_grad()\n",
    "        other_optimizer.zero_grad()\n",
    "        local_outputs = self.local_model(data)\n",
    "        other_outputs = self.other_model(data)\n",
    "        #train local_model\n",
    "        ce_loss = args.criterion_ce(local_outputs,labels)\n",
    "        kl_loss = args.criterion_kl(F.log_softmax(local_outputs, dim = 1),F.softmax(Variable(other_outputs), dim=1))\n",
    "        loss = ce_loss + kl_loss\n",
    "        running_loss += loss.item()\n",
    "        predicted = torch.argmax(local_outputs,dim=1)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "        count += len(labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), args.clip)\n",
    "        local_optimizer.step()\n",
    "\n",
    "        #train other_model\n",
    "        ce_loss = args.criterion_ce(other_outputs,labels)\n",
    "        kl_loss = args.criterion_kl(F.log_softmax(other_outputs, dim = 1),F.softmax(Variable(local_outputs), dim=1))\n",
    "        loss = ce_loss + kl_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.other_model.parameters(), args.clip)\n",
    "        other_optimizer.step()\n",
    "        \n",
    "    return 100.0*correct/count,running_loss/len(self.trainloader)\n",
    "\n",
    "        \n",
    "  def validate(self):\n",
    "    acc,loss = test(self.local_model,args.criterion_ce,self.valloader)\n",
    "    return acc,loss\n",
    "\n",
    "\n",
    "  def model_replacement(self):\n",
    "    _,loss_local = test(self.local_model,args.criterion_ce,self.valloader)\n",
    "    _,loss_other = test(self.other_model,args.criterion_ce,self.valloader)\n",
    "    if loss_other<loss_local:\n",
    "        self.local_model_id = self.other_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7-GY66gROuEU"
   },
   "outputs": [],
   "source": [
    "def train(model,criterion,trainloader,epochs):\n",
    "  optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for (data,labels) in trainloader:\n",
    "      data,labels = Variable(data),Variable(labels)\n",
    "      data,labels = data.to(args.device),labels.to(args.device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(data)\n",
    "      loss = criterion(outputs,labels)\n",
    "      running_loss += loss.item()\n",
    "      predicted = torch.argmax(outputs,dim=1)\n",
    "      correct += (predicted==labels).sum().item()\n",
    "      count += len(labels)\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "      optimizer.step()\n",
    "\n",
    "  return 100.0*correct/count,running_loss/len(trainloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  count = 0\n",
    "  for (data,labels) in testloader:\n",
    "    data,labels = data.to(args.device),labels.to(args.device)\n",
    "    outputs = model(data)\n",
    "    running_loss += criterion(outputs,labels).item()\n",
    "    predicted = torch.argmax(outputs,dim=1)\n",
    "    correct += (predicted==labels).sum().item()\n",
    "    count += len(labels)\n",
    "\n",
    "  accuracy = 100.0*correct/count\n",
    "  loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "  return accuracy,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WMO7_WSLHeGl"
   },
   "outputs": [],
   "source": [
    "class Early_Stopping():\n",
    "  def __init__(self,partience):\n",
    "    self.step = 0\n",
    "    self.loss = float('inf')\n",
    "    self.partience = partience\n",
    "\n",
    "  def validate(self,loss):\n",
    "    if self.loss<loss:\n",
    "      self.step += 1\n",
    "      if self.step>self.partience:\n",
    "        return True\n",
    "    else:\n",
    "      self.step = 0\n",
    "      self.loss = loss\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../client_best_model/client_best_model_cifar10_42.csv') as fp:\n",
    "    csvList = list(csv.reader(fp))\n",
    "client_best_model = [int(item) for subList in csvList for item in subList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "-noG_98IR-nZ",
    "outputId": "78a6ebe2-854a-4f83-dc45-5c4ac35b69e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1  loss:1.7584694743156433  accuracy:42.59606778372204\n",
      "Epoch2  loss:1.5236206412315365  accuracy:44.05957070839748\n",
      "Epoch3  loss:1.5245722144842149  accuracy:43.98694586440018\n",
      "Epoch4  loss:1.527425765991211  accuracy:43.79653328393967\n",
      "Epoch5  loss:1.5282279461622237  accuracy:44.72084428811865\n",
      "Epoch6  loss:1.563296610116959  accuracy:45.15482590905318\n",
      "Epoch7  loss:1.4778601437807082  accuracy:44.25939352346806\n",
      "Epoch8  loss:1.5054776847362517  accuracy:45.09796270230695\n",
      "Epoch9  loss:1.5064044594764712  accuracy:45.66320737997284\n",
      "Epoch10  loss:1.4515793114900588  accuracy:46.05181459680158\n",
      "Epoch11  loss:1.4186937838792801  accuracy:47.9681866233663\n",
      "Epoch12  loss:1.4451943695545197  accuracy:50.45497444368905\n",
      "Epoch13  loss:1.3802657991647724  accuracy:52.57140013801806\n",
      "Epoch14  loss:1.3419563442468645  accuracy:51.86087456436666\n",
      "Epoch15  loss:1.3299528390169144  accuracy:53.26601271792851\n",
      "Epoch16  loss:1.273381093144417  accuracy:54.31866926006681\n",
      "Epoch17  loss:1.291483849287033  accuracy:55.00883304817202\n",
      "Epoch18  loss:1.2972455829381944  accuracy:54.937796145147104\n",
      "Epoch19  loss:1.2408871829509736  accuracy:55.935262449614825\n",
      "Epoch20  loss:1.211331155896187  accuracy:56.52369594153559\n",
      "Epoch21  loss:1.2065081924200056  accuracy:57.11351072088659\n",
      "Epoch22  loss:1.193326511979103  accuracy:56.71582458252897\n",
      "Epoch23  loss:1.2193185925483703  accuracy:56.52103206419317\n",
      "Epoch24  loss:1.1710073292255403  accuracy:57.84070328932496\n",
      "Epoch25  loss:1.1617693245410918  accuracy:58.376113934438536\n",
      "Epoch26  loss:1.146079732477665  accuracy:59.250246589934534\n",
      "Epoch27  loss:1.1400979638099673  accuracy:58.600091287365416\n",
      "Epoch28  loss:1.1319414958357812  accuracy:59.37972209726094\n",
      "Epoch29  loss:1.1356025725603103  accuracy:59.09683974152681\n",
      "Epoch30  loss:1.1225831389427183  accuracy:58.232006412810506\n",
      "Epoch31  loss:1.1351462781429291  accuracy:58.46331226552181\n",
      "Epoch32  loss:1.1388900220394136  accuracy:59.68014803675929\n",
      "Epoch33  loss:1.0907510280609134  accuracy:60.41564743119698\n",
      "Epoch34  loss:1.0991987526416778  accuracy:61.20151716250172\n",
      "Epoch35  loss:1.08931465446949  accuracy:61.39441981941839\n",
      "Epoch36  loss:1.0815183758735656  accuracy:61.19013250644816\n",
      "Epoch37  loss:1.0709977969527245  accuracy:61.76493609975157\n",
      "Epoch38  loss:1.0960805386304853  accuracy:60.73639529498293\n",
      "Epoch39  loss:1.0572168201208114  accuracy:61.491996154438034\n",
      "Epoch40  loss:1.0488997757434846  accuracy:62.8432057115501\n",
      "Epoch41  loss:1.0486341252923013  accuracy:62.04553382548713\n",
      "Epoch42  loss:1.033947268128395  accuracy:62.698241031863994\n",
      "Epoch43  loss:1.0428147703409194  accuracy:62.49165093071193\n",
      "Epoch44  loss:1.0460012823343279  accuracy:62.5812221252084\n",
      "Epoch45  loss:1.012212736904621  accuracy:63.579574002411974\n",
      "Epoch46  loss:1.0227129995822908  accuracy:63.0296136075913\n",
      "Epoch47  loss:1.0249991506338119  accuracy:63.87339630178057\n",
      "Epoch48  loss:1.005623497068882  accuracy:63.52024406363074\n",
      "Epoch49  loss:1.0019357040524484  accuracy:64.45413137403206\n",
      "Epoch50  loss:1.0033905029296877  accuracy:64.13009387301753\n",
      "Epoch51  loss:0.9921318903565408  accuracy:64.2235445870921\n",
      "Epoch52  loss:0.9931443691253662  accuracy:64.19175844930255\n",
      "Epoch53  loss:0.9785969376564025  accuracy:65.29399926321607\n",
      "Epoch54  loss:0.9712464854121209  accuracy:65.15827873077873\n",
      "Epoch55  loss:0.9611919328570366  accuracy:65.74285511125753\n",
      "Epoch56  loss:0.9652702018618582  accuracy:66.30465850270336\n",
      "Epoch57  loss:0.9577296078205109  accuracy:66.0288951235662\n",
      "Epoch58  loss:0.9633005172014235  accuracy:66.00667046334097\n",
      "Epoch59  loss:0.9480446964502336  accuracy:66.53347676957351\n",
      "Epoch60  loss:0.9555289775133132  accuracy:66.46823025860554\n",
      "Epoch61  loss:0.9334063425660132  accuracy:67.28313161342993\n",
      "Epoch62  loss:0.9384549424052241  accuracy:66.35041091354807\n",
      "Epoch63  loss:0.9377565398812294  accuracy:66.93382706720514\n",
      "Epoch64  loss:0.9288599863648415  accuracy:67.13489154416793\n",
      "Epoch65  loss:0.939609183371067  accuracy:66.63923894154308\n",
      "Epoch66  loss:0.9367189466953277  accuracy:66.91712182928356\n",
      "Epoch67  loss:0.9299378424882888  accuracy:67.5123716102799\n",
      "Epoch68  loss:0.9129813969135285  accuracy:67.20135075092817\n",
      "Epoch69  loss:0.914157283306122  accuracy:68.21108872769659\n",
      "Epoch70  loss:0.9093342930078505  accuracy:68.17066179560868\n",
      "Epoch71  loss:0.9072280049324036  accuracy:68.33324514876682\n",
      "Epoch72  loss:0.9063208237290382  accuracy:68.23488419561325\n",
      "Epoch73  loss:0.8957509294152262  accuracy:68.26776229627704\n",
      "Epoch74  loss:0.8883963197469711  accuracy:68.97044113853163\n",
      "Epoch75  loss:0.8789585337042808  accuracy:69.27985563945529\n",
      "Epoch76  loss:0.8647054553031921  accuracy:69.76608842319152\n",
      "Epoch77  loss:0.8766986116766929  accuracy:69.86164089927834\n",
      "Epoch78  loss:0.8628344997763634  accuracy:69.74277361295485\n",
      "Epoch79  loss:0.8608532562851908  accuracy:70.34150321830543\n",
      "Epoch80  loss:0.8779700338840485  accuracy:69.70405375580077\n",
      "Epoch81  loss:0.8515084743499757  accuracy:70.28637341896123\n",
      "Epoch82  loss:0.8512064874172212  accuracy:70.28172607917071\n",
      "Epoch83  loss:0.8435616001486778  accuracy:71.16188153950505\n",
      "Epoch84  loss:0.8524173915386201  accuracy:70.64935379601842\n",
      "Epoch85  loss:0.8358040139079094  accuracy:70.64114190149064\n",
      "Epoch86  loss:0.8220920994877817  accuracy:71.2810808240888\n",
      "Epoch87  loss:0.8342440217733383  accuracy:70.94802803112378\n",
      "Epoch88  loss:0.8288241773843765  accuracy:71.26666364735829\n",
      "Epoch89  loss:0.8116802334785461  accuracy:71.1337986644518\n",
      "Epoch90  loss:0.8183430209755896  accuracy:71.81954127184734\n",
      "Epoch91  loss:0.8058562874794007  accuracy:71.88186691054862\n",
      "Epoch92  loss:0.8016522824764252  accuracy:72.06943234046886\n",
      "Epoch93  loss:0.803351092338562  accuracy:72.51263594242351\n",
      "Epoch94  loss:0.8199642091989519  accuracy:72.36038235638057\n",
      "Epoch95  loss:0.8019752651453018  accuracy:72.18577907270851\n",
      "Epoch96  loss:0.7906771525740623  accuracy:72.7688728743882\n",
      "Epoch97  loss:0.7801904112100602  accuracy:72.98861066538329\n",
      "Epoch98  loss:0.7839349791407586  accuracy:73.19722595478126\n",
      "Epoch99  loss:0.7911284610629082  accuracy:72.99694093819153\n",
      "Epoch100  loss:0.7655423484742641  accuracy:73.1929207505986\n",
      "Epoch101  loss:0.7702802255749703  accuracy:73.46454336338529\n",
      "Epoch102  loss:0.7571923300623893  accuracy:73.57553261958546\n",
      "Epoch103  loss:0.7605964176356792  accuracy:73.76483534012122\n",
      "Epoch104  loss:0.762325145304203  accuracy:74.06294670093297\n",
      "Epoch105  loss:0.7654473692178726  accuracy:73.66257054518519\n",
      "Epoch106  loss:0.7586024761199951  accuracy:73.98758858827178\n",
      "Epoch107  loss:0.7446317434310915  accuracy:74.47479035256282\n",
      "Epoch108  loss:0.7490691274404525  accuracy:74.56396715425046\n",
      "Epoch109  loss:0.7411024630069732  accuracy:74.4617166362698\n",
      "Epoch110  loss:0.73129822909832  accuracy:74.96806236392237\n",
      "Epoch111  loss:0.7392219111323357  accuracy:74.69224987095319\n",
      "Epoch112  loss:0.7338781118392945  accuracy:74.84440526309244\n",
      "Epoch113  loss:0.7430779509246351  accuracy:74.25136517222957\n",
      "Epoch114  loss:0.7269180327653885  accuracy:74.92840121154833\n",
      "Epoch115  loss:0.7288313888013364  accuracy:75.02987230609156\n",
      "Epoch116  loss:0.7313314840197565  accuracy:75.18322858222339\n",
      "Epoch117  loss:0.7235181868076325  accuracy:75.50641095909648\n",
      "Epoch118  loss:0.7131428763270377  accuracy:75.41242230973616\n",
      "Epoch119  loss:0.7118752554059029  accuracy:75.5986712244424\n",
      "Epoch120  loss:0.7167348563671111  accuracy:75.061741115577\n",
      "Epoch121  loss:0.7062236189842224  accuracy:75.46423649236392\n",
      "Epoch122  loss:0.707795538008213  accuracy:75.85445158108455\n",
      "Epoch123  loss:0.7061515316367151  accuracy:75.49889389237661\n",
      "Epoch124  loss:0.6994840294122697  accuracy:76.40890662141513\n",
      "Epoch125  loss:0.711602172255516  accuracy:75.560076832685\n",
      "Epoch126  loss:0.6830458514392375  accuracy:76.20903018044915\n",
      "Epoch127  loss:0.6843379884958267  accuracy:76.27767470587182\n",
      "Epoch128  loss:0.6838130816817283  accuracy:76.64131887636135\n",
      "Epoch129  loss:0.6854981943964958  accuracy:76.16044510821888\n",
      "Epoch130  loss:0.6771683402359485  accuracy:76.82462479440228\n",
      "Epoch131  loss:0.6797823101282119  accuracy:76.54426548753072\n",
      "Epoch132  loss:0.6741519019007683  accuracy:76.77092491941005\n",
      "Epoch133  loss:0.674266529083252  accuracy:76.5324713990246\n",
      "Epoch134  loss:0.6831249594688414  accuracy:76.80366883722948\n",
      "Epoch135  loss:0.6731318235397338  accuracy:77.20105546850282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch136  loss:0.6698842480778694  accuracy:77.16435998694942\n",
      "Epoch137  loss:0.6635412469506264  accuracy:76.71719202568177\n",
      "Epoch138  loss:0.6709774658083916  accuracy:76.74604120981829\n",
      "Epoch139  loss:0.6663427047431472  accuracy:77.17712703193828\n",
      "Epoch140  loss:0.6581012316048145  accuracy:77.5774998794743\n",
      "Epoch141  loss:0.6440126456320285  accuracy:77.68023076613069\n",
      "Epoch142  loss:0.6638354063034058  accuracy:77.43542224929111\n",
      "Epoch143  loss:0.6331939935684203  accuracy:78.1848596445607\n",
      "Epoch144  loss:0.6561351440846919  accuracy:78.1138303573166\n",
      "Epoch145  loss:0.6309553414583207  accuracy:78.34103022033855\n",
      "Epoch146  loss:0.6376551426947117  accuracy:78.28938960992696\n",
      "Epoch147  loss:0.6393580377101898  accuracy:77.90839938045526\n",
      "Epoch148  loss:0.635223727673292  accuracy:78.19703118835987\n",
      "Epoch149  loss:0.6243551850318909  accuracy:78.50041771185366\n",
      "Epoch150  loss:0.6311630584299565  accuracy:78.40914477820893\n",
      "Epoch151  loss:0.6221606895327567  accuracy:78.59120445306083\n",
      "Epoch152  loss:0.6194219060242176  accuracy:78.58535738495027\n",
      "Epoch153  loss:0.6135614819824696  accuracy:78.73794294758135\n",
      "Epoch154  loss:0.6128726221621037  accuracy:78.96831667237309\n",
      "Epoch155  loss:0.623537603020668  accuracy:78.91097094163845\n",
      "Epoch156  loss:0.609011036157608  accuracy:79.37700662330059\n",
      "Epoch157  loss:0.6124077349901199  accuracy:79.30100428227372\n",
      "Epoch158  loss:0.6034528367221356  accuracy:79.564886603655\n",
      "Epoch159  loss:0.6035166904330254  accuracy:79.65639198228178\n",
      "Epoch160  loss:0.606179528683424  accuracy:79.57664975210207\n",
      "Epoch161  loss:0.5963154815137385  accuracy:79.68627526095086\n",
      "Epoch162  loss:0.5869141869246961  accuracy:80.11101011877703\n",
      "Epoch163  loss:0.5862022727727889  accuracy:79.80150837410137\n",
      "Epoch164  loss:0.5911499343812465  accuracy:79.61834954164243\n",
      "Epoch165  loss:0.5858685880899429  accuracy:79.86497751515999\n",
      "Epoch166  loss:0.5864934518933296  accuracy:79.9120264128206\n",
      "Epoch167  loss:0.5779520489275456  accuracy:80.19078070636519\n",
      "Epoch168  loss:0.5852818757295608  accuracy:79.87521663202196\n",
      "Epoch169  loss:0.5907225750386715  accuracy:79.77498032163474\n",
      "Epoch170  loss:0.575625093281269  accuracy:80.72623423975159\n",
      "Epoch171  loss:0.572880980372429  accuracy:80.52912626369955\n",
      "Epoch172  loss:0.5801944613456728  accuracy:80.55070443653143\n",
      "Epoch173  loss:0.5815620988607406  accuracy:80.38295094976094\n",
      "Epoch174  loss:0.5693432554602622  accuracy:80.60077379240998\n",
      "Epoch175  loss:0.5683209352195262  accuracy:81.1015068186425\n",
      "Epoch176  loss:0.5740911982953548  accuracy:80.74824264856125\n",
      "Epoch177  loss:0.5827763676643372  accuracy:80.417948271222\n",
      "Epoch178  loss:0.5776779435575009  accuracy:80.48151717149935\n",
      "Epoch179  loss:0.5675727628171443  accuracy:80.95122396205963\n",
      "Epoch180  loss:0.5829857379198073  accuracy:80.28330846404278\n",
      "Epoch181  loss:0.5685344733297825  accuracy:80.54284424994715\n",
      "Epoch182  loss:0.5525387883186341  accuracy:81.05395476760701\n",
      "Epoch183  loss:0.5585916198790073  accuracy:80.66425334531394\n",
      "Epoch184  loss:0.569772657006979  accuracy:80.55542605921985\n",
      "Epoch185  loss:0.5568660460412503  accuracy:80.83736091537048\n",
      "Epoch186  loss:0.5561667121946812  accuracy:81.20671927336565\n",
      "Epoch187  loss:0.5510514989495277  accuracy:81.33133056872308\n",
      "Epoch188  loss:0.5528948344290257  accuracy:81.07422951857117\n",
      "Epoch189  loss:0.5477253556251525  accuracy:81.55243132597575\n",
      "Epoch190  loss:0.5464726038277149  accuracy:81.2310781792558\n",
      "Epoch191  loss:0.5363866046071053  accuracy:81.74215500253982\n",
      "Epoch192  loss:0.5423265531659126  accuracy:81.44248391026193\n",
      "Epoch193  loss:0.5448134429752827  accuracy:81.69994573053984\n",
      "Epoch194  loss:0.5433294497430324  accuracy:81.55605673032166\n",
      "Epoch195  loss:0.5466813735663891  accuracy:81.51405330136825\n",
      "Epoch196  loss:0.5371303297579288  accuracy:81.68334867549186\n",
      "Epoch197  loss:0.5388929612934589  accuracy:81.62035042784169\n",
      "Epoch198  loss:0.5336942374706268  accuracy:81.72036219678262\n",
      "Epoch199  loss:0.5376104585826397  accuracy:81.9721808259594\n",
      "Epoch200  loss:0.5386977396905422  accuracy:81.72161153885597\n",
      "Epoch201  loss:0.5347896367311478  accuracy:82.01180072207029\n",
      "Epoch202  loss:0.5309023819863797  accuracy:82.43217633170633\n",
      "Epoch203  loss:0.5271438829600811  accuracy:81.79070234656754\n",
      "Epoch204  loss:0.5218227937817573  accuracy:82.27067609823258\n",
      "Epoch205  loss:0.52728431224823  accuracy:82.10254483712396\n",
      "Epoch206  loss:0.5304200671613216  accuracy:82.24852496241084\n",
      "Epoch207  loss:0.5228160291910171  accuracy:82.30278860829833\n",
      "Epoch208  loss:0.5209413729608059  accuracy:82.18196245160807\n",
      "Epoch209  loss:0.5087996311485767  accuracy:82.57303201625643\n",
      "Epoch210  loss:0.5198292098939419  accuracy:82.20860199840013\n",
      "Epoch211  loss:0.5153929524123668  accuracy:82.59083214928586\n",
      "Epoch212  loss:0.5197501882910728  accuracy:82.65669767610056\n",
      "Epoch213  loss:0.5179809644818306  accuracy:82.89337442084276\n",
      "Epoch214  loss:0.5130809530615807  accuracy:82.63122607768638\n",
      "Epoch215  loss:0.516046816855669  accuracy:82.36355126355467\n",
      "Epoch216  loss:0.5106969960033892  accuracy:82.36182597913087\n",
      "Epoch217  loss:0.5148758858442307  accuracy:83.14060127481922\n",
      "Epoch218  loss:0.5195737056434155  accuracy:82.73530100803958\n",
      "Epoch219  loss:0.5061123333871363  accuracy:82.97950905306955\n",
      "Epoch220  loss:0.4982220873236657  accuracy:83.27201424550114\n",
      "Epoch221  loss:0.5043788060545922  accuracy:82.85008849212463\n",
      "Epoch222  loss:0.4935764037072658  accuracy:83.38131936422829\n",
      "Epoch223  loss:0.4915246590971947  accuracy:83.73573171255963\n",
      "Epoch224  loss:0.4984601944684981  accuracy:82.80418172455796\n",
      "Epoch225  loss:0.48801366649568084  accuracy:83.57599377575605\n",
      "Epoch226  loss:0.4991097599267959  accuracy:83.21006176506472\n",
      "Epoch227  loss:0.5082689821720123  accuracy:83.45847604770154\n",
      "Epoch228  loss:0.4889886520802975  accuracy:83.67577729418215\n",
      "Epoch229  loss:0.4769403725862503  accuracy:83.89176591989104\n",
      "Epoch230  loss:0.4890491567552088  accuracy:83.55952540356719\n",
      "Epoch231  loss:0.4825068622827529  accuracy:83.60171535312224\n",
      "Epoch232  loss:0.4787006311118603  accuracy:83.75984781953727\n",
      "Epoch233  loss:0.4848758228123187  accuracy:83.79764068607466\n",
      "Epoch234  loss:0.4769111469388007  accuracy:84.072952183155\n",
      "Epoch235  loss:0.4808696642518044  accuracy:83.76410172620895\n",
      "Epoch236  loss:0.4785548679530621  accuracy:84.10466437093952\n",
      "Epoch237  loss:0.4720723919570446  accuracy:84.27162178430112\n",
      "Epoch238  loss:0.47418754287064074  accuracy:84.10508378023822\n",
      "Epoch239  loss:0.4832153104245662  accuracy:83.88896582990166\n",
      "Epoch240  loss:0.4753348074853419  accuracy:84.21903428959904\n",
      "Epoch241  loss:0.47527120634913445  accuracy:83.7026082399641\n",
      "Epoch242  loss:0.4626683961600065  accuracy:84.3141944907152\n",
      "Epoch243  loss:0.457653295248747  accuracy:84.43577844280523\n",
      "Epoch244  loss:0.45989101529121396  accuracy:84.71739179801406\n",
      "Epoch245  loss:0.4660477263852954  accuracy:84.25497998896121\n",
      "Epoch246  loss:0.46430304050445553  accuracy:84.57662476409283\n",
      "Epoch247  loss:0.466571356356144  accuracy:84.51000312783503\n",
      "Epoch248  loss:0.4509715512394905  accuracy:85.04282345208692\n",
      "Epoch249  loss:0.4609944056719541  accuracy:84.64724620514902\n",
      "Epoch250  loss:0.4548912547528744  accuracy:84.91678464515432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acc12212vx/jupyter_env/lib/python3.6/site-packages/ipykernel_launcher.py:48: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch251  loss:0.4608701281249523  accuracy:84.61927337643378\n",
      "Epoch252  loss:0.4446806754916906  accuracy:85.09165405906126\n",
      "Epoch253  loss:0.44391835145652275  accuracy:85.39934584784294\n",
      "Epoch254  loss:0.45495100365951663  accuracy:84.54557010294388\n",
      "Epoch255  loss:0.4657270371913909  accuracy:84.54429725029347\n",
      "Epoch256  loss:0.45092328004539006  accuracy:85.00209531253864\n",
      "Epoch257  loss:0.4495008669793606  accuracy:84.89287872563501\n",
      "Epoch258  loss:0.4555936288088561  accuracy:84.69325197349079\n",
      "Epoch259  loss:0.4502113096415996  accuracy:85.24327598998565\n",
      "Epoch260  loss:0.44392202831804756  accuracy:85.23168409794762\n",
      "Epoch261  loss:0.4465039547532797  accuracy:84.99000754722991\n",
      "Epoch262  loss:0.4493681117892266  accuracy:84.9313819021061\n",
      "Epoch263  loss:0.44729447439312936  accuracy:85.14901510787668\n",
      "Epoch264  loss:0.4438075341284275  accuracy:85.38186233846797\n",
      "Epoch265  loss:0.4412774061784148  accuracy:85.37252126198882\n",
      "Epoch266  loss:0.4385430753231049  accuracy:85.42222135324599\n",
      "Epoch267  loss:0.44694777466356755  accuracy:85.20787580056425\n",
      "Epoch268  loss:0.44564261194318533  accuracy:85.06744176109315\n",
      "Epoch269  loss:0.4382409267127514  accuracy:85.288010212634\n",
      "Epoch270  loss:0.43921231143176553  accuracy:85.44867573846713\n",
      "Epoch271  loss:0.45188112594187263  accuracy:85.1115544790155\n",
      "Epoch272  loss:0.448204405605793  accuracy:85.03220637673991\n",
      "Epoch273  loss:0.43129538744688034  accuracy:85.46801269532834\n",
      "Epoch274  loss:0.43768855948001134  accuracy:85.39161379570427\n",
      "Epoch275  loss:0.43132015615701674  accuracy:85.92170849334481\n",
      "Epoch276  loss:0.43675450831651685  accuracy:85.68908607097052\n",
      "Epoch277  loss:0.43930119145661584  accuracy:85.80500275722625\n",
      "Epoch278  loss:0.4365067079663277  accuracy:85.51830775169859\n",
      "Epoch279  loss:0.4382824197411537  accuracy:85.58154400428782\n",
      "Epoch280  loss:0.43571491166949267  accuracy:85.56316732713452\n",
      "Epoch281  loss:0.43562386836856604  accuracy:85.42036963850575\n",
      "Epoch282  loss:0.4373172298073769  accuracy:85.54134797651787\n",
      "Epoch283  loss:0.42957074493169783  accuracy:85.83498659781625\n",
      "Epoch284  loss:0.4290641844272614  accuracy:85.75229024012137\n",
      "Epoch285  loss:0.4267229206860066  accuracy:85.44851239556745\n",
      "Epoch286  loss:0.42382108103483923  accuracy:85.84955795494714\n",
      "Epoch287  loss:0.4258640734478832  accuracy:85.7790339091635\n",
      "Epoch288  loss:0.42638670299202197  accuracy:85.51961715531422\n",
      "Epoch289  loss:0.42062505297362807  accuracy:85.7696047972122\n",
      "Epoch290  loss:0.4301485486328602  accuracy:85.86262616611705\n",
      "Epoch291  loss:0.425939342752099  accuracy:85.70917407973305\n",
      "Epoch292  loss:0.42723145335912704  accuracy:86.05734795098883\n",
      "Epoch293  loss:0.43471285104751595  accuracy:85.59609664493739\n",
      "Epoch294  loss:0.4228360652923584  accuracy:85.71839758607081\n",
      "Epoch295  loss:0.42185164503753186  accuracy:85.86780684198287\n",
      "Epoch296  loss:0.4255684876814485  accuracy:86.1580675570119\n",
      "Epoch297  loss:0.4172354765236378  accuracy:85.938317574707\n",
      "Epoch298  loss:0.4230627901852131  accuracy:85.90536112362753\n",
      "Epoch299  loss:0.4208862336352468  accuracy:86.22168017421606\n",
      "Epoch300  loss:0.41443205140531064  accuracy:86.13646028317046\n",
      "Epoch301  loss:0.41742874719202516  accuracy:86.32055879375658\n",
      "Epoch302  loss:0.4177963100373744  accuracy:86.07198779444293\n",
      "Epoch303  loss:0.41692559570074084  accuracy:86.24534977312986\n",
      "Epoch304  loss:0.4045443607494235  accuracy:86.50649713658626\n",
      "Epoch305  loss:0.4121076077222825  accuracy:86.37327584239357\n",
      "Epoch306  loss:0.40860805790871385  accuracy:86.70240055287135\n",
      "Epoch307  loss:0.4126154295168816  accuracy:86.26900015893156\n",
      "Epoch308  loss:0.4123076742514967  accuracy:86.23045603942519\n",
      "Epoch309  loss:0.40647343304008243  accuracy:86.74571262136612\n",
      "Epoch310  loss:0.4026622185483575  accuracy:86.53505205876672\n",
      "Epoch311  loss:0.4075435482896864  accuracy:86.28916120480292\n",
      "Epoch312  loss:0.40798234893009067  accuracy:86.60675165332157\n",
      "Epoch313  loss:0.41011708024889226  accuracy:86.50484959960396\n",
      "Epoch314  loss:0.40979454061016446  accuracy:86.50173004129383\n",
      "Epoch315  loss:0.41621579993516206  accuracy:86.45320206063396\n",
      "Epoch316  loss:0.4107781819067895  accuracy:86.50279545504775\n",
      "Epoch317  loss:0.41372981704771533  accuracy:86.2192512332868\n",
      "Epoch318  loss:0.41208799313753847  accuracy:86.45206776654408\n",
      "Epoch319  loss:0.4067535560578108  accuracy:86.79409664965559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acc12212vx/jupyter_env/lib/python3.6/site-packages/ipykernel_launcher.py:56: RuntimeWarning: Mean of empty slice.\n",
      "/home/acc12212vx/jupyter_env/lib/python3.6/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch320  loss:0.40930587965995086  accuracy:86.45087276342126\n",
      "Epoch321  loss:0.4125158237293362  accuracy:86.02461476304259\n",
      "Epoch322  loss:0.41064875461161143  accuracy:86.41787369093804\n",
      "Epoch323  loss:0.4109006432816386  accuracy:86.37717268492003\n",
      "Epoch324  loss:0.4076172970235348  accuracy:86.59705204835856\n",
      "Epoch325  loss:0.4103543240576982  accuracy:86.41944496456469\n",
      "Epoch326  loss:0.4074804082512856  accuracy:86.49427268127117\n",
      "Epoch327  loss:0.4115106572397054  accuracy:86.33065594752935\n",
      "Epoch328  loss:0.4061454679816961  accuracy:86.48428133276946\n",
      "Epoch329  loss:0.41495143449865285  accuracy:86.16558912423007\n",
      "Epoch330  loss:0.4169157955795526  accuracy:86.11130902819332\n",
      "Epoch331  loss:0.409582949243486  accuracy:86.5033000244823\n",
      "Epoch332  loss:0.4005668675526976  accuracy:86.96002315424553\n",
      "Epoch333  loss:0.41090300399810065  accuracy:86.56591756538455\n",
      "Epoch334  loss:0.40040800329297777  accuracy:86.75128983088635\n",
      "Epoch335  loss:0.4032193958759307  accuracy:86.54144707694783\n",
      "Epoch336  loss:0.40109250345267355  accuracy:86.62706383866467\n",
      "Epoch337  loss:0.40049792327918116  accuracy:86.46452012016049\n",
      "Epoch338  loss:0.40053252112120397  accuracy:86.86576335384004\n",
      "Epoch339  loss:0.3986664697527885  accuracy:86.85555331359203\n",
      "Epoch340  loss:0.3966429392807186  accuracy:86.78295891066381\n",
      "Epoch341  loss:0.39424286540597675  accuracy:86.98185956487129\n",
      "Epoch342  loss:0.40025223027914764  accuracy:86.86079034137576\n",
      "Epoch343  loss:0.3985267800278962  accuracy:86.77871089601808\n",
      "Epoch344  loss:0.39451962027233095  accuracy:87.07394378338125\n",
      "Epoch345  loss:0.3885706975124776  accuracy:87.37969483963778\n",
      "Epoch346  loss:0.3927430301904678  accuracy:87.07540069722462\n",
      "Epoch347  loss:0.3906766273081303  accuracy:87.0156349581461\n",
      "Epoch348  loss:0.39083637045696373  accuracy:86.7844869003187\n",
      "Epoch349  loss:0.3895794008858502  accuracy:87.26540446260225\n",
      "Epoch350  loss:0.3949911862611771  accuracy:86.75330753883306\n",
      "Epoch351  loss:0.3958507537841797  accuracy:86.89535073484363\n",
      "Epoch352  loss:0.38954743454232815  accuracy:86.98479837617569\n",
      "Epoch353  loss:0.39072824399918316  accuracy:87.03669066108678\n",
      "Epoch354  loss:0.3944985435344279  accuracy:86.98220789135405\n",
      "Epoch355  loss:0.3908947831019759  accuracy:87.31940979902889\n",
      "Epoch356  loss:0.39888042556121944  accuracy:87.019285435877\n",
      "Epoch357  loss:0.3932229381985962  accuracy:86.87746053829576\n",
      "Epoch358  loss:0.39134034020826225  accuracy:86.78285624630735\n",
      "Epoch359  loss:0.3902048131451012  accuracy:87.02382102727135\n",
      "Epoch360  loss:0.4005897050723434  accuracy:86.88751056154125\n",
      "Epoch361  loss:0.3897821243852377  accuracy:86.86983558308549\n",
      "Epoch362  loss:0.3890392259927466  accuracy:87.35443224112927\n",
      "Epoch363  loss:0.39027479523792863  accuracy:87.07574666873307\n",
      "Epoch364  loss:0.3841316260164604  accuracy:87.18478959798702\n",
      "Epoch365  loss:0.3976755680982024  accuracy:86.9139636920136\n",
      "Epoch366  loss:0.38149625053629277  accuracy:87.26719785896171\n",
      "Epoch367  loss:0.39098574239760625  accuracy:87.33062122625368\n",
      "Epoch368  loss:0.3831689379643649  accuracy:87.62946848151111\n",
      "Epoch369  loss:0.38560855183750387  accuracy:87.32050312546734\n",
      "Epoch370  loss:0.3847123747691512  accuracy:87.35139416266333\n",
      "Epoch371  loss:0.38891999414190653  accuracy:87.26972205444189\n",
      "Epoch372  loss:0.3895820400677621  accuracy:87.26682990530108\n",
      "Epoch373  loss:0.3855565466918051  accuracy:87.12937643681516\n",
      "Epoch374  loss:0.3889685143716634  accuracy:87.06252803131551\n",
      "Epoch375  loss:0.38297258452512317  accuracy:87.20948651455376\n",
      "Epoch376  loss:0.37803513580001885  accuracy:87.57585099619806\n",
      "Epoch377  loss:0.3815604378003627  accuracy:87.30150736284465\n",
      "Epoch378  loss:0.38166972734034066  accuracy:87.57572331388849\n",
      "Epoch379  loss:0.3841913925949484  accuracy:87.15690264893541\n",
      "Epoch380  loss:0.3782085198909044  accuracy:87.82266425696699\n",
      "Epoch381  loss:0.3864744316088036  accuracy:87.39977073807678\n",
      "Epoch382  loss:0.38504322090884674  accuracy:87.50639800792686\n",
      "Epoch383  loss:0.3803352574817836  accuracy:87.49135741844809\n",
      "Epoch384  loss:0.3897436671424658  accuracy:87.63288971306092\n",
      "Epoch385  loss:0.3831796791404486  accuracy:87.79981269186895\n",
      "Epoch386  loss:0.38292241875315086  accuracy:87.42473472067469\n",
      "Epoch387  loss:0.3902024687267841  accuracy:87.19677210702855\n",
      "Epoch388  loss:0.38492697023320943  accuracy:87.3481633916987\n",
      "Epoch389  loss:0.38440483267186204  accuracy:87.48794684289061\n",
      "Epoch390  loss:0.39326989524997774  accuracy:87.237808714785\n",
      "Epoch391  loss:0.3824446336366236  accuracy:87.67081601999122\n",
      "Epoch392  loss:0.3863178820902249  accuracy:87.50357514866606\n",
      "Epoch393  loss:0.37912771571427584  accuracy:87.63337033563893\n",
      "Epoch394  loss:0.38395355043467144  accuracy:87.7340279767839\n",
      "Epoch395  loss:0.3848136086540762  accuracy:87.65863811282611\n",
      "Epoch396  loss:0.38432829251978545  accuracy:87.49865777265086\n",
      "Epoch397  loss:0.3814483068417757  accuracy:87.73821776360103\n",
      "Epoch398  loss:0.3833696586603764  accuracy:87.68083565937172\n",
      "Epoch399  loss:0.3864828418765682  accuracy:87.46159347794864\n",
      "Epoch400  loss:0.3843884741421789  accuracy:87.41849900419406\n",
      "Epoch401  loss:0.3814790274016559  accuracy:87.9465933291083\n",
      "Epoch402  loss:0.3812879804521799  accuracy:87.70625821701718\n",
      "Epoch403  loss:0.3884344543330371  accuracy:87.60913842041609\n",
      "Epoch404  loss:0.3772506206179969  accuracy:87.63450326679593\n",
      "Epoch405  loss:0.38060590068344025  accuracy:87.94472699755448\n",
      "Epoch406  loss:0.3760011030128226  accuracy:88.03680560165478\n",
      "Epoch407  loss:0.3777537174522876  accuracy:87.54913290049355\n",
      "Epoch408  loss:0.38364636222831905  accuracy:87.48766510949763\n",
      "Epoch409  loss:0.38357908874750146  accuracy:87.67370738741633\n",
      "Epoch410  loss:0.3899910145904868  accuracy:87.5173476771692\n",
      "Epoch411  loss:0.3747620536480099  accuracy:87.79878784521419\n",
      "Epoch412  loss:0.3908781994367018  accuracy:87.48542818113924\n",
      "Epoch413  loss:0.372615401307121  accuracy:87.72545635737016\n",
      "Epoch414  loss:0.37853687375318257  accuracy:87.96966579615933\n",
      "Epoch415  loss:0.3729423686279916  accuracy:87.59355616688148\n",
      "Epoch416  loss:0.378822671843227  accuracy:87.93506569946217\n",
      "Epoch417  loss:0.36518319186870934  accuracy:88.06363050399422\n",
      "Epoch418  loss:0.3700424405746161  accuracy:87.70816303039612\n",
      "Epoch419  loss:0.369603192131035  accuracy:88.04662315507424\n",
      "Epoch420  loss:0.37586500616744156  accuracy:87.87408464380198\n",
      "Epoch421  loss:0.3709044881979935  accuracy:87.94948090856425\n",
      "Epoch422  loss:0.3671429486013949  accuracy:87.8823368101777\n",
      "Epoch423  loss:0.37592809393536303  accuracy:87.72403573542118\n",
      "Epoch424  loss:0.3725600150064566  accuracy:88.09320163247388\n",
      "Epoch425  loss:0.3724377353326418  accuracy:87.61761785690167\n",
      "Epoch426  loss:0.37581004102248694  accuracy:87.98500735484087\n",
      "Epoch427  loss:0.3712052553077228  accuracy:88.05415686622086\n",
      "Epoch428  loss:0.35988331362605097  accuracy:88.13518839345568\n",
      "Epoch429  loss:0.38087362152291465  accuracy:87.71165976260801\n",
      "Epoch430  loss:0.3713327055214904  accuracy:87.8300039125561\n",
      "Epoch431  loss:0.3715429814998061  accuracy:88.12954419279217\n",
      "Epoch432  loss:0.3676691929227673  accuracy:88.01777383242451\n",
      "Epoch433  loss:0.36727512011420915  accuracy:88.12379550831588\n",
      "Epoch434  loss:0.3693889322457835  accuracy:87.93201331575398\n",
      "Epoch435  loss:0.3688249071594327  accuracy:88.0944136425578\n",
      "Epoch436  loss:0.3625497832195833  accuracy:88.17032842173992\n",
      "Epoch437  loss:0.3658644485520199  accuracy:88.21353448086646\n",
      "Epoch438  loss:0.3739791862317361  accuracy:87.85906028202434\n",
      "Epoch439  loss:0.37018147949129343  accuracy:88.0800728579867\n",
      "Epoch440  loss:0.37475577625446016  accuracy:88.12011568716953\n",
      "Epoch441  loss:0.38243384901434185  accuracy:87.5932799392497\n",
      "Epoch442  loss:0.37663408124353737  accuracy:88.01896857347958\n",
      "Epoch443  loss:0.3807999648619443  accuracy:87.92925691617485\n",
      "Epoch444  loss:0.37630527404835457  accuracy:88.07607290240647\n",
      "Epoch445  loss:0.370557102560997  accuracy:88.21167749864652\n",
      "Epoch446  loss:0.36604844587855034  accuracy:88.06037024673867\n",
      "Epoch447  loss:0.3753666027914732  accuracy:87.98844873493117\n",
      "Epoch448  loss:0.3697750852443278  accuracy:87.99033378940561\n",
      "Epoch449  loss:0.37643129732459785  accuracy:87.89429385277298\n",
      "Epoch450  loss:0.3735969616798684  accuracy:88.23550080407776\n",
      "Epoch451  loss:0.36360208846162995  accuracy:88.28898135625077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch452  loss:0.36721046189777556  accuracy:87.99181695011646\n",
      "Epoch453  loss:0.3596450709272176  accuracy:88.56196437104134\n",
      "Epoch454  loss:0.3626242758240551  accuracy:88.49494335377157\n",
      "Epoch455  loss:0.3704071240965277  accuracy:88.2948195956778\n",
      "Epoch456  loss:0.3801728508726228  accuracy:88.04298939037518\n",
      "Epoch457  loss:0.3817684408277273  accuracy:88.22044742520549\n",
      "Epoch458  loss:0.3702915153466165  accuracy:88.36607657034736\n",
      "Epoch459  loss:0.37393129001720804  accuracy:88.3817649889437\n",
      "Epoch460  loss:0.37735112451773595  accuracy:88.18935747839522\n",
      "Epoch461  loss:0.381933070498053  accuracy:88.2038705355537\n",
      "Epoch462  loss:0.3801479409245076  accuracy:88.11094410237429\n",
      "Epoch463  loss:0.3778138931142166  accuracy:88.22690570542278\n",
      "Epoch464  loss:0.38087606276385494  accuracy:88.2011138795585\n",
      "Epoch465  loss:0.3605174227617681  accuracy:88.23884269800257\n",
      "Epoch466  loss:0.3622586061246694  accuracy:88.04902332745765\n",
      "Epoch467  loss:0.35972432848066094  accuracy:88.53412746444964\n",
      "Epoch468  loss:0.3693688065279275  accuracy:88.44816747517005\n",
      "Epoch469  loss:0.3687848537229002  accuracy:88.24374656558453\n",
      "Epoch470  loss:0.36172887566499407  accuracy:88.54287150421474\n",
      "Epoch471  loss:0.36928238444961603  accuracy:88.19421654377282\n",
      "Epoch472  loss:0.35623451971914616  accuracy:88.63229140261828\n",
      "Epoch473  loss:0.3627876283571823  accuracy:88.25709154231622\n",
      "Epoch474  loss:0.36438462818041445  accuracy:88.41163301258716\n",
      "Epoch475  loss:0.3637630199082195  accuracy:88.61504196290305\n",
      "Epoch476  loss:0.3562381393858232  accuracy:88.49637720073613\n",
      "Epoch477  loss:0.35107903310563415  accuracy:88.77516043642254\n",
      "Epoch478  loss:0.35546830433886506  accuracy:88.47111258141034\n",
      "Epoch479  loss:0.3609717456391081  accuracy:88.62066405019716\n",
      "Epoch480  loss:0.3578061909414828  accuracy:88.23942673099239\n",
      "Epoch481  loss:0.35782458733301603  accuracy:88.46959335153173\n",
      "Epoch482  loss:0.3526181637775153  accuracy:88.33775410128096\n",
      "Epoch483  loss:0.35514759123325346  accuracy:88.55337277279529\n",
      "Epoch484  loss:0.35925016292021605  accuracy:88.28850621860335\n",
      "Epoch485  loss:0.37129364205757154  accuracy:88.7158344082203\n",
      "Epoch486  loss:0.36936196159804235  accuracy:88.28145812808421\n",
      "Epoch487  loss:0.3632080371491611  accuracy:88.6402249771375\n",
      "Epoch488  loss:0.3571968439500779  accuracy:88.43911413169982\n",
      "Epoch489  loss:0.36563729950576096  accuracy:88.39067268024885\n",
      "Epoch490  loss:0.36009179137181485  accuracy:88.6740758068081\n",
      "Epoch491  loss:0.36660292418673635  accuracy:88.2776822974642\n",
      "Epoch492  loss:0.35799131987150745  accuracy:88.27504434219458\n",
      "Epoch493  loss:0.3559379634447396  accuracy:88.46462703224155\n",
      "Epoch494  loss:0.3489689546287991  accuracy:88.67269723026446\n",
      "Epoch495  loss:0.36669009313918655  accuracy:88.63818602603197\n",
      "Epoch496  loss:0.35807022200897326  accuracy:88.62781256881445\n",
      "Epoch497  loss:0.3565823210054077  accuracy:88.48952812221542\n",
      "Epoch498  loss:0.3534667240455746  accuracy:88.70575505611593\n",
      "Epoch499  loss:0.35458565851440654  accuracy:88.76368715994994\n",
      "Epoch500  loss:0.3616877221385949  accuracy:88.2200485855422\n"
     ]
    }
   ],
   "source": [
    "server = Server(unlabeled_dataset)\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset,client_best_model)\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "acc_valid = []\n",
    "loss_valid = []\n",
    "\n",
    "early_stopping = Early_Stopping(args.partience)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(args.global_epochs):\n",
    "  if epoch in args.turn_of_cluster_num:\n",
    "    idx = args.turn_of_cluster_num.index(epoch)\n",
    "    args.cluster_num = args.cluster_list[idx]\n",
    "  sample_worker = server.sample_worker(workers)\n",
    "  server.collect_model(sample_worker)\n",
    "  server.clustering(sample_worker)\n",
    "  server.decide_other_model(sample_worker)\n",
    "  server.send_model(sample_worker)\n",
    "\n",
    "  acc_train_avg = 0.0\n",
    "  loss_train_avg = 0.0\n",
    "  acc_valid_avg = 0.0\n",
    "  loss_valid_avg = 0.0\n",
    "  for worker in sample_worker:\n",
    "    acc_train_tmp,loss_train_tmp = worker.local_train()\n",
    "    acc_valid_tmp,loss_valid_tmp = worker.validate()\n",
    "    acc_train_avg += acc_train_tmp/len(sample_worker)\n",
    "    loss_train_avg += loss_train_tmp/len(sample_worker)\n",
    "    acc_valid_avg += acc_valid_tmp/len(sample_worker)\n",
    "    loss_valid_avg += loss_valid_tmp/len(sample_worker)\n",
    "  if epoch in args.turn_of_replacement_model:\n",
    "    for worker in sample_worker:\n",
    "      worker.model_replacement()\n",
    "  server.aggregate_model(sample_worker)\n",
    "  server.return_model(sample_worker)\n",
    "  '''\n",
    "  server.model.to(args.device)\n",
    "  for worker in workers:\n",
    "    acc_valid_tmp,loss_valid_tmp = test(server.model,args.criterion,worker.valloader)\n",
    "    acc_valid_avg += acc_valid_tmp/len(workers)\n",
    "    loss_valid_avg += loss_valid_tmp/len(workers)\n",
    "  server.model.to('cpu')\n",
    "  '''\n",
    "  print('Epoch{}  loss:{}  accuracy:{}'.format(epoch+1,loss_valid_avg,acc_valid_avg))\n",
    "  acc_train.append(acc_train_avg)\n",
    "  loss_train.append(loss_train_avg)\n",
    "  acc_valid.append(acc_valid_avg)\n",
    "  loss_valid.append(loss_valid_avg)\n",
    "\n",
    "  if early_stopping.validate(loss_valid_avg):\n",
    "    print('Early Stop')\n",
    "    break\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 accuracy:82.94930875576037  loss:0.5126264691352844\n",
      "Worker2 accuracy:87.29508196721312  loss:0.36222898960113525\n",
      "Worker3 accuracy:81.52380952380952  loss:0.582252562046051\n",
      "Worker4 accuracy:90.67460317460318  loss:0.2799048125743866\n",
      "Worker5 accuracy:86.73170731707317  loss:0.25577129423618317\n",
      "Worker6 accuracy:78.83008356545962  loss:0.7132406234741211\n",
      "Worker7 accuracy:79.57894736842105  loss:0.6156986951828003\n",
      "Worker8 accuracy:90.2557856272838  loss:0.2851831912994385\n",
      "Worker9 accuracy:91.21019108280255  loss:0.2546981871128082\n",
      "Worker10 accuracy:90.07936507936508  loss:0.2573412358760834\n",
      "Worker11 accuracy:81.9047619047619  loss:0.5319325923919678\n",
      "Worker12 accuracy:86.61870503597122  loss:0.386181503534317\n",
      "Worker13 accuracy:92.65306122448979  loss:0.2250310331583023\n",
      "Worker14 accuracy:68.27586206896552  loss:1.0923672914505005\n",
      "Worker15 accuracy:88.01742919389979  loss:0.4418434500694275\n",
      "Worker16 accuracy:82.53275109170306  loss:0.5353342890739441\n",
      "Worker17 accuracy:86.32075471698113  loss:0.45333588123321533\n",
      "Worker18 accuracy:76.41681901279708  loss:0.6829959154129028\n",
      "Worker19 accuracy:79.7752808988764  loss:0.6228920817375183\n",
      "Worker20 accuracy:83.70927318295739  loss:0.5683722496032715\n",
      "Test  loss:0.48296161741018295  accuracy:84.26767908965975\n"
     ]
    }
   ],
   "source": [
    "acc_test = []\n",
    "loss_test = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.local_model = worker.local_model.to(args.device)\n",
    "  acc_tmp,loss_tmp = test(worker.local_model,args.criterion_ce,worker.testloader)\n",
    "  acc_test.append(acc_tmp)\n",
    "  loss_test.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "  worker.local_model = worker.local_model.to('cpu')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_test_avg = sum(acc_test)/len(acc_test)\n",
    "loss_test_avg = sum(loss_test)/len(loss_test)\n",
    "print('Test  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 Valid accuracy:84.399375975039  loss:0.6457388997077942\n",
      "Worker1 Test accuracy:83.41013824884793  loss:0.6504859924316406\n",
      "Worker2 Valid accuracy:91.70124481327801  loss:0.34288904070854187\n",
      "Worker2 Test accuracy:92.62295081967213  loss:0.2978486716747284\n",
      "Worker3 Valid accuracy:86.71875  loss:0.4644041061401367\n",
      "Worker3 Test accuracy:84.95238095238095  loss:0.5421623587608337\n",
      "Worker4 Valid accuracy:90.5050505050505  loss:0.30612674355506897\n",
      "Worker4 Test accuracy:91.46825396825396  loss:0.28084203600883484\n",
      "Worker5 Valid accuracy:85.95617529880478  loss:0.2896517738554394\n",
      "Worker5 Test accuracy:87.51219512195122  loss:0.2959766685962677\n",
      "Worker6 Valid accuracy:82.38636363636364  loss:0.5884044170379639\n",
      "Worker6 Test accuracy:83.56545961002786  loss:0.6148027777671814\n",
      "Worker7 Valid accuracy:89.29336188436831  loss:0.33835938572883606\n",
      "Worker7 Test accuracy:88.63157894736842  loss:0.38957130908966064\n",
      "Worker8 Valid accuracy:93.05210918114143  loss:0.2420998513698578\n",
      "Worker8 Test accuracy:91.59561510353228  loss:0.30534085631370544\n",
      "Worker9 Valid accuracy:93.24675324675324  loss:0.2724222242832184\n",
      "Worker9 Test accuracy:94.01273885350318  loss:0.24843084812164307\n",
      "Worker10 Valid accuracy:95.95141700404858  loss:0.11946211010217667\n",
      "Worker10 Test accuracy:93.25396825396825  loss:0.1834786981344223\n",
      "Worker11 Valid accuracy:88.02588996763754  loss:0.4206511974334717\n",
      "Worker11 Test accuracy:88.88888888888889  loss:0.4462003707885742\n",
      "Worker12 Valid accuracy:86.04992657856094  loss:0.5514287352561951\n",
      "Worker12 Test accuracy:86.90647482014388  loss:0.46797826886177063\n",
      "Worker13 Valid accuracy:96.06625258799171  loss:0.1479431539773941\n",
      "Worker13 Test accuracy:96.3265306122449  loss:0.18845641613006592\n",
      "Worker14 Valid accuracy:89.71631205673759  loss:0.3537566065788269\n",
      "Worker14 Test accuracy:82.75862068965517  loss:0.5392000675201416\n",
      "Worker15 Valid accuracy:89.3569844789357  loss:0.3938213586807251\n",
      "Worker15 Test accuracy:88.45315904139433  loss:0.49291619658470154\n",
      "Worker16 Valid accuracy:89.42731277533039  loss:0.4220368266105652\n",
      "Worker16 Test accuracy:82.53275109170306  loss:0.552216649055481\n",
      "Worker17 Valid accuracy:87.68115942028986  loss:0.4584307074546814\n",
      "Worker17 Test accuracy:86.32075471698113  loss:0.5047909021377563\n",
      "Worker18 Valid accuracy:80.85501858736059  loss:0.6705743670463562\n",
      "Worker18 Test accuracy:83.18098720292505  loss:0.6028708219528198\n",
      "Worker19 Valid accuracy:88.12260536398468  loss:0.43897899985313416\n",
      "Worker19 Test accuracy:81.27340823970037  loss:0.5751481652259827\n",
      "Worker20 Valid accuracy:89.40568475452196  loss:0.387368381023407\n",
      "Worker20 Test accuracy:89.97493734335839  loss:0.3520824909210205\n",
      "Validation(tune)  loss:0.3927274443201895  accuracy:88.89588740580992\n",
      "Test(tune)  loss:0.4265400283038616  accuracy:87.88208962632505\n"
     ]
    }
   ],
   "source": [
    "acc_tune_test = []\n",
    "loss_tune_test = []\n",
    "acc_tune_valid = []\n",
    "loss_tune_valid = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "    worker.local_model = worker.local_model.to(args.device)\n",
    "    _,_ = train(worker.local_model,args.criterion_ce,worker.trainloader,args.local_epochs)\n",
    "    acc_tmp,loss_tmp = test(worker.local_model,args.criterion_ce,worker.valloader)\n",
    "    acc_tune_valid.append(acc_tmp)\n",
    "    loss_tune_valid.append(loss_tmp)\n",
    "    print('Worker{} Valid accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    \n",
    "    acc_tmp,loss_tmp = test(worker.local_model,args.criterion_ce,worker.testloader)\n",
    "    acc_tune_test.append(acc_tmp)\n",
    "    loss_tune_test.append(loss_tmp)\n",
    "    print('Worker{} Test accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    worker.local_model = worker.local_model.to('cpu')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_valid_avg = sum(acc_tune_valid)/len(acc_tune_valid)\n",
    "loss_valid_avg = sum(loss_tune_valid)/len(loss_tune_valid)\n",
    "print('Validation(tune)  loss:{}  accuracy:{}'.format(loss_valid_avg,acc_valid_avg))\n",
    "acc_test_avg = sum(acc_tune_test)/len(acc_tune_test)\n",
    "loss_test_avg = sum(loss_tune_test)/len(loss_tune_test)\n",
    "print('Test(tune)  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
