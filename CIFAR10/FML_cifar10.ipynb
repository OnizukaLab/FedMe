{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    # random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 42\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O0TfzOhU-QlG"
   },
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 40\n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 500\n",
    "    self.local_epochs = 2\n",
    "    self.lr = None\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 500\n",
    "    self.worker_num = 20\n",
    "    self.sample_num = 20\n",
    "    self.unlabeleddata_size = 1000\n",
    "    self.device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion_ce = nn.CrossEntropyLoss()\n",
    "    self.criterion_kl = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    self.alpha_label = 0.5\n",
    "    self.alpha_size = 10\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned value\n",
    "lr = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = []\n",
    "lr_list.append(10**-3.0)\n",
    "lr_list.append(10**-2.5)\n",
    "lr_list.append(10**-2.0)\n",
    "lr_list.append(10**-1.5)\n",
    "lr_list.append(10**-1.0)\n",
    "lr_list.append(10**-0.5)\n",
    "lr_list.append(10**0.0)\n",
    "lr_list.append(10**0.5)\n",
    "\n",
    "args.lr = lr_list[lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.label[idx]\n",
    "        if self.transform:\n",
    "            out_data = self.transform(out_data)\n",
    "        return out_data, out_label\n",
    "    \n",
    "class DatasetFromSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.subset[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    \n",
    "class GlobalDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,federated_dataset,transform=None):\n",
    "    self.transform = transform\n",
    "    self.data = []\n",
    "    self.label = []\n",
    "    for dataset in federated_dataset:\n",
    "      for (data,label) in dataset:\n",
    "        self.data.append(data)\n",
    "        self.label.append(label)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    out_data = self.data[idx]\n",
    "    out_label = self.label[idx]\n",
    "    if self.transform:\n",
    "        out_data = self.transform(out_data)\n",
    "    return out_data, out_label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "class UnlabeledDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,transform=None):\n",
    "    self.transform = transform\n",
    "    self.data = []\n",
    "    self.target = None\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    out_data = self.data[idx]\n",
    "    out_label = 'unlabeled'\n",
    "    if self.transform:\n",
    "        out_data = self.transform(out_data)\n",
    "    return out_data, out_label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(Centralized=False,unlabeled_data=False):\n",
    "    \n",
    "    transform_train = transforms.Compose([transforms.ToPILImage(),\n",
    "                                    transforms.RandomCrop(32, padding=2),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.491372549, 0.482352941, 0.446666667), (0.247058824, 0.243529412, 0.261568627))])\n",
    "    transform_test = transforms.Compose([transforms.ToPILImage(),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.491372549, 0.482352941, 0.446666667), (0.247058824, 0.243529412, 0.261568627))])\n",
    "\n",
    "    # download train data\n",
    "    all_trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True)\n",
    "    #trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "    # download test data\n",
    "    all_testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True)\n",
    "    #testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "    \n",
    "    ## get unlabeled dataset\n",
    "    if unlabeled_data:\n",
    "        unlabeled_dataset = UnlabeledDataset(transform_test)\n",
    "        idx = sorted(random.sample(range(len(all_trainset)),args.unlabeleddata_size))\n",
    "        unlabeled_dataset.data = np.array([all_trainset.data[i]  for i in idx])\n",
    "        all_trainset.data = np.delete(all_trainset.data,idx,0)\n",
    "        all_trainset.targets = np.delete(all_trainset.targets,idx,0)\n",
    "    all_train_data = np.array(all_trainset.data)\n",
    "    all_train_label = np.array(all_trainset.targets)\n",
    "    all_test_data = np.array(all_testset.data)\n",
    "    all_test_label = np.array(all_testset.targets)\n",
    "    print('Train:{} Test:{}'.format(len(all_train_data),len(all_test_data)))\n",
    "\n",
    "\n",
    "    ## Data size heterogeneity\n",
    "    data_proportions = np.random.dirichlet(np.repeat(args.alpha_size, args.worker_num))\n",
    "    train_data_proportions = np.array([0 for _ in range(args.worker_num)])\n",
    "    test_data_proportions = np.array([0 for _ in range(args.worker_num)])\n",
    "    for i in range(len(data_proportions)):\n",
    "        if i==(len(data_proportions)-1):\n",
    "            train_data_proportions = train_data_proportions.astype('int64')\n",
    "            test_data_proportions = test_data_proportions.astype('int64')\n",
    "            train_data_proportions[-1] = len(all_train_data) - np.sum(train_data_proportions[:-1])\n",
    "            test_data_proportions[-1] = len(all_test_data) - np.sum(test_data_proportions[:-1])\n",
    "        else:\n",
    "            train_data_proportions[i] = (data_proportions[i] * len(all_train_data))\n",
    "            test_data_proportions[i] = (data_proportions[i] * len(all_test_data))\n",
    "    min_size = 0\n",
    "    K = 10\n",
    "\n",
    "    '''\n",
    "    label_list = np.arange(10)\n",
    "    np.random.shuffle(label_list)\n",
    "    '''\n",
    "    label_list = list(range(K))\n",
    "\n",
    "\n",
    "    ## Data distribution heterogeneity\n",
    "    while min_size<10:\n",
    "        idx_train_batch = [[] for _ in range(args.worker_num)]\n",
    "        idx_test_batch = [[] for _ in range(args.worker_num)]\n",
    "        for k in label_list:\n",
    "            proportions_train = np.random.dirichlet(np.repeat(args.alpha_label, args.worker_num))\n",
    "            proportions_test = copy.deepcopy(proportions_train)\n",
    "            idx_k_train = np.where(all_train_label == k)[0]\n",
    "            idx_k_test = np.where(all_test_label == k)[0]\n",
    "            np.random.shuffle(idx_k_train)\n",
    "            np.random.shuffle(idx_k_test)\n",
    "            ## Balance (train)\n",
    "            proportions_train = np.array([p*(len(idx_j)<train_data_proportions[i]) for i,(p,idx_j) in enumerate(zip(proportions_train,idx_train_batch))])\n",
    "            proportions_train = proportions_train/proportions_train.sum()\n",
    "            proportions_train = (np.cumsum(proportions_train)*len(idx_k_train)).astype(int)[:-1]\n",
    "            idx_train_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_train_batch,np.split(idx_k_train,proportions_train))]\n",
    "\n",
    "            ## Balance (test)\n",
    "            proportions_test = np.array([p*(len(idx_j)<test_data_proportions[i]) for i,(p,idx_j) in enumerate(zip(proportions_test,idx_test_batch))])\n",
    "            proportions_test = proportions_test/proportions_test.sum()\n",
    "            proportions_test = (np.cumsum(proportions_test)*len(idx_k_test)).astype(int)[:-1]\n",
    "            idx_test_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_test_batch,np.split(idx_k_test,proportions_test))]\n",
    "\n",
    "            min_size = min([len(idx_j) for idx_j in idx_train_batch])\n",
    "\n",
    "    federated_trainset = []\n",
    "    federated_testset = []\n",
    "    for i in range(args.worker_num):\n",
    "        ## create trainset\n",
    "        data = [all_train_data[idx] for idx in idx_train_batch[i]]\n",
    "        label = [all_train_label[idx] for idx in idx_train_batch[i]]\n",
    "        federated_trainset.append(LocalDataset())\n",
    "        federated_trainset[-1].data = data\n",
    "        federated_trainset[-1].label = label\n",
    "\n",
    "        ## create testset\n",
    "        data = [all_test_data[idx] for idx in idx_test_batch[i]]\n",
    "        label = [all_test_label[idx] for idx in idx_test_batch[i]]\n",
    "        federated_testset.append(LocalDataset())\n",
    "        federated_testset[-1].data = data\n",
    "        federated_testset[-1].label = label\n",
    "\n",
    "        \n",
    "    ## split trainset\n",
    "    federated_valset = [None]*args.worker_num\n",
    "    for i in range(args.worker_num):\n",
    "        n_samples = len(federated_trainset[i])\n",
    "        if n_samples==1:\n",
    "            train_subset = federated_trainset[i]\n",
    "            val_subset = copy.deepcopy(federated_trainset[i])\n",
    "        else:\n",
    "            train_size = int(len(federated_trainset[i]) * 0.8) \n",
    "            val_size = n_samples - train_size \n",
    "            train_subset,val_subset = torch.utils.data.random_split(federated_trainset[i], [train_size, val_size])\n",
    "\n",
    "        federated_trainset[i] = DatasetFromSubset(train_subset)\n",
    "        federated_valset[i] = DatasetFromSubset(val_subset)\n",
    "\n",
    "    ## show data distribution\n",
    "    H = 4\n",
    "    W = 5\n",
    "    fig, axs = plt.subplots(H, W, figsize=(20, 5))\n",
    "    x = np.arange(1,11)\n",
    "    for i, (trainset,valset,testset) in enumerate(zip(federated_trainset,federated_valset,federated_testset)):\n",
    "        bottom = [0]*10\n",
    "        count = [0]*10\n",
    "        for _,label in trainset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        for j in range(len(count)):\n",
    "            bottom[j]+=count[j]\n",
    "        count = [0]*10\n",
    "        for _,label in valset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        for j in range(len(count)):\n",
    "            bottom[j]+=count[j]\n",
    "        count = [0]*10\n",
    "        for _,label in testset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        #axs[int(i/W), i%W].title(\"worker{}\".format(i+1), fontsize=12, color = \"green\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ## get global dataset\n",
    "    if Centralized:\n",
    "        global_trainset = GlobalDataset(federated_trainset)\n",
    "        global_valset = GlobalDataset(federated_valset)\n",
    "        global_testset =  GlobalDataset(federated_testset)\n",
    "        \n",
    "        #show_cifer(global_trainset.data,global_testset.label, cifar10_labels)\n",
    "\n",
    "        global_trainset.transform = transform_train\n",
    "        global_valset.transform = transform_test\n",
    "        global_testset.transform = transform_test\n",
    "\n",
    "        global_trainloader = torch.utils.data.DataLoader(global_trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "        global_valloader = torch.utils.data.DataLoader(global_valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "        global_testloader = torch.utils.data.DataLoader(global_testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "\n",
    "    ## set transform\n",
    "    for i in range(args.worker_num):\n",
    "        federated_trainset[i].transform = transform_train\n",
    "        federated_valset[i].transform = transform_test\n",
    "        federated_testset[i].transform = transform_test\n",
    "    \n",
    "    if Centralized and unlabeled_data:\n",
    "        return federated_trainset,federated_valset,federated_testset,global_trainloader,global_valloader,global_testloader,unlabeled_dataset\n",
    "    if Centralized:\n",
    "        return federated_trainset,federated_valset,federated_testset,global_trainloader,global_valloader,global_testloader\n",
    "    elif unlabeled_data:\n",
    "        return federated_trainset,federated_valset,federated_testset,unlabeled_dataset\n",
    "    else:\n",
    "        return federated_trainset,federated_valset,federated_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train:49000 Test:10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "federated_trainset,federated_valset,federated_testset,unlabeled_dataset = get_dataset(unlabeled_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39191, 9809, 10000]\n"
     ]
    }
   ],
   "source": [
    "total = [0,0,0]\n",
    "for i in range(args.worker_num):\n",
    "    total[0]+=len(federated_trainset[i])\n",
    "    total[1]+=len(federated_valset[i])\n",
    "    total[2]+=len(federated_testset[i])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZU3vAAb9-6SD"
   },
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG model \n",
    "    '''\n",
    "    def __init__(self, features, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            #print(\"in_channels: {}, v: {}\".format(in_channels, v))\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
    "          512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGGConvBlocks(nn.Module):\n",
    "    '''\n",
    "    VGG containers that only contains the conv layers \n",
    "    '''\n",
    "    def __init__(self, features, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class VGGContainer(nn.Module):\n",
    "    '''\n",
    "    VGG model \n",
    "    '''\n",
    "    def __init__(self, features, input_dim, hidden_dims, num_classes=10):\n",
    "        super(VGGContainer, self).__init__()\n",
    "        self.features = features\n",
    "        # note: we hard coded here a bit by assuming we only have two hidden layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dims[1], num_classes),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def matched_vgg11(matched_shapes):\n",
    "    # [(67, 27), (67,), (132, 603), (132,), (260, 1188), (260,), (261, 2340), (261,), (516, 2349), (516,), (517, 4644), (517,), \n",
    "    # (516, 4653), (516,), (516, 4644), (516,), (516, 515), (515,), (515, 515), (515,), (515, 10), (10,)]\n",
    "    processed_matched_shape = [matched_shapes[0][0], \n",
    "                                'M', \n",
    "                                matched_shapes[2][0], \n",
    "                                'M', \n",
    "                                matched_shapes[4][0], \n",
    "                                matched_shapes[6][0], \n",
    "                                'M', \n",
    "                                matched_shapes[8][0], \n",
    "                                matched_shapes[10][0], \n",
    "                                'M', \n",
    "                                matched_shapes[12][0], \n",
    "                                matched_shapes[14][0], \n",
    "                                'M']\n",
    "    return VGGContainer(make_layers(processed_matched_shape), input_dim=matched_shapes[16][0], \n",
    "            hidden_dims=[matched_shapes[16][1], matched_shapes[18][1]], num_classes=10)\n",
    "\n",
    "\n",
    "def vgg11():\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
    "    return VGG(make_layers(cfg['A']))\n",
    "\n",
    "\n",
    "def vgg11_bn(num_classes=10):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['A'], batch_norm=True), num_classes=num_classes)\n",
    "\n",
    "\n",
    "def vgg13():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n",
    "    return VGG(make_layers(cfg['B']))\n",
    "\n",
    "\n",
    "def vgg13_bn():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg16():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n",
    "    return VGG(make_layers(cfg['D']))\n",
    "\n",
    "\n",
    "def vgg16_bn():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n",
    "    return VGG(make_layers(cfg['E']))\n",
    "\n",
    "\n",
    "def vgg19_bn():\n",
    "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['E'], batch_norm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self):\n",
    "    self.global_model = vgg13()\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(federated_trainset[i],federated_valset[i],federated_testset[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "\n",
    "  def send_model(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "\n",
    "    for worker in workers:\n",
    "      worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "      worker.global_model = copy.deepcopy(self.global_model)\n",
    "\n",
    "  def aggregate_model(self,workers):   \n",
    "    new_params = OrderedDict()\n",
    "    for i,worker in enumerate(workers):\n",
    "      worker_state = worker.global_model.state_dict()\n",
    "      for key in worker_state.keys():\n",
    "        if i==0:\n",
    "          new_params[key] = worker_state[key]*worker.aggregation_weight\n",
    "        else:\n",
    "          new_params[key] += worker_state[key]*worker.aggregation_weight\n",
    "      worker.global_model = worker.global_model.to('cpu')\n",
    "      del worker.global_model\n",
    "    self.global_model.load_state_dict(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,trainset,valset,testset):\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.local_model = vgg13()\n",
    "    self.global_model = None\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "    self.aggregation_weight = None\n",
    "\n",
    "  def local_train(self):\n",
    "    self.local_model = self.local_model.to(args.device)\n",
    "    self.global_model = self.global_model.to(args.device)\n",
    "    local_optimizer = optim.SGD(self.local_model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "    global_optimizer = optim.SGD(self.global_model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "    self.local_model.train()\n",
    "    self.global_model.train()\n",
    "    for epoch in range(args.local_epochs):\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in self.trainloader:\n",
    "        data,labels = Variable(data),Variable(labels)\n",
    "        data,labels = data.to(args.device),labels.to(args.device)\n",
    "        local_optimizer.zero_grad()\n",
    "        global_optimizer.zero_grad()\n",
    "        local_outputs = self.local_model(data)\n",
    "        global_outputs = self.global_model(data)\n",
    "        #train local_model\n",
    "        ce_loss = args.criterion_ce(local_outputs,labels)\n",
    "        kl_loss = args.criterion_kl(F.log_softmax(local_outputs, dim = 1),F.softmax(Variable(global_outputs), dim=1))\n",
    "        loss = ce_loss + kl_loss\n",
    "        running_loss += loss.item()\n",
    "        predicted = torch.argmax(local_outputs,dim=1)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "        count += len(labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), args.clip)\n",
    "        local_optimizer.step()\n",
    "\n",
    "        #train global_model\n",
    "        ce_loss = args.criterion_ce(global_outputs,labels)\n",
    "        kl_loss = args.criterion_kl(F.log_softmax(global_outputs, dim = 1),F.softmax(Variable(local_outputs), dim=1))\n",
    "        loss = ce_loss + kl_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.global_model.parameters(), args.clip)\n",
    "        global_optimizer.step()\n",
    "        \n",
    "    return 100.0*correct/count,running_loss/len(self.trainloader)\n",
    "\n",
    "\n",
    "  def validate(self):\n",
    "    acc,loss = test(self.local_model,args.criterion_ce,self.valloader)\n",
    "    self.local_model = self.local_model.to('cpu')\n",
    "    return acc,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7-GY66gROuEU"
   },
   "outputs": [],
   "source": [
    "def train(model,criterion,trainloader,epochs):\n",
    "  optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for (data,labels) in trainloader:\n",
    "      data,labels = Variable(data),Variable(labels)\n",
    "      data,labels = data.to(args.device),labels.to(args.device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(data)\n",
    "      loss = criterion(outputs,labels)\n",
    "      running_loss += loss.item()\n",
    "      predicted = torch.argmax(outputs,dim=1)\n",
    "      correct += (predicted==labels).sum().item()\n",
    "      count += len(labels)\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "      optimizer.step()\n",
    "\n",
    "  return 100.0*correct/count,running_loss/len(trainloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  count = 0\n",
    "  for (data,labels) in testloader:\n",
    "    data,labels = data.to(args.device),labels.to(args.device)\n",
    "    outputs = model(data)\n",
    "    running_loss += criterion(outputs,labels).item()\n",
    "    predicted = torch.argmax(outputs,dim=1)\n",
    "    correct += (predicted==labels).sum().item()\n",
    "    count += len(labels)\n",
    "\n",
    "  accuracy = 100.0*correct/count\n",
    "  loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "  return accuracy,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WMO7_WSLHeGl"
   },
   "outputs": [],
   "source": [
    "class Early_Stopping():\n",
    "  def __init__(self,partience):\n",
    "    self.step = 0\n",
    "    self.loss = float('inf')\n",
    "    self.partience = partience\n",
    "\n",
    "  def validate(self,loss):\n",
    "    if self.loss<loss:\n",
    "      self.step += 1\n",
    "      if self.step>self.partience:\n",
    "        return True\n",
    "    else:\n",
    "      self.step = 0\n",
    "      self.loss = loss\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "-noG_98IR-nZ",
    "outputId": "78a6ebe2-854a-4f83-dc45-5c4ac35b69e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1  loss:1.663982126116753  accuracy:43.71685200903926\n",
      "Epoch2  loss:1.4982869386672975  accuracy:43.56939597543205\n",
      "Epoch3  loss:1.474682769179344  accuracy:44.5067976409522\n",
      "Epoch4  loss:1.463369381427765  accuracy:43.98918840214481\n",
      "Epoch5  loss:1.4534801065921785  accuracy:45.08686388517641\n",
      "Epoch6  loss:1.446540766954422  accuracy:44.51164369180111\n",
      "Epoch7  loss:1.434243494272232  accuracy:45.26356433297916\n",
      "Epoch8  loss:1.4249505430459979  accuracy:45.972269987647685\n",
      "Epoch9  loss:1.410945948958397  accuracy:46.459381670690256\n",
      "Epoch10  loss:1.3907708138227461  accuracy:48.61095370890093\n",
      "Epoch11  loss:1.3640322029590608  accuracy:49.20251333031607\n",
      "Epoch12  loss:1.345792070031166  accuracy:50.354387049467036\n",
      "Epoch13  loss:1.3243111461400987  accuracy:51.66106743948682\n",
      "Epoch14  loss:1.2922062665224074  accuracy:53.076986897531384\n",
      "Epoch15  loss:1.2725894957780837  accuracy:53.975705415359926\n",
      "Epoch16  loss:1.2661075443029406  accuracy:54.414367448550045\n",
      "Epoch17  loss:1.2484104633331299  accuracy:54.75615830411613\n",
      "Epoch18  loss:1.242982316017151  accuracy:54.422015902073916\n",
      "Epoch19  loss:1.2191310346126556  accuracy:55.617856009376126\n",
      "Epoch20  loss:1.2016071319580077  accuracy:55.88977347202952\n",
      "Epoch21  loss:1.1897828221321105  accuracy:56.13263946792309\n",
      "Epoch22  loss:1.1833957195281983  accuracy:56.74333924441298\n",
      "Epoch23  loss:1.1735097378492356  accuracy:57.54790825916148\n",
      "Epoch24  loss:1.1562662348151207  accuracy:58.01281983703962\n",
      "Epoch25  loss:1.1559437334537506  accuracy:58.36913850180913\n",
      "Epoch26  loss:1.1406506672501564  accuracy:58.69968864679338\n",
      "Epoch27  loss:1.1317932546138765  accuracy:59.06003082636824\n",
      "Epoch28  loss:1.12245092689991  accuracy:59.553844827923896\n",
      "Epoch29  loss:1.1131119057536125  accuracy:59.80857912925191\n",
      "Epoch30  loss:1.1128379315137862  accuracy:59.341408543479645\n",
      "Epoch31  loss:1.104915802180767  accuracy:59.94815362387254\n",
      "Epoch32  loss:1.0884408727288246  accuracy:60.11453852744571\n",
      "Epoch33  loss:1.0898221462965014  accuracy:60.53872988003105\n",
      "Epoch34  loss:1.0775113463401795  accuracy:61.20930180191094\n",
      "Epoch35  loss:1.062422299385071  accuracy:61.965724830712965\n",
      "Epoch36  loss:1.0513700455427168  accuracy:61.93033329040824\n",
      "Epoch37  loss:1.0466756016016008  accuracy:62.53373774712326\n",
      "Epoch38  loss:1.0447937801480294  accuracy:62.158319161821524\n",
      "Epoch39  loss:1.0373689979314804  accuracy:62.61283373469735\n",
      "Epoch40  loss:1.0394205108284948  accuracy:62.74526763320691\n",
      "Epoch41  loss:1.0193890765309335  accuracy:64.01144989057865\n",
      "Epoch42  loss:1.0143153101205824  accuracy:63.84713714656477\n",
      "Epoch43  loss:1.011088155210018  accuracy:64.00609675143747\n",
      "Epoch44  loss:0.9950639575719831  accuracy:64.60658883536381\n",
      "Epoch45  loss:0.9896608963608741  accuracy:64.9788498071364\n",
      "Epoch46  loss:0.9738904699683187  accuracy:64.91792972400056\n",
      "Epoch47  loss:0.9759059906005859  accuracy:65.3440451388506\n",
      "Epoch48  loss:0.9716954573988914  accuracy:65.53698657563136\n",
      "Epoch49  loss:0.9650651782751083  accuracy:66.02087862610249\n",
      "Epoch50  loss:0.9655257612466812  accuracy:65.84728883018698\n",
      "Epoch51  loss:0.9465043663978576  accuracy:66.58960756846473\n",
      "Epoch52  loss:0.9387902215123176  accuracy:66.50754930465362\n",
      "Epoch53  loss:0.9452332705259322  accuracy:67.29580766837465\n",
      "Epoch54  loss:0.9278064370155333  accuracy:67.17787436854158\n",
      "Epoch55  loss:0.9285141155123708  accuracy:67.40408924505323\n",
      "Epoch56  loss:0.9213875994086267  accuracy:67.65183995899781\n",
      "Epoch57  loss:0.9076049104332925  accuracy:68.31603892574972\n",
      "Epoch58  loss:0.9090222403407097  accuracy:68.05182606159342\n",
      "Epoch59  loss:0.8975418969988822  accuracy:68.64521702145214\n",
      "Epoch60  loss:0.8927398145198823  accuracy:68.6110494701444\n",
      "Epoch61  loss:0.8841205283999443  accuracy:68.87293510937623\n",
      "Epoch62  loss:0.8931154772639275  accuracy:68.51134490806565\n",
      "Epoch63  loss:0.8838290326297282  accuracy:68.58506920003187\n",
      "Epoch64  loss:0.8784327521920204  accuracy:69.20640045291796\n",
      "Epoch65  loss:0.8756150990724564  accuracy:69.3275509184277\n",
      "Epoch66  loss:0.8692922681570052  accuracy:69.40859590168992\n",
      "Epoch67  loss:0.876188188791275  accuracy:68.91553295184487\n",
      "Epoch68  loss:0.8570673063397407  accuracy:70.14500311561268\n",
      "Epoch69  loss:0.8544181682169436  accuracy:69.72412252358376\n",
      "Epoch70  loss:0.8581528365612029  accuracy:69.9523941264559\n",
      "Epoch71  loss:0.8627561166882516  accuracy:69.7147574303876\n",
      "Epoch72  loss:0.8358859628438949  accuracy:70.63516759150892\n",
      "Epoch73  loss:0.8459981031715871  accuracy:70.38311842650538\n",
      "Epoch74  loss:0.839889554679394  accuracy:71.1027173337548\n",
      "Epoch75  loss:0.8341985166072846  accuracy:71.09201143969184\n",
      "Epoch76  loss:0.8340145714581012  accuracy:71.06943066671451\n",
      "Epoch77  loss:0.826831191778183  accuracy:71.48839982542592\n",
      "Epoch78  loss:0.8289581589400766  accuracy:70.88892053874736\n",
      "Epoch79  loss:0.8237249031662942  accuracy:71.38710021668318\n",
      "Epoch80  loss:0.813565956056118  accuracy:71.76490855713186\n",
      "Epoch81  loss:0.8160709716379643  accuracy:71.93303379783575\n",
      "Epoch82  loss:0.8107235610485076  accuracy:71.6452239868413\n",
      "Epoch83  loss:0.8041832871735095  accuracy:72.49904764675505\n",
      "Epoch84  loss:0.8059191197156907  accuracy:71.81810134187867\n",
      "Epoch85  loss:0.8056101128458977  accuracy:71.90752390068613\n",
      "Epoch86  loss:0.7939497992396355  accuracy:72.38343949389855\n",
      "Epoch87  loss:0.7803521230816841  accuracy:73.00300870247185\n",
      "Epoch88  loss:0.798559008538723  accuracy:72.34649716032166\n",
      "Epoch89  loss:0.7919706553220748  accuracy:72.74483539682687\n",
      "Epoch90  loss:0.7763786934316157  accuracy:73.05253769019833\n",
      "Epoch91  loss:0.7731387607753277  accuracy:73.34323832440617\n",
      "Epoch92  loss:0.7705991461873054  accuracy:73.30011269146655\n",
      "Epoch93  loss:0.7762181550264359  accuracy:73.78169252014919\n",
      "Epoch94  loss:0.761898571997881  accuracy:73.73524623264247\n",
      "Epoch95  loss:0.7547370895743368  accuracy:74.00971620922029\n",
      "Epoch96  loss:0.7669874578714369  accuracy:73.72827734456183\n",
      "Epoch97  loss:0.7487638995051384  accuracy:74.4186091329593\n",
      "Epoch98  loss:0.7763693541288376  accuracy:73.33965631027472\n",
      "Epoch99  loss:0.7522145867347717  accuracy:74.11231719577205\n",
      "Epoch100  loss:0.7494810283184051  accuracy:74.15587219716635\n",
      "Epoch101  loss:0.7493094116449356  accuracy:74.24771435594027\n",
      "Epoch102  loss:0.7549085915088652  accuracy:74.22898614299052\n",
      "Epoch103  loss:0.7398136712610721  accuracy:74.2014752590063\n",
      "Epoch104  loss:0.7457058086991308  accuracy:74.17355259118607\n",
      "Epoch105  loss:0.7399859122931958  accuracy:74.71020444495909\n",
      "Epoch106  loss:0.7411352254450322  accuracy:75.36559160939248\n",
      "Epoch107  loss:0.7411774083971978  accuracy:74.40654162989806\n",
      "Epoch108  loss:0.7369837284088133  accuracy:75.08869211921622\n",
      "Epoch109  loss:0.7346662320196629  accuracy:75.07128094526328\n",
      "Epoch110  loss:0.7329681195318699  accuracy:75.21642841876748\n",
      "Epoch111  loss:0.7260141067206859  accuracy:75.27791097829306\n",
      "Epoch112  loss:0.7188221618533133  accuracy:75.64972073605254\n",
      "Epoch113  loss:0.7236629370599985  accuracy:75.35211758216747\n",
      "Epoch114  loss:0.7257471784949303  accuracy:75.19249236173258\n",
      "Epoch115  loss:0.711435128003359  accuracy:76.15748234824561\n",
      "Epoch116  loss:0.7199298694729807  accuracy:75.4773432614159\n",
      "Epoch117  loss:0.7170314691960812  accuracy:75.71199716947568\n",
      "Epoch118  loss:0.7179627798497678  accuracy:75.72029767862281\n",
      "Epoch119  loss:0.7052154410630466  accuracy:76.02840958967639\n",
      "Epoch120  loss:0.7031949840486048  accuracy:75.95419822988524\n",
      "Epoch121  loss:0.7069861520081758  accuracy:75.82881726005492\n",
      "Epoch122  loss:0.7033881474286319  accuracy:76.15760063209872\n",
      "Epoch123  loss:0.6966398563235997  accuracy:76.36540355114211\n",
      "Epoch124  loss:0.6959350690245629  accuracy:76.58022865761325\n",
      "Epoch125  loss:0.697900063917041  accuracy:76.36868770523019\n",
      "Epoch126  loss:0.7126776918768882  accuracy:75.79155637858933\n",
      "Epoch127  loss:0.7142278999090195  accuracy:76.09774912666846\n",
      "Epoch128  loss:0.6935972597450019  accuracy:76.62690102494946\n",
      "Epoch129  loss:0.6966250330209733  accuracy:76.5468883837305\n",
      "Epoch130  loss:0.6933907322585582  accuracy:76.57280885499034\n",
      "Epoch131  loss:0.6841298967599869  accuracy:77.01676569340725\n",
      "Epoch132  loss:0.6797316521406175  accuracy:77.30088553020614\n",
      "Epoch133  loss:0.6795823253691198  accuracy:76.90724550147831\n",
      "Epoch134  loss:0.6825150724500418  accuracy:76.82890103970308\n",
      "Epoch135  loss:0.6858411490917207  accuracy:76.85774514791599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch136  loss:0.6995026104152203  accuracy:76.45261898658674\n",
      "Epoch137  loss:0.6803780611604451  accuracy:77.10611633157826\n",
      "Epoch138  loss:0.6867974705994128  accuracy:76.7675607500402\n",
      "Epoch139  loss:0.6802399717271328  accuracy:77.25273027695472\n",
      "Epoch140  loss:0.6825607255101204  accuracy:77.06867317335826\n",
      "Epoch141  loss:0.6755380153656005  accuracy:77.16969668463794\n",
      "Epoch142  loss:0.6779172644019127  accuracy:77.21270491915462\n",
      "Epoch143  loss:0.6757138526067139  accuracy:77.1155323870205\n",
      "Epoch144  loss:0.667306899279356  accuracy:77.7483710035051\n",
      "Epoch145  loss:0.6699051443487406  accuracy:77.30823268467864\n",
      "Epoch146  loss:0.6741863116621971  accuracy:77.36750239560214\n",
      "Epoch147  loss:0.6657050311565398  accuracy:77.57076891548009\n",
      "Epoch148  loss:0.6671045184135436  accuracy:77.74593341891038\n",
      "Epoch149  loss:0.6634644269943236  accuracy:77.69886105592079\n",
      "Epoch150  loss:0.6770287394523622  accuracy:77.12691438807857\n",
      "Epoch151  loss:0.6662525467574596  accuracy:77.69629258396513\n",
      "Epoch152  loss:0.6615287475287913  accuracy:77.96026469551511\n",
      "Epoch153  loss:0.6651867870241404  accuracy:78.04988991935075\n",
      "Epoch154  loss:0.6712522741407155  accuracy:77.31885392429845\n",
      "Epoch155  loss:0.6618239749222993  accuracy:77.65781445744597\n",
      "Epoch156  loss:0.6581155259162187  accuracy:77.78461328660576\n",
      "Epoch157  loss:0.6598712820559739  accuracy:77.76009899212498\n",
      "Epoch158  loss:0.6600871384143829  accuracy:77.84691721452886\n",
      "Epoch159  loss:0.6652009315788746  accuracy:77.33007938088892\n",
      "Epoch160  loss:0.6539779208600521  accuracy:78.26507478426682\n",
      "Epoch161  loss:0.6627767857164145  accuracy:78.00002725059755\n",
      "Epoch162  loss:0.6625955011695623  accuracy:77.81831324940944\n",
      "Epoch163  loss:0.6608620069921016  accuracy:78.07379241845956\n",
      "Epoch164  loss:0.6579519610852003  accuracy:78.0174550846146\n",
      "Epoch165  loss:0.663234337605536  accuracy:77.82099209580561\n",
      "Epoch166  loss:0.6506985597312449  accuracy:78.40658953348714\n",
      "Epoch167  loss:0.6566355003044009  accuracy:78.07959713293141\n",
      "Epoch168  loss:0.6546116560697556  accuracy:78.45002703894268\n",
      "Epoch169  loss:0.6555034164339304  accuracy:78.09117512946517\n",
      "Epoch170  loss:0.662241905555129  accuracy:77.76590917459\n",
      "Epoch171  loss:0.6600915223360062  accuracy:78.53611422193157\n",
      "Epoch172  loss:0.6542218875139952  accuracy:78.29727422529815\n",
      "Epoch173  loss:0.6446032017469406  accuracy:78.79706204272884\n",
      "Epoch174  loss:0.6489457778632641  accuracy:78.52263959852894\n",
      "Epoch175  loss:0.6525866605341435  accuracy:78.58061644823505\n",
      "Epoch176  loss:0.6628191575407981  accuracy:78.13996278326383\n",
      "Epoch177  loss:0.6455209389328956  accuracy:78.95403538777498\n",
      "Epoch178  loss:0.6510673202574252  accuracy:78.35911208935885\n",
      "Epoch179  loss:0.6483034580945969  accuracy:78.28592277016726\n",
      "Epoch180  loss:0.6444219455122948  accuracy:78.53347774214498\n",
      "Epoch181  loss:0.6462212104350329  accuracy:78.60053611106568\n",
      "Epoch182  loss:0.6405488155782224  accuracy:78.45585230209538\n",
      "Epoch183  loss:0.6486253291368484  accuracy:78.43867629958066\n",
      "Epoch184  loss:0.6535484202206135  accuracy:78.65051833170324\n",
      "Epoch185  loss:0.6423395862802862  accuracy:78.66541791903312\n",
      "Epoch186  loss:0.64848883934319  accuracy:78.5257987941848\n",
      "Epoch187  loss:0.650268416106701  accuracy:78.4872474529896\n",
      "Epoch188  loss:0.6482392159290612  accuracy:78.57239313219384\n",
      "Epoch189  loss:0.6525156339630485  accuracy:78.24212996960514\n",
      "Epoch190  loss:0.6481825347989798  accuracy:78.98777009433196\n",
      "Epoch191  loss:0.6441879373043776  accuracy:78.97343684777275\n",
      "Epoch192  loss:0.6588838631287217  accuracy:78.10497317397304\n",
      "Epoch193  loss:0.6434681788086891  accuracy:78.68804369665723\n",
      "Epoch194  loss:0.641838390380144  accuracy:78.78215517201109\n",
      "Epoch195  loss:0.6421197660267352  accuracy:78.57717874658377\n",
      "Epoch196  loss:0.6573115777224303  accuracy:78.47154342290285\n",
      "Epoch197  loss:0.6477377706207333  accuracy:78.71165603498669\n",
      "Epoch198  loss:0.6414835269562901  accuracy:78.8273568065379\n",
      "Epoch199  loss:0.6661751616746188  accuracy:78.07823452376361\n",
      "Epoch200  loss:0.6465690845623612  accuracy:78.75331642417582\n",
      "Epoch201  loss:0.6385964997112751  accuracy:79.14114158495276\n",
      "Epoch202  loss:0.6384145541116595  accuracy:79.21716539122065\n",
      "Epoch203  loss:0.6456242416054011  accuracy:78.7794633602862\n",
      "Epoch204  loss:0.6369166119024157  accuracy:78.91302948309803\n",
      "Epoch205  loss:0.643014638312161  accuracy:78.85085530365397\n",
      "Epoch206  loss:0.6444907746277749  accuracy:78.63025259611312\n",
      "Epoch207  loss:0.6397722087800504  accuracy:79.15616821920273\n",
      "Epoch208  loss:0.6411739839240909  accuracy:78.99029511012446\n",
      "Epoch209  loss:0.6402185076847672  accuracy:79.12709190768963\n",
      "Epoch210  loss:0.6487753597088157  accuracy:78.75705468739854\n",
      "Epoch211  loss:0.6440554243512452  accuracy:78.81398207317324\n",
      "Epoch212  loss:0.6380697732791304  accuracy:79.29990673970283\n",
      "Epoch213  loss:0.642877184599638  accuracy:79.2609534669669\n",
      "Epoch214  loss:0.6428889392875135  accuracy:78.98766397286195\n",
      "Epoch215  loss:0.6391621999442578  accuracy:79.16571579843114\n",
      "Epoch216  loss:0.6456282600760459  accuracy:79.05720263252687\n",
      "Epoch217  loss:0.6401343984529376  accuracy:78.83229968243725\n",
      "Epoch218  loss:0.6490752000827342  accuracy:78.77897730019973\n",
      "Epoch219  loss:0.6388594727031888  accuracy:79.33895424927354\n",
      "Epoch220  loss:0.6374705269932749  accuracy:79.128821756098\n",
      "Epoch221  loss:0.6439936622977256  accuracy:78.89925257692317\n",
      "Epoch222  loss:0.6426886761561035  accuracy:78.74000032799199\n",
      "Epoch223  loss:0.6467046281322837  accuracy:78.94335626802717\n",
      "Epoch224  loss:0.6474914074875416  accuracy:78.87467443789268\n",
      "Epoch225  loss:0.6365481253713369  accuracy:79.45834065341147\n",
      "Epoch226  loss:0.6368335509207099  accuracy:79.0954892568653\n",
      "Epoch227  loss:0.6459661176428197  accuracy:79.04642843272455\n",
      "Epoch228  loss:0.6405613455921412  accuracy:79.14577480143679\n",
      "Epoch229  loss:0.6508566636592149  accuracy:79.08939969926882\n",
      "Epoch230  loss:0.6409427461214363  accuracy:79.3241129521754\n",
      "Epoch231  loss:0.6458406833931805  accuracy:79.14434633737335\n",
      "Epoch232  loss:0.6358529034536332  accuracy:79.4629602258134\n",
      "Epoch233  loss:0.6385943073779345  accuracy:79.1921641538518\n",
      "Epoch234  loss:0.6445860072039068  accuracy:78.8790696585909\n",
      "Epoch235  loss:0.6469947730191052  accuracy:79.19130918952702\n",
      "Epoch236  loss:0.6394509464502335  accuracy:79.31586950471709\n",
      "Epoch237  loss:0.6482029676437379  accuracy:79.04196878850485\n",
      "Epoch238  loss:0.6387997884303331  accuracy:79.18948846565524\n",
      "Epoch239  loss:0.6494936737697572  accuracy:78.8842048751614\n",
      "Epoch240  loss:0.6487916719168424  accuracy:79.18993288497431\n",
      "Epoch241  loss:0.643872600980103  accuracy:79.36099267181542\n",
      "Epoch242  loss:0.6510198135860265  accuracy:79.0361579684982\n",
      "Epoch243  loss:0.6429454751778395  accuracy:79.0088563085678\n",
      "Epoch244  loss:0.646561728231609  accuracy:79.0393120273639\n",
      "Epoch245  loss:0.6436810322105885  accuracy:79.37296905551734\n",
      "Epoch246  loss:0.645162273105234  accuracy:79.16271988230449\n",
      "Epoch247  loss:0.6450586622115223  accuracy:79.12733267010269\n",
      "Epoch248  loss:0.6438274912536144  accuracy:79.43850959563115\n",
      "Epoch249  loss:0.642478116415441  accuracy:79.01484904908303\n",
      "Epoch250  loss:0.6512143487576396  accuracy:79.06775660548494\n",
      "Epoch251  loss:0.6432622202206403  accuracy:79.56681538439456\n",
      "Epoch252  loss:0.6320222296053544  accuracy:79.38710343771933\n",
      "Epoch253  loss:0.6478891620878131  accuracy:78.97967673787954\n",
      "Epoch254  loss:0.6521034932695329  accuracy:78.66351723375155\n",
      "Epoch255  loss:0.6546345365233719  accuracy:78.94966662652378\n",
      "Epoch256  loss:0.6401426966302097  accuracy:79.57318777838005\n",
      "Epoch257  loss:0.6460806934628637  accuracy:79.06194470892268\n",
      "Epoch258  loss:0.6443560171872378  accuracy:79.27764184868609\n",
      "Epoch259  loss:0.6431670867372304  accuracy:79.38219916951437\n",
      "Epoch260  loss:0.6547758312895893  accuracy:79.58854053995442\n",
      "Epoch261  loss:0.6477188874967396  accuracy:79.50394873542575\n",
      "Epoch262  loss:0.6463553043082357  accuracy:79.79654955049894\n",
      "Epoch263  loss:0.6513339187949895  accuracy:79.23845711187604\n",
      "Epoch264  loss:0.6548607606906444  accuracy:79.16233822768689\n",
      "Epoch265  loss:0.652459339890629  accuracy:79.04386725162634\n",
      "Epoch266  loss:0.6467371391132474  accuracy:79.07140672098701\n",
      "Epoch267  loss:0.6518334534019231  accuracy:78.87972287368058\n",
      "Epoch268  loss:0.651164095569402  accuracy:79.19078250390965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch269  loss:0.6529439810197799  accuracy:79.44742718535917\n",
      "Epoch270  loss:0.6464462715201079  accuracy:79.46517886504544\n",
      "Epoch271  loss:0.6514387242030352  accuracy:79.28884766614286\n",
      "Epoch272  loss:0.6464096246287228  accuracy:79.30605825423179\n",
      "Epoch273  loss:0.6571569133549928  accuracy:78.98886391420623\n",
      "Epoch274  loss:0.6439578797202556  accuracy:79.95137626993218\n",
      "Epoch275  loss:0.6512431782204658  accuracy:79.18931246451655\n",
      "Epoch276  loss:0.65772994607687  accuracy:79.23738174598019\n",
      "Epoch277  loss:0.6561937740072608  accuracy:79.27716997183869\n",
      "Epoch278  loss:0.6567149654962123  accuracy:78.87880596123019\n",
      "Epoch279  loss:0.6529754977673291  accuracy:79.90315859255556\n",
      "Epoch280  loss:0.6553349625086412  accuracy:79.30753958932955\n",
      "Epoch281  loss:0.6593528794590384  accuracy:79.17194618181881\n",
      "Epoch282  loss:0.6569015063345433  accuracy:79.22931014360798\n",
      "Epoch283  loss:0.6523304757196455  accuracy:79.52689425357498\n",
      "Epoch284  loss:0.6560969168320298  accuracy:79.40473253880573\n",
      "Epoch285  loss:0.6574683455750345  accuracy:79.31474968506824\n",
      "Epoch286  loss:0.6593147437553852  accuracy:79.33140756214684\n",
      "Epoch287  loss:0.6650613593868913  accuracy:79.09384759567946\n",
      "Epoch288  loss:0.6507999598979951  accuracy:79.73304326100656\n",
      "Epoch289  loss:0.6565086210612205  accuracy:79.13319941100686\n",
      "Epoch290  loss:0.6482890509068966  accuracy:79.39046491032997\n",
      "Epoch291  loss:0.653626258391887  accuracy:79.23881671550097\n",
      "Epoch292  loss:0.6640213786275126  accuracy:79.32838587492277\n",
      "Epoch293  loss:0.6534691828768702  accuracy:79.6647748659913\n",
      "Epoch294  loss:0.6597176702227443  accuracy:79.55248397923367\n",
      "Epoch295  loss:0.6676447444129735  accuracy:78.84659367362798\n",
      "Epoch296  loss:0.6506405648309738  accuracy:79.68331830382515\n",
      "Epoch297  loss:0.6586247434373944  accuracy:79.16788627531416\n",
      "Epoch298  loss:0.6464750366751104  accuracy:79.50750106182886\n",
      "Epoch299  loss:0.6629193598986605  accuracy:79.31486449947444\n",
      "Epoch300  loss:0.6523004658054561  accuracy:79.71663468434073\n",
      "Epoch301  loss:0.6652988743036986  accuracy:79.49836015787898\n",
      "Epoch302  loss:0.6528746791998856  accuracy:79.57962476724539\n",
      "Epoch303  loss:0.6682299135252834  accuracy:79.42114477073694\n",
      "Epoch304  loss:0.6543278904864563  accuracy:79.72285092761236\n",
      "Epoch305  loss:0.6545601521153003  accuracy:79.61343288795263\n",
      "Epoch306  loss:0.6613968615652994  accuracy:79.82814711470985\n",
      "Epoch307  loss:0.6595062382984906  accuracy:79.82267771116872\n",
      "Epoch308  loss:0.6589726630831139  accuracy:79.42196552169845\n",
      "Epoch309  loss:0.6629156604409218  accuracy:79.30354562331921\n",
      "Epoch310  loss:0.6766683654394  accuracy:79.28363476355288\n",
      "Epoch311  loss:0.6691658543422818  accuracy:79.45527345566941\n",
      "Epoch312  loss:0.6628671605722047  accuracy:79.53227535132771\n",
      "Epoch313  loss:0.671328470716253  accuracy:79.20016772209637\n",
      "Epoch314  loss:0.6687619517557324  accuracy:79.33061325468823\n",
      "Epoch315  loss:0.6716429027961568  accuracy:79.21910074723505\n",
      "Epoch316  loss:0.6730221438221633  accuracy:79.24716437977956\n",
      "Epoch317  loss:0.665840688208118  accuracy:79.6416003016992\n",
      "Epoch318  loss:0.6724389355629684  accuracy:79.58580908710621\n",
      "Epoch319  loss:0.6676536397077143  accuracy:79.46185621538726\n",
      "Epoch320  loss:0.6727605108637364  accuracy:79.21815907497599\n",
      "Epoch321  loss:0.6673699664883317  accuracy:79.71495207962087\n",
      "Epoch322  loss:0.6703245496843013  accuracy:79.38270146449545\n",
      "Epoch323  loss:0.6702749578515067  accuracy:79.48595267669327\n",
      "Epoch324  loss:0.6737911755219104  accuracy:78.9702474179186\n",
      "Epoch325  loss:0.6752838991116733  accuracy:79.47500070846263\n",
      "Epoch326  loss:0.6797775583108888  accuracy:79.32571905403083\n",
      "Epoch327  loss:0.6655512686818839  accuracy:79.23490393985875\n",
      "Epoch328  loss:0.6748836934566497  accuracy:79.36262928260923\n",
      "Epoch329  loss:0.6706784671870992  accuracy:79.18137194297437\n",
      "Epoch330  loss:0.6749880617484451  accuracy:79.31118661557115\n",
      "Epoch331  loss:0.6722455685026943  accuracy:79.54330699687831\n",
      "Epoch332  loss:0.6713834148598835  accuracy:79.47863478746537\n",
      "Epoch333  loss:0.6748348657041789  accuracy:79.57509213797049\n",
      "Epoch334  loss:0.6784844663459808  accuracy:79.56873557401954\n",
      "Epoch335  loss:0.67603750214912  accuracy:79.67977435258791\n",
      "Epoch336  loss:0.6791494711302222  accuracy:79.45726887795972\n",
      "Epoch337  loss:0.6648124609142542  accuracy:79.76821700309273\n",
      "Epoch338  loss:0.6808526115491986  accuracy:79.5399198922065\n",
      "Epoch339  loss:0.6797850171104074  accuracy:79.453830637932\n",
      "Epoch340  loss:0.6740657584276051  accuracy:79.62445848781567\n",
      "Epoch341  loss:0.6783820077544078  accuracy:79.29040642004144\n",
      "Epoch342  loss:0.6808182939188556  accuracy:79.61544759047901\n",
      "Epoch343  loss:0.6725525511428713  accuracy:79.86766744258183\n",
      "Epoch344  loss:0.6870382119552232  accuracy:79.52457122205415\n",
      "Epoch345  loss:0.6793095498112962  accuracy:79.29493967857914\n",
      "Epoch346  loss:0.6827965408097952  accuracy:79.47494012829912\n",
      "Epoch347  loss:0.6774767988594249  accuracy:79.79860880493044\n",
      "Epoch348  loss:0.6797296162694694  accuracy:79.55580734463106\n",
      "Epoch349  loss:0.689708556421101  accuracy:79.38623838862482\n",
      "Epoch350  loss:0.6902655841084196  accuracy:79.3944439591511\n",
      "Epoch351  loss:0.6805361494421959  accuracy:79.57800868534203\n",
      "Epoch352  loss:0.6844829020090402  accuracy:79.16510141968773\n",
      "Epoch353  loss:0.6812051248503849  accuracy:79.81213207258341\n",
      "Epoch354  loss:0.6819506295956671  accuracy:79.58548931491845\n",
      "Epoch355  loss:0.686109795840457  accuracy:79.5554846683581\n",
      "Epoch356  loss:0.6927753510884941  accuracy:79.33864065575258\n",
      "Epoch357  loss:0.6875408742111178  accuracy:79.39739755498665\n",
      "Epoch358  loss:0.6944414317607879  accuracy:79.32703352820582\n",
      "Epoch359  loss:0.68489643803332  accuracy:79.72260889492071\n",
      "Epoch360  loss:0.6884023721911944  accuracy:79.4983673370459\n",
      "Epoch361  loss:0.6834410483716055  accuracy:79.59192879051656\n",
      "Epoch362  loss:0.6919919701293111  accuracy:79.1299932064979\n",
      "Epoch363  loss:0.6960977993905545  accuracy:79.13469721417398\n",
      "Epoch364  loss:0.7036590419709683  accuracy:79.36678387561406\n",
      "Epoch365  loss:0.6944044334581122  accuracy:79.21818547950723\n",
      "Epoch366  loss:0.687826088257134  accuracy:79.47995829694163\n",
      "Epoch367  loss:0.685148826148361  accuracy:79.78373698608948\n",
      "Epoch368  loss:0.6932158227544276  accuracy:79.68927180000166\n",
      "Epoch369  loss:0.6914646831341089  accuracy:79.31753779942008\n",
      "Epoch370  loss:0.6928924791514873  accuracy:79.43374934195965\n",
      "Epoch371  loss:0.6883639776147901  accuracy:79.72617271116764\n",
      "Epoch372  loss:0.6949172495165838  accuracy:79.78066559343287\n",
      "Epoch373  loss:0.7042163397767582  accuracy:79.5073035444632\n",
      "Epoch374  loss:0.6975802472967191  accuracy:79.45086992240844\n",
      "Epoch375  loss:0.6992470606695861  accuracy:79.22085992607421\n",
      "Epoch376  loss:0.7064858278958126  accuracy:79.3749729971059\n",
      "Epoch377  loss:0.7008307890268043  accuracy:79.44854453601286\n",
      "Epoch378  loss:0.6902294147759676  accuracy:79.59236716945455\n",
      "Epoch379  loss:0.7100969564693514  accuracy:79.3847826140654\n",
      "Epoch380  loss:0.697113166982308  accuracy:79.45184370674393\n",
      "Epoch381  loss:0.7114579316228626  accuracy:79.39680512112389\n",
      "Epoch382  loss:0.7072938234952745  accuracy:79.32022022853172\n",
      "Epoch383  loss:0.6986778483260423  accuracy:79.43082662966728\n",
      "Epoch384  loss:0.7082907357485965  accuracy:79.44311079779857\n",
      "Epoch385  loss:0.704373776644934  accuracy:79.63408607339426\n",
      "Epoch386  loss:0.6972048515221104  accuracy:79.5612779962346\n",
      "Epoch387  loss:0.7024582078680396  accuracy:79.70589601370878\n",
      "Epoch388  loss:0.6944839363917708  accuracy:79.60084316146376\n",
      "Epoch389  loss:0.7026196259772405  accuracy:79.62342901063276\n",
      "Epoch390  loss:0.707644822797738  accuracy:79.48588373930764\n",
      "Epoch391  loss:0.7120956049824599  accuracy:79.55590724252905\n",
      "Epoch392  loss:0.7078784894023558  accuracy:79.49279835957252\n",
      "Epoch393  loss:0.7102837408077903  accuracy:79.7268949502611\n",
      "Epoch394  loss:0.7123922009544911  accuracy:79.80687935993899\n",
      "Epoch395  loss:0.7129399025579916  accuracy:79.45861643088472\n",
      "Epoch396  loss:0.7105342429480516  accuracy:79.34542831996887\n",
      "Epoch397  loss:0.7080962170613929  accuracy:79.59558108855454\n",
      "Epoch398  loss:0.7026468914467844  accuracy:79.58522543436456\n",
      "Epoch399  loss:0.7156149468384684  accuracy:79.4967097721577\n",
      "Epoch400  loss:0.7122001334559172  accuracy:79.37368570256312\n",
      "Epoch401  loss:0.7200976313324645  accuracy:79.3684920216372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch402  loss:0.7146137198084034  accuracy:79.54645850964405\n",
      "Epoch403  loss:0.7148583146627062  accuracy:79.28410145719045\n",
      "Epoch404  loss:0.7248706777201733  accuracy:79.39709606652394\n",
      "Epoch405  loss:0.7159262916829903  accuracy:79.51002636746585\n",
      "Epoch406  loss:0.7240250683098566  accuracy:79.23846826856143\n",
      "Epoch407  loss:0.7229719304596073  accuracy:79.43542975164016\n",
      "Epoch408  loss:0.724940707033966  accuracy:79.27602527349387\n",
      "Epoch409  loss:0.7254998823511414  accuracy:79.47866464504058\n",
      "Epoch410  loss:0.7329430838493863  accuracy:79.13450351614786\n",
      "Epoch411  loss:0.7325914091721643  accuracy:79.59873501974214\n",
      "Epoch412  loss:0.7407091720102471  accuracy:79.53578168712889\n",
      "Epoch413  loss:0.7246176148357335  accuracy:79.68200268092941\n",
      "Epoch414  loss:0.730041321471799  accuracy:79.63325706344283\n",
      "Epoch415  loss:0.7377906578942202  accuracy:79.279193859009\n",
      "Epoch416  loss:0.7189221077773255  accuracy:79.63700061112779\n",
      "Epoch417  loss:0.7338810355635359  accuracy:79.62754906300745\n",
      "Epoch418  loss:0.7202178294304759  accuracy:79.50794076246834\n",
      "Epoch419  loss:0.7251655441010374  accuracy:79.4812132228112\n",
      "Epoch420  loss:0.7346417888271389  accuracy:79.40219893677185\n",
      "Epoch421  loss:0.7368611920857803  accuracy:79.43157733949008\n",
      "Epoch422  loss:0.7324152648448944  accuracy:79.42844037004305\n",
      "Epoch423  loss:0.7359942339826375  accuracy:79.76929977493317\n",
      "Epoch424  loss:0.753907218913082  accuracy:79.2714542963306\n",
      "Epoch425  loss:0.7362730580382049  accuracy:79.42621922386398\n",
      "Epoch426  loss:0.7322132435569074  accuracy:79.66243680568377\n",
      "Epoch427  loss:0.7341099865210708  accuracy:79.8287597234963\n",
      "Epoch428  loss:0.7323378275847062  accuracy:79.39992564762184\n",
      "Epoch429  loss:0.7565153624862433  accuracy:79.47057134730403\n",
      "Epoch430  loss:0.7470572435529903  accuracy:79.67167037169469\n",
      "Epoch431  loss:0.7528736877022312  accuracy:79.29273176561982\n",
      "Epoch432  loss:0.7343986905005293  accuracy:79.43499334734285\n",
      "Epoch433  loss:0.7392885680776087  accuracy:79.3530712400364\n",
      "Epoch434  loss:0.748431800538674  accuracy:79.57872105224497\n",
      "Epoch435  loss:0.7406433348427526  accuracy:79.5159197222702\n",
      "Epoch436  loss:0.7435163825633936  accuracy:79.42801689341324\n",
      "Epoch437  loss:0.7392689781438093  accuracy:79.44573224891741\n",
      "Epoch438  loss:0.744450658839196  accuracy:79.36204174206817\n",
      "Epoch439  loss:0.7389287007157691  accuracy:79.39892339168253\n",
      "Epoch440  loss:0.7635787304141559  accuracy:79.53699964857572\n",
      "Epoch441  loss:0.7611983344890177  accuracy:79.37296986249437\n",
      "Epoch442  loss:0.7546798128401859  accuracy:79.57467755635523\n",
      "Epoch443  loss:0.7571987694012932  accuracy:79.43992398305456\n",
      "Epoch444  loss:0.7503007622435689  accuracy:79.3361437203708\n",
      "Epoch445  loss:0.7460499357897791  accuracy:79.47914198342046\n",
      "Epoch446  loss:0.7521502789808437  accuracy:79.39121215430706\n",
      "Epoch447  loss:0.7477930390741676  accuracy:79.38701519973168\n",
      "Epoch448  loss:0.75178189673461  accuracy:79.74095513598364\n",
      "Epoch449  loss:0.7456958423368633  accuracy:79.60455413578192\n",
      "Epoch450  loss:0.7602921933867037  accuracy:78.9947354618566\n",
      "Epoch451  loss:0.7579753954953049  accuracy:79.70980126096656\n",
      "Epoch452  loss:0.7552653075661508  accuracy:79.69400248765413\n",
      "Epoch453  loss:0.7658068874268793  accuracy:79.49879232806116\n",
      "Epoch454  loss:0.7668308549909854  accuracy:79.7136352284414\n",
      "Epoch455  loss:0.7561202113516627  accuracy:79.19854517678303\n",
      "Epoch456  loss:0.7583627768326551  accuracy:79.31940594965735\n",
      "Epoch457  loss:0.7696118916850537  accuracy:79.35806678698437\n",
      "Epoch458  loss:0.7856865077861585  accuracy:79.2036464950189\n",
      "Epoch459  loss:0.7667330357595349  accuracy:79.38360957475493\n",
      "Epoch460  loss:0.7565960817504674  accuracy:79.50092797364081\n",
      "Epoch461  loss:0.7624736219877378  accuracy:79.41139196665958\n",
      "Epoch462  loss:0.7741664450964889  accuracy:79.24635161393269\n",
      "Epoch463  loss:0.759143160196254  accuracy:79.19064130773367\n",
      "Epoch464  loss:0.7738342639524489  accuracy:79.31182017289883\n",
      "Epoch465  loss:0.7817075481289067  accuracy:79.29661159015208\n",
      "Epoch466  loss:0.7546706513385287  accuracy:79.58678973665927\n",
      "Epoch467  loss:0.7675088656309527  accuracy:79.75369236004993\n",
      "Epoch468  loss:0.7682689155451954  accuracy:79.39428344433286\n",
      "Epoch469  loss:0.7725401674630118  accuracy:79.58461831710535\n",
      "Epoch470  loss:0.7729569694958628  accuracy:79.21394498964783\n",
      "Epoch471  loss:0.7716665330226534  accuracy:79.49854650771273\n",
      "Epoch472  loss:0.7878080101683735  accuracy:79.64262537010953\n",
      "Epoch473  loss:0.7714507701573893  accuracy:79.30424075542417\n",
      "Epoch474  loss:0.7840131101198495  accuracy:79.44848888932681\n",
      "Epoch475  loss:0.7768113968428223  accuracy:79.54599897605652\n",
      "Epoch476  loss:0.7827221858431586  accuracy:79.44210046642175\n",
      "Epoch477  loss:0.7777091797208414  accuracy:79.70311122425751\n",
      "Epoch478  loss:0.7797728465578984  accuracy:79.54376223417685\n",
      "Epoch479  loss:0.779209583159536  accuracy:79.85309144615948\n",
      "Epoch480  loss:0.7976438318146393  accuracy:79.62942525417408\n",
      "Epoch481  loss:0.7755661073664669  accuracy:79.63748928700821\n",
      "Epoch482  loss:0.7919479292235339  accuracy:79.26035652095847\n",
      "Epoch483  loss:0.7714166174642743  accuracy:79.95594240474358\n",
      "Epoch484  loss:0.7809949474642053  accuracy:79.5011177542401\n",
      "Epoch485  loss:0.7869508659234269  accuracy:79.89108497446286\n",
      "Epoch486  loss:0.7885273091058479  accuracy:79.58777019511889\n",
      "Epoch487  loss:0.7776639668649296  accuracy:79.49013843824072\n",
      "Epoch488  loss:0.7820928994566203  accuracy:79.74952426553453\n",
      "Epoch489  loss:0.8008027330535694  accuracy:79.64888376982125\n",
      "Epoch490  loss:0.7704296708223409  accuracy:79.90840679220578\n",
      "Epoch491  loss:0.790871360633173  accuracy:79.61082371127341\n",
      "Epoch492  loss:0.7896712883826692  accuracy:79.59558673446834\n",
      "Epoch493  loss:0.7898075824050467  accuracy:79.89354491174748\n",
      "Epoch494  loss:0.7862695187854114  accuracy:79.7531182606153\n",
      "Epoch495  loss:0.7954549961315935  accuracy:79.69651241877742\n",
      "Epoch496  loss:0.7974446454143617  accuracy:79.75250306236904\n",
      "Epoch497  loss:0.7948919584567191  accuracy:79.87556028667865\n",
      "Epoch498  loss:0.799392434951733  accuracy:79.57715845727891\n",
      "Epoch499  loss:0.7937185128219427  accuracy:79.6498606529139\n",
      "Epoch500  loss:0.7853402665816247  accuracy:79.5991313751581\n"
     ]
    }
   ],
   "source": [
    "server = Server()\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset)\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "acc_valid = []\n",
    "loss_valid = []\n",
    "\n",
    "early_stopping = Early_Stopping(args.partience)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(args.global_epochs):\n",
    "  sample_worker = server.sample_worker(workers)\n",
    "  server.send_model(sample_worker)\n",
    "\n",
    "  acc_train_avg = 0.0\n",
    "  loss_train_avg = 0.0\n",
    "  acc_valid_avg = 0.0\n",
    "  loss_valid_avg = 0.0\n",
    "  for worker in sample_worker:\n",
    "    acc_train_tmp,loss_train_tmp = worker.local_train()\n",
    "    acc_valid_tmp,loss_valid_tmp = worker.validate()\n",
    "    acc_train_avg += acc_train_tmp/len(sample_worker)\n",
    "    loss_train_avg += loss_train_tmp/len(sample_worker)\n",
    "    acc_valid_avg += acc_valid_tmp/len(sample_worker)\n",
    "    loss_valid_avg += loss_valid_tmp/len(sample_worker)\n",
    "  server.aggregate_model(sample_worker)\n",
    "  '''\n",
    "  server.model.to(args.device)\n",
    "  for worker in workers:\n",
    "    acc_valid_tmp,loss_valid_tmp = test(server.model,args.criterion,worker.valloader)\n",
    "    acc_valid_avg += acc_valid_tmp/len(workers)\n",
    "    loss_valid_avg += loss_valid_tmp/len(workers)\n",
    "  server.model.to('cpu')\n",
    "  '''\n",
    "  print('Epoch{}  loss:{}  accuracy:{}'.format(epoch+1,loss_valid_avg,acc_valid_avg))\n",
    "  acc_train.append(acc_train_avg)\n",
    "  loss_train.append(loss_train_avg)\n",
    "  acc_valid.append(acc_valid_avg)\n",
    "  loss_valid.append(loss_valid_avg)\n",
    "\n",
    "  if early_stopping.validate(loss_valid_avg):\n",
    "    print('Early Stop')\n",
    "    break\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 accuracy:73.57910906298002  loss:1.2372404336929321\n",
      "Worker2 accuracy:86.27049180327869  loss:0.5966750383377075\n",
      "Worker3 accuracy:72.38095238095238  loss:1.1705563068389893\n",
      "Worker4 accuracy:82.73809523809524  loss:0.6692302823066711\n",
      "Worker5 accuracy:82.2439024390244  loss:0.4627133049070835\n",
      "Worker6 accuracy:68.24512534818942  loss:1.1762255430221558\n",
      "Worker7 accuracy:80.84210526315789  loss:0.7236917018890381\n",
      "Worker8 accuracy:88.55054811205846  loss:0.5238895416259766\n",
      "Worker9 accuracy:88.6624203821656  loss:0.5901362299919128\n",
      "Worker10 accuracy:85.31746031746032  loss:0.44625425338745117\n",
      "Worker11 accuracy:77.14285714285714  loss:0.9603099822998047\n",
      "Worker12 accuracy:77.84172661870504  loss:1.0478436946868896\n",
      "Worker13 accuracy:89.79591836734694  loss:0.5337216258049011\n",
      "Worker14 accuracy:75.51724137931035  loss:0.996197521686554\n",
      "Worker15 accuracy:79.95642701525054  loss:0.941811203956604\n",
      "Worker16 accuracy:65.06550218340611  loss:1.2389367818832397\n",
      "Worker17 accuracy:76.41509433962264  loss:0.9750219583511353\n",
      "Worker18 accuracy:70.20109689213893  loss:1.2097368240356445\n",
      "Worker19 accuracy:73.03370786516854  loss:1.1052676439285278\n",
      "Worker20 accuracy:83.45864661654136  loss:0.6320821046829224\n",
      "Test  loss:0.8618770988658071  accuracy:78.86292143838548\n"
     ]
    }
   ],
   "source": [
    "acc_test = []\n",
    "loss_test = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.global_model = copy.deepcopy(server.global_model)\n",
    "  _,_ = worker.local_train()\n",
    "  acc_tmp,loss_tmp = test(worker.local_model,args.criterion_ce,worker.testloader)\n",
    "  acc_test.append(acc_tmp)\n",
    "  loss_test.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "  worker.local_model = worker.local_model.to('cpu')\n",
    "  worker.global_model = worker.global_model.to('cpu')\n",
    "  del worker.global_model\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_test_avg = sum(acc_test)/len(acc_test)\n",
    "loss_test_avg = sum(loss_test)/len(loss_test)\n",
    "print('Test  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
