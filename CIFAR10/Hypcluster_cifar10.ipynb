{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    # random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 42\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O0TfzOhU-QlG"
   },
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 40\n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 500\n",
    "    self.local_epochs = 2\n",
    "    self.lr = None\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 500\n",
    "    self.worker_num = 20\n",
    "    self.sample_num = 20\n",
    "    self.cluster_num = 2\n",
    "    self.unlabeleddata_size = 1000\n",
    "    self.device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    self.alpha_label = 0.5\n",
    "    self.alpha_size = 10\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned value\n",
    "lr = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = []\n",
    "lr_list.append(10**-3.0)\n",
    "lr_list.append(10**-2.5)\n",
    "lr_list.append(10**-2.0)\n",
    "lr_list.append(10**-1.5)\n",
    "lr_list.append(10**-1.0)\n",
    "lr_list.append(10**-0.5)\n",
    "lr_list.append(10**0.0)\n",
    "lr_list.append(10**0.5)\n",
    "\n",
    "args.lr = lr_list[lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.label[idx]\n",
    "        if self.transform:\n",
    "            out_data = self.transform(out_data)\n",
    "        return out_data, out_label\n",
    "    \n",
    "class DatasetFromSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.subset[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    \n",
    "class GlobalDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,federated_dataset,transform=None):\n",
    "    self.transform = transform\n",
    "    self.data = []\n",
    "    self.label = []\n",
    "    for dataset in federated_dataset:\n",
    "      for (data,label) in dataset:\n",
    "        self.data.append(data)\n",
    "        self.label.append(label)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    out_data = self.data[idx]\n",
    "    out_label = self.label[idx]\n",
    "    if self.transform:\n",
    "        out_data = self.transform(out_data)\n",
    "    return out_data, out_label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "class UnlabeledDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,transform=None):\n",
    "    self.transform = transform\n",
    "    self.data = []\n",
    "    self.target = None\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    out_data = self.data[idx]\n",
    "    out_label = 'unlabeled'\n",
    "    if self.transform:\n",
    "        out_data = self.transform(out_data)\n",
    "    return out_data, out_label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(Centralized=False,unlabeled_data=False):\n",
    "    \n",
    "    transform_train = transforms.Compose([transforms.ToPILImage(),\n",
    "                                    transforms.RandomCrop(32, padding=2),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.491372549, 0.482352941, 0.446666667), (0.247058824, 0.243529412, 0.261568627))])\n",
    "    transform_test = transforms.Compose([transforms.ToPILImage(),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.491372549, 0.482352941, 0.446666667), (0.247058824, 0.243529412, 0.261568627))])\n",
    "\n",
    "    # download train data\n",
    "    all_trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True)\n",
    "    #trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "    # download test data\n",
    "    all_testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True)\n",
    "    #testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "    \n",
    "    ## get unlabeled dataset\n",
    "    if unlabeled_data:\n",
    "        unlabeled_dataset = UnlabeledDataset(transform_test)\n",
    "        idx = sorted(random.sample(range(len(all_trainset)),args.unlabeleddata_size))\n",
    "        unlabeled_dataset.data = np.array([all_trainset.data[i]  for i in idx])\n",
    "        all_trainset.data = np.delete(all_trainset.data,idx,0)\n",
    "        all_trainset.targets = np.delete(all_trainset.targets,idx,0)\n",
    "    all_train_data = np.array(all_trainset.data)\n",
    "    all_train_label = np.array(all_trainset.targets)\n",
    "    all_test_data = np.array(all_testset.data)\n",
    "    all_test_label = np.array(all_testset.targets)\n",
    "    print('Train:{} Test:{}'.format(len(all_train_data),len(all_test_data)))\n",
    "\n",
    "\n",
    "    ## Data size heterogeneity\n",
    "    data_proportions = np.random.dirichlet(np.repeat(args.alpha_size, args.worker_num))\n",
    "    train_data_proportions = np.array([0 for _ in range(args.worker_num)])\n",
    "    test_data_proportions = np.array([0 for _ in range(args.worker_num)])\n",
    "    for i in range(len(data_proportions)):\n",
    "        if i==(len(data_proportions)-1):\n",
    "            train_data_proportions = train_data_proportions.astype('int64')\n",
    "            test_data_proportions = test_data_proportions.astype('int64')\n",
    "            train_data_proportions[-1] = len(all_train_data) - np.sum(train_data_proportions[:-1])\n",
    "            test_data_proportions[-1] = len(all_test_data) - np.sum(test_data_proportions[:-1])\n",
    "        else:\n",
    "            train_data_proportions[i] = (data_proportions[i] * len(all_train_data))\n",
    "            test_data_proportions[i] = (data_proportions[i] * len(all_test_data))\n",
    "    min_size = 0\n",
    "    K = 10\n",
    "\n",
    "    '''\n",
    "    label_list = np.arange(10)\n",
    "    np.random.shuffle(label_list)\n",
    "    '''\n",
    "    label_list = list(range(K))\n",
    "\n",
    "\n",
    "    ## Data distribution heterogeneity\n",
    "    while min_size<10:\n",
    "        idx_train_batch = [[] for _ in range(args.worker_num)]\n",
    "        idx_test_batch = [[] for _ in range(args.worker_num)]\n",
    "        for k in label_list:\n",
    "            proportions_train = np.random.dirichlet(np.repeat(args.alpha_label, args.worker_num))\n",
    "            proportions_test = copy.deepcopy(proportions_train)\n",
    "            idx_k_train = np.where(all_train_label == k)[0]\n",
    "            idx_k_test = np.where(all_test_label == k)[0]\n",
    "            np.random.shuffle(idx_k_train)\n",
    "            np.random.shuffle(idx_k_test)\n",
    "            ## Balance (train)\n",
    "            proportions_train = np.array([p*(len(idx_j)<train_data_proportions[i]) for i,(p,idx_j) in enumerate(zip(proportions_train,idx_train_batch))])\n",
    "            proportions_train = proportions_train/proportions_train.sum()\n",
    "            proportions_train = (np.cumsum(proportions_train)*len(idx_k_train)).astype(int)[:-1]\n",
    "            idx_train_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_train_batch,np.split(idx_k_train,proportions_train))]\n",
    "\n",
    "            ## Balance (test)\n",
    "            proportions_test = np.array([p*(len(idx_j)<test_data_proportions[i]) for i,(p,idx_j) in enumerate(zip(proportions_test,idx_test_batch))])\n",
    "            proportions_test = proportions_test/proportions_test.sum()\n",
    "            proportions_test = (np.cumsum(proportions_test)*len(idx_k_test)).astype(int)[:-1]\n",
    "            idx_test_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_test_batch,np.split(idx_k_test,proportions_test))]\n",
    "\n",
    "            min_size = min([len(idx_j) for idx_j in idx_train_batch])\n",
    "\n",
    "    federated_trainset = []\n",
    "    federated_testset = []\n",
    "    for i in range(args.worker_num):\n",
    "        ## create trainset\n",
    "        data = [all_train_data[idx] for idx in idx_train_batch[i]]\n",
    "        label = [all_train_label[idx] for idx in idx_train_batch[i]]\n",
    "        federated_trainset.append(LocalDataset())\n",
    "        federated_trainset[-1].data = data\n",
    "        federated_trainset[-1].label = label\n",
    "\n",
    "        ## create testset\n",
    "        data = [all_test_data[idx] for idx in idx_test_batch[i]]\n",
    "        label = [all_test_label[idx] for idx in idx_test_batch[i]]\n",
    "        federated_testset.append(LocalDataset())\n",
    "        federated_testset[-1].data = data\n",
    "        federated_testset[-1].label = label\n",
    "\n",
    "        \n",
    "    ## split trainset\n",
    "    federated_valset = [None]*args.worker_num\n",
    "    for i in range(args.worker_num):\n",
    "        n_samples = len(federated_trainset[i])\n",
    "        if n_samples==1:\n",
    "            train_subset = federated_trainset[i]\n",
    "            val_subset = copy.deepcopy(federated_trainset[i])\n",
    "        else:\n",
    "            train_size = int(len(federated_trainset[i]) * 0.8) \n",
    "            val_size = n_samples - train_size \n",
    "            train_subset,val_subset = torch.utils.data.random_split(federated_trainset[i], [train_size, val_size])\n",
    "\n",
    "        federated_trainset[i] = DatasetFromSubset(train_subset)\n",
    "        federated_valset[i] = DatasetFromSubset(val_subset)\n",
    "\n",
    "    ## show data distribution\n",
    "    H = 4\n",
    "    W = 5\n",
    "    fig, axs = plt.subplots(H, W, figsize=(20, 5))\n",
    "    x = np.arange(1,11)\n",
    "    for i, (trainset,valset,testset) in enumerate(zip(federated_trainset,federated_valset,federated_testset)):\n",
    "        bottom = [0]*10\n",
    "        count = [0]*10\n",
    "        for _,label in trainset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        for j in range(len(count)):\n",
    "            bottom[j]+=count[j]\n",
    "        count = [0]*10\n",
    "        for _,label in valset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        for j in range(len(count)):\n",
    "            bottom[j]+=count[j]\n",
    "        count = [0]*10\n",
    "        for _,label in testset:\n",
    "            count[label] += 1\n",
    "        axs[int(i/W), i%W].bar(x, count,bottom=bottom)\n",
    "        #axs[int(i/W), i%W].title(\"worker{}\".format(i+1), fontsize=12, color = \"green\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ## get global dataset\n",
    "    if Centralized:\n",
    "        global_trainset = GlobalDataset(federated_trainset)\n",
    "        global_valset = GlobalDataset(federated_valset)\n",
    "        global_testset =  GlobalDataset(federated_testset)\n",
    "        \n",
    "        #show_cifer(global_trainset.data,global_testset.label, cifar10_labels)\n",
    "\n",
    "        global_trainset.transform = transform_train\n",
    "        global_valset.transform = transform_test\n",
    "        global_testset.transform = transform_test\n",
    "\n",
    "        global_trainloader = torch.utils.data.DataLoader(global_trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "        global_valloader = torch.utils.data.DataLoader(global_valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "        global_testloader = torch.utils.data.DataLoader(global_testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "\n",
    "    ## set transform\n",
    "    for i in range(args.worker_num):\n",
    "        federated_trainset[i].transform = transform_train\n",
    "        federated_valset[i].transform = transform_test\n",
    "        federated_testset[i].transform = transform_test\n",
    "    \n",
    "    if Centralized and unlabeled_data:\n",
    "        return federated_trainset,federated_valset,federated_testset,global_trainloader,global_valloader,global_testloader,unlabeled_dataset\n",
    "    if Centralized:\n",
    "        return federated_trainset,federated_valset,federated_testset,global_trainloader,global_valloader,global_testloader\n",
    "    elif unlabeled_data:\n",
    "        return federated_trainset,federated_valset,federated_testset,unlabeled_dataset\n",
    "    else:\n",
    "        return federated_trainset,federated_valset,federated_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train:49000 Test:10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "federated_trainset,federated_valset,federated_testset,unlabeled_dataset = get_dataset(unlabeled_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39191, 9809, 10000]\n"
     ]
    }
   ],
   "source": [
    "total = [0,0,0]\n",
    "for i in range(args.worker_num):\n",
    "    total[0]+=len(federated_trainset[i])\n",
    "    total[1]+=len(federated_valset[i])\n",
    "    total[2]+=len(federated_testset[i])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZU3vAAb9-6SD"
   },
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG model \n",
    "    '''\n",
    "    def __init__(self, features, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            #print(\"in_channels: {}, v: {}\".format(in_channels, v))\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
    "          512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGGConvBlocks(nn.Module):\n",
    "    '''\n",
    "    VGG containers that only contains the conv layers \n",
    "    '''\n",
    "    def __init__(self, features, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class VGGContainer(nn.Module):\n",
    "    '''\n",
    "    VGG model \n",
    "    '''\n",
    "    def __init__(self, features, input_dim, hidden_dims, num_classes=10):\n",
    "        super(VGGContainer, self).__init__()\n",
    "        self.features = features\n",
    "        # note: we hard coded here a bit by assuming we only have two hidden layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dims[1], num_classes),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def matched_vgg11(matched_shapes):\n",
    "    # [(67, 27), (67,), (132, 603), (132,), (260, 1188), (260,), (261, 2340), (261,), (516, 2349), (516,), (517, 4644), (517,), \n",
    "    # (516, 4653), (516,), (516, 4644), (516,), (516, 515), (515,), (515, 515), (515,), (515, 10), (10,)]\n",
    "    processed_matched_shape = [matched_shapes[0][0], \n",
    "                                'M', \n",
    "                                matched_shapes[2][0], \n",
    "                                'M', \n",
    "                                matched_shapes[4][0], \n",
    "                                matched_shapes[6][0], \n",
    "                                'M', \n",
    "                                matched_shapes[8][0], \n",
    "                                matched_shapes[10][0], \n",
    "                                'M', \n",
    "                                matched_shapes[12][0], \n",
    "                                matched_shapes[14][0], \n",
    "                                'M']\n",
    "    return VGGContainer(make_layers(processed_matched_shape), input_dim=matched_shapes[16][0], \n",
    "            hidden_dims=[matched_shapes[16][1], matched_shapes[18][1]], num_classes=10)\n",
    "\n",
    "\n",
    "def vgg11():\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
    "    return VGG(make_layers(cfg['A']))\n",
    "\n",
    "\n",
    "def vgg11_bn(num_classes=10):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['A'], batch_norm=True), num_classes=num_classes)\n",
    "\n",
    "\n",
    "def vgg13():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n",
    "    return VGG(make_layers(cfg['B']))\n",
    "\n",
    "\n",
    "def vgg13_bn():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg16():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n",
    "    return VGG(make_layers(cfg['D']))\n",
    "\n",
    "\n",
    "def vgg16_bn():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n",
    "    return VGG(make_layers(cfg['E']))\n",
    "\n",
    "\n",
    "def vgg19_bn():\n",
    "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['E'], batch_norm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self):\n",
    "    self.models = []\n",
    "    for i in range(args.cluster_num):\n",
    "      self.models.append(vgg13())\n",
    "\n",
    "  def model_initialize(self,workers):\n",
    "    sample_worker = self.sample_worker(workers)\n",
    "    self.send_models(sample_worker)\n",
    "    for i,worker in enumerate(sample_worker):\n",
    "      worker.cluster = i%args.cluster_num\n",
    "      _ = worker.local_train()\n",
    "    self.aggregate_models(sample_worker)\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(federated_trainset[i],federated_valset[i],federated_testset[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "\n",
    "  def send_models(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "\n",
    "    for worker in workers:\n",
    "      worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "      worker.models = copy.deepcopy(self.models)\n",
    "      for i in range(args.cluster_num):\n",
    "        worker.models[i] = worker.models[i].to(args.device)\n",
    "\n",
    "  def aggregate_models(self,workers):\n",
    "    new_params = []\n",
    "    for i in range(args.cluster_num):   \n",
    "      new_params.append(OrderedDict())\n",
    "    total_num = [0]*args.cluster_num\n",
    "    for worker in workers:\n",
    "      total_num[worker.cluster] += worker.train_data_num\n",
    "    count = [0]*args.cluster_num    \n",
    "    for worker in workers:\n",
    "      worker_state = worker.models[worker.cluster].state_dict()\n",
    "      for key in worker_state.keys():\n",
    "        if count[worker.cluster]==0:\n",
    "          new_params[worker.cluster][key] = 1.0*worker_state[key]*worker.train_data_num/total_num[worker.cluster]\n",
    "        else:\n",
    "          new_params[worker.cluster][key] += 1.0*worker_state[key]*worker.train_data_num/total_num[worker.cluster]\n",
    "      count[worker.cluster] += 1\n",
    "      for i in range(args.cluster_num):\n",
    "        worker.models[i] = worker.models[i].to('cpu')\n",
    "      del worker.models\n",
    "\n",
    "    for i in range(args.cluster_num):\n",
    "      if total_num[i]!=0:\n",
    "        self.models[i].load_state_dict(new_params[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,trainset,valset,testset):\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.models = None\n",
    "    self.cluster = None\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "    self.aggregation_weight = None\n",
    "\n",
    "  def local_train(self):\n",
    "    acc_train,loss_train = train(self.models[self.cluster],args.criterion,self.trainloader,args.local_epochs)\n",
    "    acc_valid,loss_valid = test(self.models[self.cluster],args.criterion,self.valloader)\n",
    "    return acc_train,loss_train,acc_valid,loss_valid\n",
    "\n",
    "  def clustering(self,models):\n",
    "    for i in range(args.cluster_num):\n",
    "      if i==0:\n",
    "        cluster = 0\n",
    "        _,loss = test(models[i],args.criterion,self.trainloader)\n",
    "      else:\n",
    "        _,tmp = test(models[i],args.criterion,self.trainloader)\n",
    "        if tmp<loss:\n",
    "          cluster = i\n",
    "    self.cluster = cluster\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7-GY66gROuEU"
   },
   "outputs": [],
   "source": [
    "def train(model,criterion,trainloader,epochs):\n",
    "  optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for (data,labels) in trainloader:\n",
    "      data,labels = Variable(data),Variable(labels)\n",
    "      data,labels = data.to(args.device),labels.to(args.device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(data)\n",
    "      loss = criterion(outputs,labels)\n",
    "      running_loss += loss.item()\n",
    "      predicted = torch.argmax(outputs,dim=1)\n",
    "      correct += (predicted==labels).sum().item()\n",
    "      count += len(labels)\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "      optimizer.step()\n",
    "\n",
    "  return 100.0*correct/count,running_loss/len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  count = 0\n",
    "  for (data,labels) in testloader:\n",
    "    data,labels = data.to(args.device),labels.to(args.device)\n",
    "    outputs = model(data)\n",
    "    running_loss += criterion(outputs,labels).item()\n",
    "    predicted = torch.argmax(outputs,dim=1)\n",
    "    correct += (predicted==labels).sum().item()\n",
    "    count += len(labels)\n",
    "\n",
    "  accuracy = 100.0*correct/count\n",
    "  loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "  return accuracy,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WMO7_WSLHeGl"
   },
   "outputs": [],
   "source": [
    "class Early_Stopping():\n",
    "  def __init__(self,partience):\n",
    "    self.step = 0\n",
    "    self.loss = float('inf')\n",
    "    self.partience = partience\n",
    "\n",
    "  def validate(self,loss):\n",
    "    if self.loss<loss:\n",
    "      self.step += 1\n",
    "      if self.step>self.partience:\n",
    "        return True\n",
    "    else:\n",
    "      self.step = 0\n",
    "      self.loss = loss\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "-noG_98IR-nZ",
    "outputId": "78a6ebe2-854a-4f83-dc45-5c4ac35b69e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1  loss:1.6331642270088194  accuracy:42.33116331868391\n",
      "Epoch2  loss:1.4816619038581846  accuracy:43.51094328763154\n",
      "Epoch3  loss:1.4736214756965638  accuracy:43.565912313447875\n",
      "Epoch4  loss:1.4662814199924468  accuracy:43.72183208872053\n",
      "Epoch5  loss:1.457570168375969  accuracy:44.496378493277184\n",
      "Epoch6  loss:1.4506900578737256  accuracy:44.56562942362666\n",
      "Epoch7  loss:1.4407233834266662  accuracy:44.793657719148754\n",
      "Epoch8  loss:1.427360486984253  accuracy:45.82021025622126\n",
      "Epoch9  loss:1.3768667608499527  accuracy:49.525259271876045\n",
      "Epoch10  loss:1.3237205862998962  accuracy:52.84124336401177\n",
      "Epoch11  loss:1.280868783593178  accuracy:53.25233899650368\n",
      "Epoch12  loss:1.2343385905027389  accuracy:55.080089826621084\n",
      "Epoch13  loss:1.2124754965305329  accuracy:55.16439063563963\n",
      "Epoch14  loss:1.19622410684824  accuracy:55.83482821965234\n",
      "Epoch15  loss:1.1726156622171404  accuracy:56.193387986893434\n",
      "Epoch16  loss:1.1595454186201093  accuracy:57.22596348516061\n",
      "Epoch17  loss:1.1445778012275696  accuracy:57.929422813766635\n",
      "Epoch18  loss:1.135908956825733  accuracy:57.12832614978522\n",
      "Epoch19  loss:1.125919571518898  accuracy:58.51110515821063\n",
      "Epoch20  loss:1.1161148399114607  accuracy:59.12009723575205\n",
      "Epoch21  loss:1.104527920484543  accuracy:59.67901015563302\n",
      "Epoch22  loss:1.0797784119844436  accuracy:60.19557338074391\n",
      "Epoch23  loss:1.0759200483560563  accuracy:60.761945755595015\n",
      "Epoch24  loss:1.0568442299962044  accuracy:61.104026286693276\n",
      "Epoch25  loss:1.045157651603222  accuracy:61.67244953211527\n",
      "Epoch26  loss:1.0262819916009902  accuracy:62.113865471001795\n",
      "Epoch27  loss:1.0335316836833952  accuracy:61.77713147917633\n",
      "Epoch28  loss:1.0201807737350461  accuracy:62.82633094500646\n",
      "Epoch29  loss:1.0100430637598037  accuracy:62.80874150020686\n",
      "Epoch30  loss:1.0013937994837763  accuracy:62.95216885839668\n",
      "Epoch31  loss:0.9964667037129402  accuracy:63.56820028939363\n",
      "Epoch32  loss:0.9863751828670503  accuracy:63.53749625420221\n",
      "Epoch33  loss:0.9671731501817702  accuracy:64.09915959089746\n",
      "Epoch34  loss:0.9861692994832993  accuracy:63.4419713990179\n",
      "Epoch35  loss:0.980238825082779  accuracy:63.44372560881795\n",
      "Epoch36  loss:0.955054634809494  accuracy:64.77198582840798\n",
      "Epoch37  loss:0.9520546615123747  accuracy:65.12130500778059\n",
      "Epoch38  loss:0.9510420516133309  accuracy:65.3504830456272\n",
      "Epoch39  loss:0.9566726908087729  accuracy:64.98331006027217\n",
      "Epoch40  loss:0.9442193001508713  accuracy:65.4475239632593\n",
      "Epoch41  loss:0.9293124884366989  accuracy:65.92155231110755\n",
      "Epoch42  loss:0.9367474883794785  accuracy:65.7898411312871\n",
      "Epoch43  loss:0.9285279378294945  accuracy:66.34707626437063\n",
      "Epoch44  loss:0.9188340678811073  accuracy:65.64861664787796\n",
      "Epoch45  loss:0.9022042199969293  accuracy:66.59059837255577\n",
      "Epoch46  loss:0.9161200717091559  accuracy:66.42350840900754\n",
      "Epoch47  loss:0.9072646558284758  accuracy:66.77335531713734\n",
      "Epoch48  loss:0.8981673896312713  accuracy:67.24177438405451\n",
      "Epoch49  loss:0.9063053518533708  accuracy:66.78797683538542\n",
      "Epoch50  loss:0.8999896794557571  accuracy:67.67206832581671\n",
      "Epoch51  loss:0.8630174100399017  accuracy:68.72297892103933\n",
      "Epoch52  loss:0.8562898173928261  accuracy:68.63366363452522\n",
      "Epoch53  loss:0.8606549426913264  accuracy:68.73749567439178\n",
      "Epoch54  loss:0.857323145866394  accuracy:68.64738987155916\n",
      "Epoch55  loss:0.8707466594874859  accuracy:68.64600353775846\n",
      "Epoch56  loss:0.8454863607883452  accuracy:69.56693073720136\n",
      "Epoch57  loss:0.8505095764994621  accuracy:69.958528031974\n",
      "Epoch58  loss:0.8432757765054704  accuracy:70.2219999021112\n",
      "Epoch59  loss:0.8325217977166175  accuracy:70.15249075853797\n",
      "Epoch60  loss:0.8288373723626136  accuracy:70.33828406646975\n",
      "Epoch61  loss:0.8011063739657404  accuracy:71.74082820374595\n",
      "Epoch62  loss:0.8071644723415374  accuracy:70.88301643926064\n",
      "Epoch63  loss:0.8230092197656631  accuracy:70.79320734260692\n",
      "Epoch64  loss:0.7910404682159423  accuracy:71.81014585112443\n",
      "Epoch65  loss:0.7996499121189116  accuracy:71.9549023650527\n",
      "Epoch66  loss:0.7785878762602806  accuracy:71.37160053767332\n",
      "Epoch67  loss:0.7792977884411812  accuracy:72.13667628088821\n",
      "Epoch68  loss:0.7603835888206958  accuracy:72.79988915379136\n",
      "Epoch69  loss:0.7463805302977561  accuracy:73.44904023925206\n",
      "Epoch70  loss:0.7497185751795769  accuracy:73.55390441277133\n",
      "Epoch71  loss:0.7571855336427689  accuracy:72.68589879060822\n",
      "Epoch72  loss:0.7491521611809732  accuracy:73.7527342837985\n",
      "Epoch73  loss:0.7453572317957877  accuracy:73.42548242545263\n",
      "Epoch74  loss:0.7488994687795638  accuracy:73.97253784699491\n",
      "Epoch75  loss:0.7357545912265776  accuracy:74.4120594709717\n",
      "Epoch76  loss:0.7388257339596748  accuracy:74.13288848053838\n",
      "Epoch77  loss:0.7485148862004278  accuracy:73.91608130809769\n",
      "Epoch78  loss:0.7052828073501587  accuracy:74.82427751669645\n",
      "Epoch79  loss:0.7282497614622117  accuracy:74.16372773773638\n",
      "Epoch80  loss:0.7021591734141112  accuracy:74.94250023977692\n",
      "Epoch81  loss:0.7224019423127174  accuracy:74.47935282111095\n",
      "Epoch82  loss:0.7175906591117381  accuracy:75.20722505560649\n",
      "Epoch83  loss:0.6910641670227049  accuracy:75.786769000541\n",
      "Epoch84  loss:0.6925695717334747  accuracy:75.84079621150994\n",
      "Epoch85  loss:0.6812253266572951  accuracy:76.62338292065078\n",
      "Epoch86  loss:0.6867946967482567  accuracy:75.72271290633299\n",
      "Epoch87  loss:0.6879960812628269  accuracy:75.68012218821822\n",
      "Epoch88  loss:0.6774440065026284  accuracy:76.4305132588695\n",
      "Epoch89  loss:0.677121401950717  accuracy:76.68102811645602\n",
      "Epoch90  loss:0.6688609898090363  accuracy:76.50760150104402\n",
      "Epoch91  loss:0.6543381225317716  accuracy:76.89307021133693\n",
      "Epoch92  loss:0.6688965119421482  accuracy:77.12526084576352\n",
      "Epoch93  loss:0.6759895808994769  accuracy:76.22121866437159\n",
      "Epoch94  loss:0.6476593680679799  accuracy:77.57445105848583\n",
      "Epoch95  loss:0.659336470812559  accuracy:76.741529670291\n",
      "Epoch96  loss:0.6733267165720463  accuracy:76.42554232061738\n",
      "Epoch97  loss:0.6468629367649555  accuracy:77.25155673236421\n",
      "Epoch98  loss:0.6467636331915855  accuracy:77.59936296025118\n",
      "Epoch99  loss:0.6335815757513047  accuracy:78.35701142795328\n",
      "Epoch100  loss:0.635335648804903  accuracy:77.78312383317547\n",
      "Epoch101  loss:0.632503790408373  accuracy:78.30317189540723\n",
      "Epoch102  loss:0.6241167072206736  accuracy:78.3035499536401\n",
      "Epoch103  loss:0.6323654644191264  accuracy:77.99609129644739\n",
      "Epoch104  loss:0.6337873756885528  accuracy:77.78681000719428\n",
      "Epoch105  loss:0.6220455389469861  accuracy:78.25885042225451\n",
      "Epoch106  loss:0.6372707061469556  accuracy:77.77777501721037\n",
      "Epoch107  loss:0.6183500831946731  accuracy:78.1539880686701\n",
      "Epoch108  loss:0.6324149496853351  accuracy:78.2157508018587\n",
      "Epoch109  loss:0.6267736423760653  accuracy:78.61986950844035\n",
      "Epoch110  loss:0.6301426604390143  accuracy:78.84823953982826\n",
      "Epoch111  loss:0.6027276895940303  accuracy:79.26933765831342\n",
      "Epoch112  loss:0.6263961210846901  accuracy:78.7223026567042\n",
      "Epoch113  loss:0.6270267076790332  accuracy:78.50506338637629\n",
      "Epoch114  loss:0.6035001553595066  accuracy:79.83281606283698\n",
      "Epoch115  loss:0.6100814197212457  accuracy:79.5717817787252\n",
      "Epoch116  loss:0.5966733057051898  accuracy:79.44194649029993\n",
      "Epoch117  loss:0.5978476367890834  accuracy:79.53763957952954\n",
      "Epoch118  loss:0.6154465161263942  accuracy:79.3009447490982\n",
      "Epoch119  loss:0.5960601799190044  accuracy:79.92308930650083\n",
      "Epoch120  loss:0.6085219278931617  accuracy:80.10012402217546\n",
      "Epoch121  loss:0.5971760991960764  accuracy:79.72769660434098\n",
      "Epoch122  loss:0.5841808199882508  accuracy:80.26999640429543\n",
      "Epoch123  loss:0.5818223521113396  accuracy:80.39216681506608\n",
      "Epoch124  loss:0.5870676137506962  accuracy:80.26271067572496\n",
      "Epoch125  loss:0.5758049856871368  accuracy:80.55460830571448\n",
      "Epoch126  loss:0.5942254178225993  accuracy:80.11681905846474\n",
      "Epoch127  loss:0.6035007543861866  accuracy:80.25496127521234\n",
      "Epoch128  loss:0.599033784121275  accuracy:80.22578822397088\n",
      "Epoch129  loss:0.594306655228138  accuracy:80.59367400285244\n",
      "Epoch130  loss:0.5705906418152153  accuracy:80.73481656692785\n",
      "Epoch131  loss:0.5755723625421525  accuracy:80.91951964879092\n",
      "Epoch132  loss:0.5776149719953536  accuracy:80.62402178168503\n",
      "Epoch133  loss:0.5815889716148376  accuracy:81.00986910621494\n",
      "Epoch134  loss:0.572699647396803  accuracy:80.90756934695878\n",
      "Epoch135  loss:0.5796619154512882  accuracy:80.97683196292964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch136  loss:0.5884734809398651  accuracy:80.8221556450566\n",
      "Epoch137  loss:0.5802225477993488  accuracy:81.28359682161269\n",
      "Epoch138  loss:0.5939632698893548  accuracy:81.04660060860384\n",
      "Epoch139  loss:0.57257479429245  accuracy:81.29449173175713\n",
      "Epoch140  loss:0.5748556330800056  accuracy:81.65947847713939\n",
      "Epoch141  loss:0.5752149093896151  accuracy:81.35573035274916\n",
      "Epoch142  loss:0.5764927595853805  accuracy:81.09862975580285\n",
      "Epoch143  loss:0.5902578480541705  accuracy:81.60805280672724\n",
      "Epoch144  loss:0.5694693230092527  accuracy:81.7576216642319\n",
      "Epoch145  loss:0.5833124287426471  accuracy:81.41436373471387\n",
      "Epoch146  loss:0.5920983657240868  accuracy:81.00927216084447\n",
      "Epoch147  loss:0.5535596281290055  accuracy:82.32819708076849\n",
      "Epoch148  loss:0.5478063613176346  accuracy:82.44941939937583\n",
      "Epoch149  loss:0.5824028037488461  accuracy:81.55604973674795\n",
      "Epoch150  loss:0.5827917911112309  accuracy:81.78315465292971\n",
      "Epoch151  loss:0.5813098527491092  accuracy:81.93508105701986\n",
      "Epoch152  loss:0.5639363847672939  accuracy:82.24009391694581\n",
      "Epoch153  loss:0.5721117809414864  accuracy:82.1594398252255\n",
      "Epoch154  loss:0.5666230887174606  accuracy:82.02902891084356\n",
      "Epoch155  loss:0.5646013744175433  accuracy:82.14814245640854\n",
      "Epoch156  loss:0.5490583676844836  accuracy:82.7950283633508\n",
      "Epoch157  loss:0.5543177139014006  accuracy:82.91745070160867\n",
      "Epoch158  loss:0.574947671405971  accuracy:82.22686006077578\n",
      "Epoch159  loss:0.5728457510471344  accuracy:82.24047730360228\n",
      "Epoch160  loss:0.5661564900539815  accuracy:82.47522412465462\n",
      "Epoch161  loss:0.5826321944594384  accuracy:82.11983803366148\n",
      "Epoch162  loss:0.5514710076153279  accuracy:82.78551597925609\n",
      "Epoch163  loss:0.5751136161386967  accuracy:82.43357856767983\n",
      "Epoch164  loss:0.5671443855389953  accuracy:82.38340900255139\n",
      "Epoch165  loss:0.5659359071403741  accuracy:82.70558260379767\n",
      "Epoch166  loss:0.5560904294252396  accuracy:83.22939258932404\n",
      "Epoch167  loss:0.5584387525916099  accuracy:83.08631511204936\n",
      "Epoch168  loss:0.5543573361355811  accuracy:83.00111999919619\n",
      "Epoch169  loss:0.5567182213068009  accuracy:82.8117908194387\n",
      "Epoch170  loss:0.5524001874029636  accuracy:83.57313579256751\n",
      "Epoch171  loss:0.5580674692988395  accuracy:83.24131321947297\n",
      "Epoch172  loss:0.5548435281962156  accuracy:82.69410736859163\n",
      "Epoch173  loss:0.5494138673879205  accuracy:83.07205978359978\n",
      "Epoch174  loss:0.5649152390658854  accuracy:82.78900700401599\n",
      "Epoch175  loss:0.5623835477977991  accuracy:83.21340699443815\n",
      "Epoch176  loss:0.5792432817630471  accuracy:83.074163915283\n",
      "Epoch177  loss:0.5517989914864301  accuracy:82.89749641290527\n",
      "Epoch178  loss:0.5757847249507904  accuracy:83.21855643705852\n",
      "Epoch179  loss:0.55646015368402  accuracy:83.28799863964004\n",
      "Epoch180  loss:0.5602834455668927  accuracy:83.2519438781303\n",
      "Epoch181  loss:0.5679159156978132  accuracy:83.67466910967921\n",
      "Epoch182  loss:0.5551910482347011  accuracy:83.54255375907759\n",
      "Epoch183  loss:0.5605944506824015  accuracy:83.4124711009178\n",
      "Epoch184  loss:0.5864786513149739  accuracy:83.01091662180743\n",
      "Epoch185  loss:0.5522226832807063  accuracy:83.59429862899061\n",
      "Epoch186  loss:0.5627556414343416  accuracy:83.2341650377009\n",
      "Epoch187  loss:0.5744484858587385  accuracy:83.23036977430974\n",
      "Epoch188  loss:0.5854091666638851  accuracy:83.1814994115178\n",
      "Epoch189  loss:0.5765961047261954  accuracy:83.18211995552714\n",
      "Epoch190  loss:0.5489788614213468  accuracy:83.8783958151983\n",
      "Epoch191  loss:0.5624866100028157  accuracy:83.50259318998215\n",
      "Epoch192  loss:0.5523229232057929  accuracy:84.00583614546807\n",
      "Epoch193  loss:0.5719613471068442  accuracy:83.41944721053855\n",
      "Epoch194  loss:0.5752590354532003  accuracy:83.73499659497223\n",
      "Epoch195  loss:0.5452051132917405  accuracy:84.12180639824808\n",
      "Epoch196  loss:0.5697901678271592  accuracy:83.39991779340527\n",
      "Epoch197  loss:0.5634408447891474  accuracy:83.82470889091955\n",
      "Epoch198  loss:0.5771433930844069  accuracy:83.96620704754105\n",
      "Epoch199  loss:0.564987576752901  accuracy:83.90461023312275\n",
      "Epoch200  loss:0.5901511408388613  accuracy:83.92842399780557\n",
      "Epoch201  loss:0.5882502131164074  accuracy:83.88474737677359\n",
      "Epoch202  loss:0.5755097413901239  accuracy:84.15526174381291\n",
      "Epoch203  loss:0.5749717903323471  accuracy:84.20138760390392\n",
      "Epoch204  loss:0.5810792703181504  accuracy:83.89041155503527\n",
      "Epoch205  loss:0.5919213585555553  accuracy:84.12319388248454\n",
      "Epoch206  loss:0.5740902447374537  accuracy:83.91432200638467\n",
      "Epoch207  loss:0.5866691195173189  accuracy:84.23182301527031\n",
      "Epoch208  loss:0.5797915866598486  accuracy:83.72177181189042\n",
      "Epoch209  loss:0.5741335428552702  accuracy:84.35449888392885\n",
      "Epoch210  loss:0.6020941132679581  accuracy:84.05798375808143\n",
      "Epoch211  loss:0.5809352903277614  accuracy:83.67390666594763\n",
      "Epoch212  loss:0.6046566002070903  accuracy:83.549922494254\n",
      "Epoch213  loss:0.5983633625146467  accuracy:83.95085784749661\n",
      "Epoch214  loss:0.5934464000165462  accuracy:84.03381257785577\n",
      "Epoch215  loss:0.5747518900898286  accuracy:83.90866529520977\n",
      "Epoch216  loss:0.5934471204876899  accuracy:83.71174425703576\n",
      "Epoch217  loss:0.5819604341988451  accuracy:84.44095739201103\n",
      "Epoch218  loss:0.5871259979903698  accuracy:84.18698826566197\n",
      "Epoch219  loss:0.6123525594943204  accuracy:83.87996765999449\n",
      "Epoch220  loss:0.62352991791995  accuracy:83.83369772171697\n",
      "Epoch221  loss:0.5943264985689894  accuracy:84.49997581514914\n",
      "Epoch222  loss:0.5972547846613454  accuracy:83.93859627242284\n",
      "Epoch223  loss:0.5820499722216482  accuracy:84.34449881619967\n",
      "Epoch224  loss:0.582254739291966  accuracy:84.83616917102427\n",
      "Epoch225  loss:0.5945159654133021  accuracy:83.97585369089381\n",
      "Epoch226  loss:0.5939204162918033  accuracy:84.50909980682118\n",
      "Epoch227  loss:0.5995401402033166  accuracy:84.54660248053837\n",
      "Epoch228  loss:0.5962937831995078  accuracy:83.96668561019841\n",
      "Epoch229  loss:0.5895627108591727  accuracy:84.61312764522071\n",
      "Epoch230  loss:0.6351181796053423  accuracy:83.90409821199036\n",
      "Epoch231  loss:0.6200601140968501  accuracy:84.42494891122497\n",
      "Epoch232  loss:0.6063760505028767  accuracy:83.96529763903432\n",
      "Epoch233  loss:0.600910687353462  accuracy:84.49686149417424\n",
      "Epoch234  loss:0.6245441745966673  accuracy:84.20530156113534\n",
      "Epoch235  loss:0.6214481875300408  accuracy:84.47419427140618\n",
      "Epoch236  loss:0.6162694576909417  accuracy:84.05358252061173\n",
      "Epoch237  loss:0.5951650622300804  accuracy:84.92912868380095\n",
      "Epoch238  loss:0.6240363608638291  accuracy:84.46188901211237\n",
      "Epoch239  loss:0.6057631504372694  accuracy:84.5004611295414\n",
      "Epoch240  loss:0.616980137315113  accuracy:84.33856482939554\n",
      "Epoch241  loss:0.6034773295530614  accuracy:84.6549752273506\n",
      "Epoch242  loss:0.6124764843378216  accuracy:84.25178420140394\n",
      "Epoch243  loss:0.6228309930418617  accuracy:84.65414338397238\n",
      "Epoch244  loss:0.6510202633602604  accuracy:84.27656185918183\n",
      "Epoch245  loss:0.6480604718439281  accuracy:83.99915953290505\n",
      "Epoch246  loss:0.6080616995081073  accuracy:84.60858436647163\n",
      "Epoch247  loss:0.6450440439420164  accuracy:84.30788301823476\n",
      "Epoch248  loss:0.6344378687441349  accuracy:84.23798649346784\n",
      "Epoch249  loss:0.6594718450680376  accuracy:84.61653278025638\n",
      "Epoch250  loss:0.6378745560708922  accuracy:84.65004315412793\n",
      "Epoch251  loss:0.6296036255798755  accuracy:85.02998685476133\n",
      "Epoch252  loss:0.6133535244502127  accuracy:84.8388754423905\n",
      "Epoch253  loss:0.6512860362417996  accuracy:84.87983986996495\n",
      "Epoch254  loss:0.6524236714933068  accuracy:84.21016779315599\n",
      "Epoch255  loss:0.6241428386827464  accuracy:84.73995858659816\n",
      "Epoch256  loss:0.6456705942211556  accuracy:84.32597586442083\n",
      "Epoch257  loss:0.663744209514698  accuracy:84.67275222951665\n",
      "Epoch258  loss:0.6611078299640213  accuracy:84.61424779870032\n",
      "Epoch259  loss:0.6327109382793423  accuracy:85.09404392294061\n",
      "Epoch260  loss:0.6520079952315426  accuracy:85.24691904920662\n",
      "Epoch261  loss:0.6573337845145942  accuracy:84.45650084592194\n",
      "Epoch262  loss:0.6486802376728976  accuracy:84.82207386781616\n",
      "Epoch263  loss:0.6471660934379768  accuracy:84.72863071676444\n",
      "Epoch264  loss:0.645760613258608  accuracy:84.96859360727446\n",
      "Epoch265  loss:0.6801742343608567  accuracy:84.32062366736398\n",
      "Epoch266  loss:0.6543247242998406  accuracy:84.56299637791498\n",
      "Epoch267  loss:0.6469353229680566  accuracy:84.98064012510825\n",
      "Epoch268  loss:0.6527620561173536  accuracy:84.72803248254566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch269  loss:0.6419994605847023  accuracy:84.97905814564194\n",
      "Epoch270  loss:0.6369643399710185  accuracy:85.20189428973693\n",
      "Epoch271  loss:0.6583644454090972  accuracy:84.52416643091286\n",
      "Epoch272  loss:0.6684408286004327  accuracy:85.24197084215574\n",
      "Epoch273  loss:0.65898678660742  accuracy:84.75865701562776\n",
      "Epoch274  loss:0.6694331582995801  accuracy:84.9289419447007\n",
      "Epoch275  loss:0.6713259525597095  accuracy:84.59148512321195\n",
      "Epoch276  loss:0.6828881023022405  accuracy:84.40438144196769\n",
      "Epoch277  loss:0.6756092472023738  accuracy:84.82097797766929\n",
      "Epoch278  loss:0.7032632730867363  accuracy:84.5841534784511\n",
      "Epoch279  loss:0.6692429616039589  accuracy:85.50000389235609\n",
      "Epoch280  loss:0.6643249662862218  accuracy:85.50739795117644\n",
      "Epoch281  loss:0.6725849533624569  accuracy:84.55357820686189\n",
      "Epoch282  loss:0.6942603247938677  accuracy:85.37015746866587\n",
      "Epoch283  loss:0.7113106732431335  accuracy:84.34313373310056\n",
      "Epoch284  loss:0.6919634216355918  accuracy:84.70892344887721\n",
      "Epoch285  loss:0.6783868204627651  accuracy:84.9048614774852\n",
      "Epoch286  loss:0.658725821970188  accuracy:85.26519208065292\n",
      "Epoch287  loss:0.6981677799150928  accuracy:84.91042552741764\n",
      "Epoch288  loss:0.6957639299258745  accuracy:84.48375264530668\n",
      "Epoch289  loss:0.6826825328060748  accuracy:84.85524349118528\n",
      "Epoch290  loss:0.7137956079226568  accuracy:84.9109890910134\n",
      "Epoch291  loss:0.6861169791409337  accuracy:84.92054583651182\n",
      "Epoch292  loss:0.6839144617109014  accuracy:85.1366295863544\n",
      "Epoch293  loss:0.6819099507983082  accuracy:85.63298714943721\n",
      "Epoch294  loss:0.6926175041361603  accuracy:84.99691837832461\n",
      "Epoch295  loss:0.7218488255592094  accuracy:85.10209443957065\n",
      "Epoch296  loss:0.6917878561274847  accuracy:84.93767300926805\n",
      "Epoch297  loss:0.6818148821570503  accuracy:85.19961664716087\n",
      "Epoch298  loss:0.7139641477819167  accuracy:84.59016257580831\n",
      "Epoch299  loss:0.685516547167208  accuracy:85.00871283962086\n",
      "Epoch300  loss:0.7104421144351364  accuracy:84.87446134102446\n",
      "Epoch301  loss:0.7259641424752772  accuracy:84.79431181158355\n",
      "Epoch302  loss:0.696637361471221  accuracy:85.17404903494786\n",
      "Epoch303  loss:0.6961961997127218  accuracy:85.44171793213214\n",
      "Epoch304  loss:0.7109325334421102  accuracy:85.4013553143387\n",
      "Epoch305  loss:0.7064825440058484  accuracy:84.99956633177167\n",
      "Epoch306  loss:0.7104453748092056  accuracy:85.0337830456553\n",
      "Epoch307  loss:0.7126637876029916  accuracy:85.22485410896485\n",
      "Epoch308  loss:0.7060222697342398  accuracy:85.36078236995861\n",
      "Epoch309  loss:0.7407481141388359  accuracy:85.23213677962646\n",
      "Epoch310  loss:0.7532998293383116  accuracy:84.54502016965466\n",
      "Epoch311  loss:0.7158806895138695  accuracy:84.88942248146847\n",
      "Epoch312  loss:0.7089509130608349  accuracy:84.9129850355967\n",
      "Epoch313  loss:0.7028361596167085  accuracy:85.34704510169115\n",
      "Epoch314  loss:0.7246035226737149  accuracy:85.33063364489365\n",
      "Epoch315  loss:0.7077078699929643  accuracy:85.51646467524613\n",
      "Epoch316  loss:0.7474092692136083  accuracy:84.78052758904599\n",
      "Epoch317  loss:0.7284452952443983  accuracy:85.41435411622759\n",
      "Epoch318  loss:0.717356412112703  accuracy:85.70325057939571\n",
      "Epoch319  loss:0.7208953425252731  accuracy:85.21929501400689\n",
      "Epoch320  loss:0.7354112613945689  accuracy:85.6601649202068\n",
      "Epoch321  loss:0.7115523737633339  accuracy:85.5077529054688\n",
      "Epoch322  loss:0.7149161658863704  accuracy:85.28388691898674\n",
      "Epoch323  loss:0.7175083285188519  accuracy:85.36534302135036\n",
      "Epoch324  loss:0.7454696625307407  accuracy:85.29704990227778\n",
      "Epoch325  loss:0.741183590493165  accuracy:84.971576277066\n",
      "Epoch326  loss:0.7095416778407525  accuracy:85.6804198349799\n",
      "Epoch327  loss:0.723264043472591  accuracy:85.59106135924438\n",
      "Epoch328  loss:0.7101405993103977  accuracy:85.54859814731512\n",
      "Epoch329  loss:0.7444911330854893  accuracy:85.581023185344\n",
      "Epoch330  loss:0.6990021484496536  accuracy:85.12855564067969\n",
      "Epoch331  loss:0.7054102100431908  accuracy:85.77527733858494\n",
      "Epoch332  loss:0.7239680274757119  accuracy:85.25901029150603\n",
      "Epoch333  loss:0.7544668756424499  accuracy:85.32699397639779\n",
      "Epoch334  loss:0.7612671777569177  accuracy:85.2235799467964\n",
      "Epoch335  loss:0.7165682680904409  accuracy:85.43817740972567\n",
      "Epoch336  loss:0.7470293179148939  accuracy:85.3044308766831\n",
      "Epoch337  loss:0.7425104597941754  accuracy:85.63916172741172\n",
      "Epoch338  loss:0.7466407164931111  accuracy:85.56130672508932\n",
      "Epoch339  loss:0.7549953370285949  accuracy:85.2912936908454\n",
      "Epoch340  loss:0.765833097696304  accuracy:85.26575915059105\n",
      "Epoch341  loss:0.8011757930185922  accuracy:85.02958537111435\n",
      "Epoch342  loss:0.7631273575126669  accuracy:85.28100283163646\n",
      "Epoch343  loss:0.7532027959470724  accuracy:85.18485892046161\n",
      "Epoch344  loss:0.7573778159911398  accuracy:85.31235867877923\n",
      "Epoch345  loss:0.7354930453002452  accuracy:85.88244887387654\n",
      "Epoch346  loss:0.7461554719899139  accuracy:85.53587856346766\n",
      "Epoch347  loss:0.7385737792945293  accuracy:85.61436372103051\n",
      "Epoch348  loss:0.7831217776765698  accuracy:84.93865111140312\n",
      "Epoch349  loss:0.772547774016833  accuracy:85.35716628751538\n",
      "Epoch350  loss:0.7701234102217086  accuracy:85.29155856986712\n",
      "Epoch351  loss:0.7615817762907225  accuracy:85.12624833521794\n",
      "Epoch352  loss:0.7603638600558041  accuracy:85.41578505790439\n",
      "Epoch353  loss:0.7529205070393801  accuracy:85.66515518740435\n",
      "Epoch354  loss:0.7387096188921077  accuracy:86.08750244902652\n",
      "Epoch355  loss:0.7817782945930957  accuracy:85.02080466991184\n",
      "Epoch356  loss:0.7569378457960284  accuracy:84.84794520710707\n",
      "Epoch357  loss:0.7516392962192185  accuracy:85.7372951247653\n",
      "Epoch358  loss:0.775670638214797  accuracy:85.25131574898072\n",
      "Epoch359  loss:0.8061570598672916  accuracy:85.37961654311337\n",
      "Epoch360  loss:0.755971759557724  accuracy:85.80190286953032\n",
      "Epoch361  loss:0.7922469876008108  accuracy:85.17508412696493\n",
      "Epoch362  loss:0.7984842769801603  accuracy:85.01281773631703\n",
      "Epoch363  loss:0.7475625649046832  accuracy:85.53089236204984\n",
      "Epoch364  loss:0.7627222716807992  accuracy:85.26659598445077\n",
      "Epoch365  loss:0.7743543587624955  accuracy:85.62857644665532\n",
      "Epoch366  loss:0.7626903682947079  accuracy:85.92177189377092\n",
      "Epoch367  loss:0.7756162069737516  accuracy:85.00656555456983\n",
      "Epoch368  loss:0.7591857828199777  accuracy:85.73430027189069\n",
      "Epoch369  loss:0.7812588175482233  accuracy:85.8248039392172\n",
      "Epoch370  loss:0.7783969908942368  accuracy:85.26913099345256\n",
      "Epoch371  loss:0.7959340467679795  accuracy:85.50031439841446\n",
      "Epoch372  loss:0.7655417360365262  accuracy:85.28770460240949\n",
      "Epoch373  loss:0.7772434838116168  accuracy:85.52994692020359\n",
      "Epoch374  loss:0.767198756337165  accuracy:85.85930149836855\n",
      "Epoch375  loss:0.7672108106306609  accuracy:85.48400546350243\n",
      "Epoch376  loss:0.7885149955734049  accuracy:85.99218244127917\n",
      "Epoch377  loss:0.8090214357296645  accuracy:85.2696751530132\n",
      "Epoch378  loss:0.7694989635143428  accuracy:85.64370236083168\n",
      "Epoch379  loss:0.7940539874136446  accuracy:85.36318722760824\n",
      "Epoch380  loss:0.8030946843326091  accuracy:85.8582882306878\n",
      "Epoch381  loss:0.8344427600502968  accuracy:85.45272641442372\n",
      "Epoch382  loss:0.7929787404834684  accuracy:85.35751793000621\n",
      "Epoch383  loss:0.794172043352569  accuracy:85.91093163731936\n",
      "Epoch384  loss:0.8107939302921295  accuracy:85.5384215628843\n",
      "Epoch385  loss:0.8021495519154995  accuracy:85.54439140657924\n",
      "Epoch386  loss:0.8215270973742002  accuracy:85.64652394537461\n",
      "Epoch387  loss:0.7862871110439102  accuracy:85.68861342469974\n",
      "Epoch388  loss:0.8091229210051099  accuracy:85.68443228458847\n",
      "Epoch389  loss:0.8063418753440773  accuracy:85.9671525687097\n",
      "Epoch390  loss:0.8104471064172686  accuracy:85.80779640599762\n",
      "Epoch391  loss:0.8118119255246711  accuracy:85.20146268908968\n",
      "Epoch392  loss:0.834644139558054  accuracy:84.69459457014203\n",
      "Epoch393  loss:0.7958357855677605  accuracy:85.80510497516755\n",
      "Epoch394  loss:0.836884769797325  accuracy:85.49248981739989\n",
      "Epoch395  loss:0.7989364318547132  accuracy:86.36124106670376\n",
      "Epoch396  loss:0.8117958314239333  accuracy:85.45010935813642\n",
      "Epoch397  loss:0.8219538472546901  accuracy:85.7655765985271\n",
      "Epoch398  loss:0.7741241216659525  accuracy:85.95514042135855\n",
      "Epoch399  loss:0.8118243098258964  accuracy:85.24396362352813\n",
      "Epoch400  loss:0.8354822580271504  accuracy:85.16631588516549\n",
      "Epoch401  loss:0.8171523086698699  accuracy:85.3789275499503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch402  loss:0.8552299036945441  accuracy:85.46145394395712\n",
      "Epoch403  loss:0.8037350952625191  accuracy:85.97718827448114\n",
      "Epoch404  loss:0.8044758421368895  accuracy:86.0054022269916\n",
      "Epoch405  loss:0.8186604149581627  accuracy:85.611803109412\n",
      "Epoch406  loss:0.7876132287083522  accuracy:86.1439663314548\n",
      "Epoch407  loss:0.8064191773533818  accuracy:85.6012057890666\n",
      "Epoch408  loss:0.8091803960502149  accuracy:85.61897089033745\n",
      "Epoch409  loss:0.8364233888685698  accuracy:85.83109860616372\n",
      "Epoch410  loss:0.7847022613917033  accuracy:86.21406183371408\n",
      "Epoch411  loss:0.809412495791912  accuracy:85.87296515600352\n",
      "Epoch412  loss:0.8159265622496593  accuracy:85.29140713119358\n",
      "Epoch413  loss:0.8037915830529528  accuracy:85.89519970339435\n",
      "Epoch414  loss:0.8766627855598927  accuracy:85.01212611309859\n",
      "Epoch415  loss:0.816124400496466  accuracy:85.87566418460936\n",
      "Epoch416  loss:0.8644996166226692  accuracy:85.7750042476255\n",
      "Epoch417  loss:0.8172097727656336  accuracy:85.78374222306411\n",
      "Epoch418  loss:0.8321036383509637  accuracy:85.70765971564971\n",
      "Epoch419  loss:0.8591166138648987  accuracy:85.84996966964513\n",
      "Epoch420  loss:0.8358819909382759  accuracy:85.93807331967484\n",
      "Epoch421  loss:0.8383786670863623  accuracy:85.95816480266288\n",
      "Epoch422  loss:0.8789571687578246  accuracy:85.21466575994373\n",
      "Epoch423  loss:0.8363291501998902  accuracy:85.93054180387514\n",
      "Epoch424  loss:0.8088130213320255  accuracy:86.07944041033116\n",
      "Epoch425  loss:0.8239358332008122  accuracy:85.64122913905828\n",
      "Epoch426  loss:0.8356806166467949  accuracy:85.48270680748844\n",
      "Epoch427  loss:0.8099726445972011  accuracy:86.00478154142023\n",
      "Epoch428  loss:0.8055036112611333  accuracy:86.10611932390465\n",
      "Epoch429  loss:0.8286591120063519  accuracy:85.99250844300343\n",
      "Epoch430  loss:0.8113151513040044  accuracy:86.32293818084794\n",
      "Epoch431  loss:0.8106599152088165  accuracy:85.77731048290919\n",
      "Epoch432  loss:0.8140178925951659  accuracy:85.70683511706692\n",
      "Epoch433  loss:0.8343375161289855  accuracy:85.79179281917739\n",
      "Epoch434  loss:0.8496753677725792  accuracy:86.0768113700484\n",
      "Epoch435  loss:0.8392246365547177  accuracy:85.73231914881065\n",
      "Epoch436  loss:0.8326895028352737  accuracy:86.14201458008411\n",
      "Epoch437  loss:0.8274420037864958  accuracy:85.8118242376913\n",
      "Epoch438  loss:0.812764684855938  accuracy:85.94204628949637\n",
      "Epoch439  loss:0.8496520824715275  accuracy:85.86302098612025\n",
      "Epoch440  loss:0.8143694914872297  accuracy:85.81464750409195\n",
      "Epoch441  loss:0.8408885166893016  accuracy:86.07157230739409\n",
      "Epoch442  loss:0.8543959415168502  accuracy:85.66497500754886\n",
      "Epoch443  loss:0.8377296179533005  accuracy:85.85942548369968\n",
      "Epoch444  loss:0.8391062568945926  accuracy:85.93315633928646\n",
      "Epoch445  loss:0.8326617456972598  accuracy:86.27382804541284\n",
      "Epoch446  loss:0.8434826470911495  accuracy:86.01100458456011\n",
      "Epoch447  loss:0.8561057984828947  accuracy:85.87050327019556\n",
      "Epoch448  loss:0.8464022717750593  accuracy:86.21443429282797\n",
      "Epoch449  loss:0.8858275943901388  accuracy:85.83602741345983\n",
      "Epoch450  loss:0.9050622798372389  accuracy:85.29313945031772\n",
      "Epoch451  loss:0.8528301462527339  accuracy:86.11286522753193\n",
      "Epoch452  loss:0.8512812696392758  accuracy:85.87342175743406\n",
      "Epoch453  loss:0.8493155002593965  accuracy:86.61707739155631\n",
      "Epoch454  loss:0.8508184522390367  accuracy:85.662222217075\n",
      "Epoch455  loss:0.8463586941360914  accuracy:86.2527629005825\n",
      "Epoch456  loss:0.8619043298065179  accuracy:85.74488415765579\n",
      "Epoch457  loss:0.8356086223433522  accuracy:86.24697285324702\n",
      "Epoch458  loss:0.8316594593226909  accuracy:86.14062916413754\n",
      "Epoch459  loss:0.8448408253490922  accuracy:86.37012160172115\n",
      "Epoch460  loss:0.8541885446211381  accuracy:85.84656935710522\n",
      "Epoch461  loss:0.8757998979430338  accuracy:85.51054752140972\n",
      "Epoch462  loss:0.8691133849282549  accuracy:86.07616329939678\n",
      "Epoch463  loss:0.8231771171092985  accuracy:86.48960696470348\n",
      "Epoch464  loss:0.8766898311674595  accuracy:85.7648247870571\n",
      "Epoch465  loss:0.819133350253105  accuracy:86.47761017344617\n",
      "Epoch466  loss:0.8442233055826875  accuracy:86.50197887452202\n",
      "Epoch467  loss:0.8644239827990529  accuracy:85.97894375653998\n",
      "Epoch468  loss:0.8554068088531492  accuracy:86.06895715151182\n",
      "Epoch469  loss:0.8341099917888642  accuracy:86.42774321844257\n",
      "Epoch470  loss:0.8926593303680419  accuracy:85.29106537154861\n",
      "Epoch471  loss:0.8196447163820266  accuracy:86.3976504227981\n",
      "Epoch472  loss:0.8763087660074234  accuracy:85.96116786741368\n",
      "Epoch473  loss:0.8197933460585775  accuracy:86.09800850259434\n",
      "Epoch474  loss:0.8719813399016854  accuracy:85.63735417614002\n",
      "Epoch475  loss:0.8369488358497619  accuracy:86.32198048237264\n",
      "Epoch476  loss:0.848932075493235  accuracy:86.16611719305399\n",
      "Epoch477  loss:0.8758984155952928  accuracy:86.42992418373066\n",
      "Epoch478  loss:0.8746924884617323  accuracy:85.86493153987294\n",
      "Epoch479  loss:0.8737155474582322  accuracy:86.00938011091466\n",
      "Epoch480  loss:0.8588017001227853  accuracy:86.15460672304724\n",
      "Epoch481  loss:0.8495430082082746  accuracy:86.24057628501164\n",
      "Epoch482  loss:0.892555083334446  accuracy:85.86293564526736\n",
      "Epoch483  loss:0.8962962858376784  accuracy:86.17410636059627\n",
      "Epoch484  loss:0.871628576517105  accuracy:86.39376472461547\n",
      "Epoch485  loss:0.8761762127279723  accuracy:86.16505206438477\n",
      "Epoch486  loss:0.8913625605395623  accuracy:86.04581251067115\n",
      "Epoch487  loss:0.8923633649945258  accuracy:85.7980610232875\n",
      "Epoch488  loss:0.8750564597547054  accuracy:85.8755476416915\n",
      "Epoch489  loss:0.8930775545534004  accuracy:86.16058353377367\n",
      "Epoch490  loss:0.8564264118671416  accuracy:86.1573533612195\n",
      "Epoch491  loss:0.8747235588729383  accuracy:85.81635413845095\n",
      "Epoch492  loss:0.9043686151488828  accuracy:85.7481039665864\n",
      "Epoch493  loss:0.8519027039408684  accuracy:86.07755290932496\n",
      "Epoch494  loss:0.8503962064132793  accuracy:86.13689250141704\n",
      "Epoch495  loss:0.8755936784416917  accuracy:85.98975561104157\n",
      "Epoch496  loss:0.850172074582224  accuracy:86.11593673314158\n",
      "Epoch497  loss:0.8736037224531175  accuracy:86.1223382434467\n",
      "Epoch498  loss:0.8498936027288437  accuracy:86.61118360094237\n",
      "Epoch499  loss:0.8836146034300326  accuracy:86.3225223808735\n",
      "Epoch500  loss:0.8584974020719527  accuracy:86.19563439871295\n"
     ]
    }
   ],
   "source": [
    "server = Server()\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset)\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "acc_valid = []\n",
    "loss_valid = []\n",
    "\n",
    "early_stopping = Early_Stopping(args.partience)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(args.global_epochs):\n",
    "  sample_worker = server.sample_worker(workers)\n",
    "  server.send_models(sample_worker)\n",
    "\n",
    "  acc_train_avg = 0.0\n",
    "  loss_train_avg = 0.0\n",
    "  acc_valid_avg = 0.0\n",
    "  loss_valid_avg = 0.0\n",
    "  for worker in sample_worker:\n",
    "    worker.clustering(worker.models)\n",
    "    acc_train_tmp,loss_train_tmp,acc_valid_tmp,loss_valid_tmp = worker.local_train()\n",
    "    acc_train_avg += acc_train_tmp/len(sample_worker)\n",
    "    loss_train_avg += loss_train_tmp/len(sample_worker)\n",
    "    acc_valid_avg += acc_valid_tmp/len(sample_worker)\n",
    "    loss_valid_avg += loss_valid_tmp/len(sample_worker)\n",
    "  server.aggregate_models(sample_worker)\n",
    "  '''\n",
    "  for i in range(args.cluster_num):\n",
    "    server.models[i].to(args.device)\n",
    "  for worker in workers:\n",
    "    worker.clustering(server.models)\n",
    "    acc_valid_tmp,loss_valid_tmp = test(server.models[worker.cluster],args.criterion,worker.valloader)\n",
    "    acc_valid_avg += acc_valid_tmp/len(workers)\n",
    "    loss_valid_avg += loss_valid_tmp/len(workers)\n",
    "  for i in range(args.cluster_num):\n",
    "    server.models[i].to('cpu')\n",
    "  '''\n",
    "  print('Epoch{}  loss:{}  accuracy:{}'.format(epoch+1,loss_valid_avg,acc_valid_avg))\n",
    "  acc_train.append(acc_train_avg)\n",
    "  loss_train.append(loss_train_avg)\n",
    "  acc_valid.append(acc_valid_avg)\n",
    "  loss_valid.append(loss_valid_avg)\n",
    "\n",
    "  if early_stopping.validate(loss_valid_avg):\n",
    "    print('Early Stop')\n",
    "    break\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "mi_uceyoptLP",
    "outputId": "bc067e09-01bc-4e65-daf9-ac2f42373cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 accuracy:76.036866359447  loss:1.3084914684295654\n",
      "Worker2 accuracy:79.71311475409836  loss:1.4983618259429932\n",
      "Worker3 accuracy:82.28571428571429  loss:1.0096631050109863\n",
      "Worker4 accuracy:82.14285714285714  loss:0.9509595632553101\n",
      "Worker5 accuracy:85.07317073170732  loss:0.6511935293674469\n",
      "Worker6 accuracy:77.15877437325905  loss:1.5021748542785645\n",
      "Worker7 accuracy:77.47368421052632  loss:1.241562843322754\n",
      "Worker8 accuracy:87.9415347137637  loss:0.6782227754592896\n",
      "Worker9 accuracy:89.04458598726114  loss:0.7916321754455566\n",
      "Worker10 accuracy:79.76190476190476  loss:0.9706471562385559\n",
      "Worker11 accuracy:77.77777777777777  loss:1.1192662715911865\n",
      "Worker12 accuracy:74.96402877697842  loss:1.7317689657211304\n",
      "Worker13 accuracy:90.0  loss:0.5182749032974243\n",
      "Worker14 accuracy:78.96551724137932  loss:1.2932301759719849\n",
      "Worker15 accuracy:78.43137254901961  loss:1.4323174953460693\n",
      "Worker16 accuracy:79.91266375545851  loss:1.2022693157196045\n",
      "Worker17 accuracy:75.23584905660377  loss:2.115704298019409\n",
      "Worker18 accuracy:78.42778793418647  loss:1.122031331062317\n",
      "Worker19 accuracy:75.28089887640449  loss:1.3406150341033936\n",
      "Worker20 accuracy:74.18546365914787  loss:1.4880493879318237\n",
      "Test  loss:1.1983218237757682  accuracy:79.99067834737477\n"
     ]
    }
   ],
   "source": [
    "acc_test = []\n",
    "loss_test = []\n",
    "\n",
    "for i in range(args.cluster_num):\n",
    "  server.models[i].to(args.device)\n",
    "\n",
    "nums = 0\n",
    "for worker in workers:\n",
    "  nums += worker.test_data_num\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.aggregation_weight = 1.0*worker.test_data_num/nums\n",
    "  worker.clustering(server.models)\n",
    "  acc_tmp,loss_tmp = test(server.models[worker.cluster],args.criterion,worker.testloader)\n",
    "  acc_test.append(acc_tmp)\n",
    "  loss_test.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_test_avg = sum(acc_test)/len(acc_test)\n",
    "loss_test_avg = sum(loss_test)/len(loss_test)\n",
    "print('Test  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 Valid accuracy:83.15132605304213  loss:1.4889482259750366\n",
      "Worker1 Test accuracy:84.17818740399386  loss:1.2197999954223633\n",
      "Worker2 Valid accuracy:86.92946058091286  loss:1.005157232284546\n",
      "Worker2 Test accuracy:87.90983606557377  loss:0.9609335064888\n",
      "Worker3 Valid accuracy:83.203125  loss:0.9617648720741272\n",
      "Worker3 Test accuracy:85.14285714285714  loss:0.9634528756141663\n",
      "Worker4 Valid accuracy:89.0909090909091  loss:0.6090174317359924\n",
      "Worker4 Test accuracy:88.88888888888889  loss:0.6335127949714661\n",
      "Worker5 Valid accuracy:84.9601593625498  loss:0.5409492254257202\n",
      "Worker5 Test accuracy:87.60975609756098  loss:0.5938011556863785\n",
      "Worker6 Valid accuracy:80.39772727272727  loss:1.2702502012252808\n",
      "Worker6 Test accuracy:79.66573816155989  loss:1.3863167762756348\n",
      "Worker7 Valid accuracy:86.7237687366167  loss:0.8496512770652771\n",
      "Worker7 Test accuracy:86.73684210526316  loss:0.8544411063194275\n",
      "Worker8 Valid accuracy:91.56327543424318  loss:0.5470044016838074\n",
      "Worker8 Test accuracy:92.69183922046285  loss:0.5418573617935181\n",
      "Worker9 Valid accuracy:90.9090909090909  loss:0.7447324991226196\n",
      "Worker9 Test accuracy:91.0828025477707  loss:0.808160662651062\n",
      "Worker10 Valid accuracy:92.71255060728745  loss:0.3057478666305542\n",
      "Worker10 Test accuracy:89.68253968253968  loss:0.5513297319412231\n",
      "Worker11 Valid accuracy:86.73139158576052  loss:0.5410857200622559\n",
      "Worker11 Test accuracy:88.57142857142857  loss:0.5355696678161621\n",
      "Worker12 Valid accuracy:82.37885462555066  loss:1.4562952518463135\n",
      "Worker12 Test accuracy:80.28776978417267  loss:1.4793003797531128\n",
      "Worker13 Valid accuracy:94.824016563147  loss:0.28181809186935425\n",
      "Worker13 Test accuracy:95.10204081632654  loss:0.3207252025604248\n",
      "Worker14 Valid accuracy:86.87943262411348  loss:0.7225237488746643\n",
      "Worker14 Test accuracy:81.72413793103448  loss:1.1313921213150024\n",
      "Worker15 Valid accuracy:86.47450110864744  loss:1.3000366687774658\n",
      "Worker15 Test accuracy:83.87799564270152  loss:1.5018194913864136\n",
      "Worker16 Valid accuracy:85.46255506607929  loss:0.901329755783081\n",
      "Worker16 Test accuracy:81.22270742358079  loss:1.2089426517486572\n",
      "Worker17 Valid accuracy:79.95169082125604  loss:1.6200873851776123\n",
      "Worker17 Test accuracy:79.24528301886792  loss:1.527320146560669\n",
      "Worker18 Valid accuracy:80.85501858736059  loss:1.2369434833526611\n",
      "Worker18 Test accuracy:79.70749542961609  loss:1.227265477180481\n",
      "Worker19 Valid accuracy:83.52490421455938  loss:0.885205090045929\n",
      "Worker19 Test accuracy:82.77153558052434  loss:0.8705577850341797\n",
      "Worker20 Valid accuracy:88.88888888888889  loss:0.6117208003997803\n",
      "Worker20 Test accuracy:84.46115288220551  loss:0.8082666397094727\n",
      "Validation(tune)  loss:0.8940134614706039  accuracy:86.28063235663714\n",
      "Test(tune)  loss:0.9562382765114308  accuracy:85.52804171984647\n"
     ]
    }
   ],
   "source": [
    "acc_tune_test = []\n",
    "loss_tune_test = []\n",
    "acc_tune_valid = []\n",
    "loss_tune_valid = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "    worker.models = copy.deepcopy(server.models)\n",
    "    worker.models[worker.cluster] = worker.models[worker.cluster].to(args.device)\n",
    "    _,_,acc_tmp,loss_tmp = worker.local_train()\n",
    "    acc_tune_valid.append(acc_tmp)\n",
    "    loss_tune_valid.append(loss_tmp)\n",
    "    print('Worker{} Valid accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    \n",
    "    acc_tmp,loss_tmp = test(worker.models[worker.cluster],args.criterion,worker.testloader)\n",
    "    acc_tune_test.append(acc_tmp)\n",
    "    loss_tune_test.append(loss_tmp)\n",
    "    print('Worker{} Test accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    for i in range(args.cluster_num):\n",
    "        worker.models[i] = worker.models[i].to('cpu')\n",
    "    del worker.models\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_valid_avg = sum(acc_tune_valid)/len(acc_tune_valid)\n",
    "loss_valid_avg = sum(loss_tune_valid)/len(loss_tune_valid)\n",
    "print('Validation(tune)  loss:{}  accuracy:{}'.format(loss_valid_avg,acc_valid_avg))\n",
    "acc_test_avg = sum(acc_tune_test)/len(acc_tune_test)\n",
    "loss_test_avg = sum(loss_tune_test)/len(loss_tune_test)\n",
    "print('Test(tune)  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 15]\n"
     ]
    }
   ],
   "source": [
    "cluster = [0]*args.cluster_num\n",
    "for worker in workers:\n",
    "  cluster[worker.cluster] += 1\n",
    "\n",
    "print(cluster)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
