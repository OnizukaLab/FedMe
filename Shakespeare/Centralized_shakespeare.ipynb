{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    # random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 42\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O0TfzOhU-QlG"
   },
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 10\n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 300\n",
    "    self.local_epochs = 2\n",
    "    self.lr = None\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 10\n",
    "    self.worker_num = 20\n",
    "    self.sample_num = 20\n",
    "    self.device = device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned value\n",
    "lr = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = []\n",
    "lr_list.append(10**-3.0)\n",
    "lr_list.append(10**-2.5)\n",
    "lr_list.append(10**-2.0)\n",
    "lr_list.append(10**-1.5)\n",
    "lr_list.append(10**-1.0)\n",
    "lr_list.append(10**-0.5)\n",
    "lr_list.append(10**0.0)\n",
    "lr_list.append(10**0.5)\n",
    "\n",
    "args.lr = lr_list[lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r5PuCcqmJNUQ"
   },
   "outputs": [],
   "source": [
    "class LocalDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,dataset,worker_id):\n",
    "    self.data = []\n",
    "    self.target = []\n",
    "    self.id = worker_id\n",
    "    for i in range(len(dataset)):\n",
    "      self.data.append(dataset[i][0][0])\n",
    "      self.target.append(dataset[i][1][0])\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.data[index],self.target[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/federated_trainset_shakespeare.pickle', 'rb') as f:\n",
    "    all_federated_trainset = pickle.load(f)\n",
    "with open('../data/federated_testset_shakespeare.pickle', 'rb') as f:\n",
    "    all_federated_testset = pickle.load(f)\n",
    "all_worker_num = len(all_federated_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 6, 70, 62, 57, 35, 26, 139, 22, 108, 8, 7, 23, 55, 59, 129, 50, 107, 56, 114]\n"
     ]
    }
   ],
   "source": [
    "worker_id_list = random.sample(range(all_worker_num),args.worker_num)\n",
    "print(worker_id_list)\n",
    "federated_trainset = []\n",
    "federated_testset = []\n",
    "for i in worker_id_list:\n",
    "    federated_trainset.append(all_federated_trainset[i])\n",
    "    federated_testset.append(all_federated_testset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_valset = [None]*args.worker_num\n",
    "for i in range(args.worker_num):\n",
    "  n_samples = len(federated_trainset[i])\n",
    "  if n_samples==1:\n",
    "    federated_valset[i] = copy.deepcopy(federated_trainset[i])\n",
    "  else:\n",
    "    train_size = int(len(federated_trainset[i]) * 0.7) \n",
    "    val_size = n_samples - train_size \n",
    "    federated_trainset[i],federated_valset[i] = torch.utils.data.random_split(federated_trainset[i], [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,federated_dataset):\n",
    "    self.data = []\n",
    "    self.target = []\n",
    "    for dataset in federated_dataset:\n",
    "      for (data,target) in dataset:\n",
    "        self.data.append(data)\n",
    "        self.target.append(target)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.data[index],self.target[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_trainset = GlobalDataset(federated_trainset)\n",
    "global_valset = GlobalDataset(federated_valset)\n",
    "global_testset =  GlobalDataset(federated_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_trainloader = torch.utils.data.DataLoader(global_trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "global_valloader = torch.utils.data.DataLoader(global_valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "global_testloader = torch.utils.data.DataLoader(global_testset,batch_size=args.test_batch,shuffle=False,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ZU3vAAb9-6SD"
   },
   "outputs": [],
   "source": [
    "class RNN1(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN1, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output\n",
    "\n",
    "class RNN2(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN2, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output\n",
    "\n",
    "class RNN3(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN3, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=3, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output\n",
    "    \n",
    "class RNN4(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN4, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=4, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self):\n",
    "    self.model = RNN2()\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(federated_trainset[i],federated_valset[i],federated_testset[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "\n",
    "  def send_model(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "\n",
    "    for worker in workers:\n",
    "      worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "      worker.model = copy.deepcopy(self.model)\n",
    "      worker.model = worker.model.to(args.device)\n",
    "\n",
    "  def aggregate_model(self,workers):   \n",
    "    new_params = OrderedDict()\n",
    "    for i,worker in enumerate(workers):\n",
    "      worker_state = worker.model.state_dict()\n",
    "      for key in worker_state.keys():\n",
    "        if i==0:\n",
    "          new_params[key] = worker_state[key]*worker.aggregation_weight\n",
    "        else:\n",
    "          new_params[key] += worker_state[key]*worker.aggregation_weight\n",
    "      worker.model = worker.model.to('cpu')\n",
    "      del worker.model\n",
    "    self.model.load_state_dict(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,trainset,valset,testset):\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.model = None\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "    self.aggregation_weight = None\n",
    "\n",
    "  def local_train(self):\n",
    "    acc_train,loss_train = local_train(self.model,args.criterion,self.trainloader,args.local_epochs)\n",
    "    acc_valid,loss_valid = test(self.model,args.criterion,self.valloader)\n",
    "    return acc_train,loss_train,acc_valid,loss_valid\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7-GY66gROuEU"
   },
   "outputs": [],
   "source": [
    "def local_train(model,criterion,trainloader,epochs):\n",
    "  optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for (data,labels) in trainloader:\n",
    "      data,labels = Variable(data),Variable(labels)\n",
    "      data,labels = data.to(args.device),labels.to(args.device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(data)\n",
    "      loss = criterion(outputs,labels)\n",
    "      running_loss += loss.item()\n",
    "      predicted = torch.argmax(outputs,dim=1)\n",
    "      correct += (predicted==labels).sum().item()\n",
    "      count += len(labels)\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "      optimizer.step()\n",
    "\n",
    "  return 100.0*correct/count,running_loss/len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_train(model,criterion,trainloader,valloader,epochs,partience=0,early_stop=False):\n",
    "  if early_stop:\n",
    "    early_stopping = Early_Stopping(partience)\n",
    "\n",
    "  acc_train = []\n",
    "  loss_train = []\n",
    "  acc_valid = []\n",
    "  loss_valid = []\n",
    "  optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    model.train()\n",
    "    for (data,labels) in trainloader:\n",
    "      count += len(labels)\n",
    "      data,labels = Variable(data),Variable(labels)\n",
    "      data,labels = data.to(args.device),labels.to(args.device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(data)\n",
    "      loss = criterion(outputs,labels)\n",
    "      running_loss += loss.item()\n",
    "      predicted = torch.argmax(outputs,dim=1)\n",
    "      correct += (predicted==labels).sum().item()\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "      optimizer.step()\n",
    "    acc_train.append(100.0*correct/count)\n",
    "    loss_train.append(running_loss/len(trainloader))\n",
    "        \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    model.eval()\n",
    "    for (data,labels) in valloader:\n",
    "      count += len(labels)\n",
    "      data,labels = data.to(args.device),labels.to(args.device)\n",
    "      outputs = model(data)\n",
    "      loss = criterion(outputs,labels)\n",
    "      running_loss += loss.item()\n",
    "      predicted = torch.argmax(outputs,dim=1)\n",
    "      correct += (predicted==labels).sum().item()\n",
    "      \n",
    "    print('Epoch:{}  accuracy:{}  loss:{}'.format(epoch+1,100.0*correct/count,running_loss/len(valloader)))\n",
    "    acc_valid.append(100.0*correct/count)\n",
    "    loss_valid.append(running_loss/len(valloader))\n",
    "    if early_stop:\n",
    "      if early_stopping.validate(running_loss):\n",
    "        print('Early Stop')\n",
    "        return acc_train,loss_train,acc_valid,loss_valid\n",
    "\n",
    "  return acc_train,loss_train,acc_valid,loss_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  count = 0\n",
    "  for (data,labels) in testloader:\n",
    "    data,labels = data.to(args.device),labels.to(args.device)\n",
    "    outputs = model(data)\n",
    "    running_loss += criterion(outputs,labels).item()\n",
    "    predicted = torch.argmax(outputs,dim=1)\n",
    "    correct += (predicted==labels).sum().item()\n",
    "    count += len(labels)\n",
    "\n",
    "  accuracy = 100.0*correct/count\n",
    "  loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "  return accuracy,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "WMO7_WSLHeGl"
   },
   "outputs": [],
   "source": [
    "class Early_Stopping():\n",
    "  def __init__(self,partience):\n",
    "    self.step = 0\n",
    "    self.loss = float('inf')\n",
    "    self.partience = partience\n",
    "\n",
    "  def validate(self,loss):\n",
    "    if self.loss<loss:\n",
    "      self.step += 1\n",
    "      if self.step>self.partience:\n",
    "        return True\n",
    "    else:\n",
    "      self.step = 0\n",
    "      self.loss = loss\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1  accuracy:18.73873873873874  loss:3.131506359577179\n",
      "Epoch:2  accuracy:18.73873873873874  loss:3.1234628677368166\n",
      "Epoch:3  accuracy:18.73873873873874  loss:3.1091001510620115\n",
      "Epoch:4  accuracy:18.60860860860861  loss:3.068432021141052\n",
      "Epoch:5  accuracy:20.31031031031031  loss:2.9974594116210938\n",
      "Epoch:6  accuracy:22.06206206206206  loss:2.9100290060043337\n",
      "Epoch:7  accuracy:23.943943943943943  loss:2.829341995716095\n",
      "Epoch:8  accuracy:24.104104104104103  loss:2.789800024032593\n",
      "Epoch:9  accuracy:23.953953953953953  loss:2.7204760909080505\n",
      "Epoch:10  accuracy:26.426426426426428  loss:2.640570282936096\n",
      "Epoch:11  accuracy:29.13913913913914  loss:2.580618166923523\n",
      "Epoch:12  accuracy:29.954954954954953  loss:2.5127779603004456\n",
      "Epoch:13  accuracy:31.066066066066067  loss:2.4634976029396056\n",
      "Epoch:14  accuracy:32.56256256256256  loss:2.4282159209251404\n",
      "Epoch:15  accuracy:33.548548548548546  loss:2.400114691257477\n",
      "Epoch:16  accuracy:34.51951951951952  loss:2.357797622680664\n",
      "Epoch:17  accuracy:34.81981981981982  loss:2.3413434863090514\n",
      "Epoch:18  accuracy:35.26526526526526  loss:2.3104456424713136\n",
      "Epoch:19  accuracy:35.92092092092092  loss:2.2784114718437194\n",
      "Epoch:20  accuracy:36.331331331331334  loss:2.262404370307922\n",
      "Epoch:21  accuracy:37.12212212212212  loss:2.232147240638733\n",
      "Epoch:22  accuracy:37.587587587587585  loss:2.222842788696289\n",
      "Epoch:23  accuracy:37.98298298298298  loss:2.1868064403533936\n",
      "Epoch:24  accuracy:38.31831831831832  loss:2.1756513714790344\n",
      "Epoch:25  accuracy:38.873873873873876  loss:2.149200415611267\n",
      "Epoch:26  accuracy:39.67967967967968  loss:2.1276599645614622\n",
      "Epoch:27  accuracy:40.35535535535536  loss:2.105725610256195\n",
      "Epoch:28  accuracy:40.61561561561562  loss:2.0896960616111757\n",
      "Epoch:29  accuracy:40.57557557557558  loss:2.0714003682136535\n",
      "Epoch:30  accuracy:41.071071071071074  loss:2.0562744140625\n",
      "Epoch:31  accuracy:41.406406406406404  loss:2.043825227022171\n",
      "Epoch:32  accuracy:41.83683683683684  loss:2.0257923722267153\n",
      "Epoch:33  accuracy:41.38138138138138  loss:2.035966104269028\n",
      "Epoch:34  accuracy:42.4974974974975  loss:1.9948266625404358\n",
      "Epoch:35  accuracy:42.567567567567565  loss:1.992114382982254\n",
      "Epoch:36  accuracy:43.41841841841842  loss:1.9684871673583983\n",
      "Epoch:37  accuracy:43.528528528528525  loss:1.959369921684265\n",
      "Epoch:38  accuracy:44.009009009009006  loss:1.9448500216007232\n",
      "Epoch:39  accuracy:43.728728728728726  loss:1.952528315782547\n",
      "Epoch:40  accuracy:44.429429429429426  loss:1.9315190315246582\n",
      "Epoch:41  accuracy:44.414414414414416  loss:1.9295251071453094\n",
      "Epoch:42  accuracy:45.005005005005  loss:1.9034971117973327\n",
      "Epoch:43  accuracy:45.110110110110114  loss:1.8964253842830658\n",
      "Epoch:44  accuracy:45.23523523523524  loss:1.8922272205352784\n",
      "Epoch:45  accuracy:45.490490490490494  loss:1.8945848762989044\n",
      "Epoch:46  accuracy:45.745745745745744  loss:1.883553797006607\n",
      "Epoch:47  accuracy:45.33533533533534  loss:1.884150117635727\n",
      "Epoch:48  accuracy:45.685685685685684  loss:1.8771909534931184\n",
      "Epoch:49  accuracy:45.5955955955956  loss:1.883548367023468\n",
      "Epoch:50  accuracy:45.985985985985984  loss:1.8692103624343872\n",
      "Epoch:51  accuracy:46.04104104104104  loss:1.8657447040081023\n",
      "Epoch:52  accuracy:46.03603603603604  loss:1.8689776420593263\n",
      "Epoch:53  accuracy:46.11611611611612  loss:1.8735106348991395\n",
      "Epoch:54  accuracy:46.0960960960961  loss:1.8557876765727996\n",
      "Epoch:55  accuracy:46.43643643643644  loss:1.862326020002365\n",
      "Epoch:56  accuracy:46.391391391391394  loss:1.8658138513565063\n",
      "Epoch:57  accuracy:46.266266266266264  loss:1.8697930812835692\n",
      "Epoch:58  accuracy:46.54154154154154  loss:1.8774996101856232\n",
      "Epoch:59  accuracy:45.98098098098098  loss:1.881933331489563\n",
      "Epoch:60  accuracy:45.885885885885884  loss:1.8903608858585357\n",
      "Epoch:61  accuracy:46.58158158158158  loss:1.8814577102661132\n",
      "Epoch:62  accuracy:46.38138138138138  loss:1.8991033911705018\n",
      "Epoch:63  accuracy:46.346346346346344  loss:1.9076491177082062\n",
      "Epoch:64  accuracy:45.995995995996  loss:1.9187048971652985\n",
      "Epoch:65  accuracy:46.24124124124124  loss:1.9300699472427367\n",
      "Early Stop\n"
     ]
    }
   ],
   "source": [
    "model = RNN2()\n",
    "model = model.to(args.device)\n",
    "\n",
    "start = time.time()\n",
    "acc_train,loss_train,acc_valid,loss_valid = global_train(model,args.criterion,global_trainloader,global_valloader,args.global_epochs,partience=args.partience,early_stop=True)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = Server()\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset)\n",
    "server.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "mi_uceyoptLP",
    "outputId": "bc067e09-01bc-4e65-daf9-ac2f42373cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 accuracy:46.80382072005878  loss:1.9149193934031896\n",
      "Worker2 accuracy:45.36082474226804  loss:2.0038139820098877\n",
      "Worker3 accuracy:31.818181818181817  loss:2.385687828063965\n",
      "Worker4 accuracy:46.96969696969697  loss:1.9381753206253052\n",
      "Worker5 accuracy:62.5  loss:1.4323594570159912\n",
      "Worker6 accuracy:38.095238095238095  loss:3.1111762523651123\n",
      "Worker7 accuracy:40.0  loss:1.5672590732574463\n",
      "Worker8 accuracy:51.65289256198347  loss:1.7215585708618164\n",
      "Worker9 accuracy:66.66666666666667  loss:1.166546106338501\n",
      "Worker10 accuracy:42.54032258064516  loss:2.0178048610687256\n",
      "Worker11 accuracy:46.728971962616825  loss:1.8134396076202393\n",
      "Worker12 accuracy:42.5  loss:1.5625320672988892\n",
      "Worker13 accuracy:46.87740940632228  loss:1.8970635732014973\n",
      "Worker14 accuracy:38.622754491017965  loss:2.2313292026519775\n",
      "Worker15 accuracy:50.0  loss:1.5804443359375\n",
      "Worker16 accuracy:47.15140069247718  loss:1.8578182756900787\n",
      "Worker17 accuracy:41.02564102564103  loss:2.7087905406951904\n",
      "Worker18 accuracy:51.351351351351354  loss:1.5235930681228638\n",
      "Worker19 accuracy:44.67323187108326  loss:1.9757150808970134\n",
      "Worker20 accuracy:33.333333333333336  loss:1.6007953882217407\n",
      "Test  loss:1.9005410992673464  accuracy:45.73358691442912\n"
     ]
    }
   ],
   "source": [
    "acc_test = []\n",
    "loss_test = []\n",
    "\n",
    "server.model.to(args.device)\n",
    "\n",
    "nums = 0\n",
    "for worker in workers:\n",
    "  nums += worker.test_data_num\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.aggregation_weight = 1.0*worker.test_data_num/nums\n",
    "  acc_tmp,loss_tmp = test(server.model,args.criterion,worker.testloader)\n",
    "  acc_test.append(acc_tmp)\n",
    "  loss_test.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_test_avg = sum(acc_test)/len(acc_test)\n",
    "loss_test_avg = sum(loss_test)/len(loss_test)\n",
    "print('Test  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 Valid accuracy:46.95652173913044  loss:1.9003532462649875\n",
      "Worker1 Test accuracy:45.9808963997061  loss:1.9440261806760515\n",
      "Worker2 Valid accuracy:43.26647564469914  loss:1.9634466171264648\n",
      "Worker2 Test accuracy:44.67353951890034  loss:1.9227676391601562\n",
      "Worker3 Valid accuracy:37.735849056603776  loss:2.1340293884277344\n",
      "Worker3 Test accuracy:31.818181818181817  loss:2.458251714706421\n",
      "Worker4 Valid accuracy:53.164556962025316  loss:1.6840870380401611\n",
      "Worker4 Test accuracy:51.515151515151516  loss:1.970503807067871\n",
      "Worker5 Valid accuracy:44.73684210526316  loss:2.3607890605926514\n",
      "Worker5 Test accuracy:65.625  loss:1.4286117553710938\n",
      "Worker6 Valid accuracy:33.333333333333336  loss:2.0992465019226074\n",
      "Worker6 Test accuracy:38.095238095238095  loss:3.0134758949279785\n",
      "Worker7 Valid accuracy:0.0  loss:3.5820915699005127\n",
      "Worker7 Test accuracy:40.0  loss:1.5882651805877686\n",
      "Worker8 Valid accuracy:50.171821305841924  loss:1.8159635066986084\n",
      "Worker8 Test accuracy:51.65289256198347  loss:1.6975868940353394\n",
      "Worker9 Valid accuracy:0.0  loss:2.8804142475128174\n",
      "Worker9 Test accuracy:66.66666666666667  loss:1.1651045083999634\n",
      "Worker10 Valid accuracy:46.72268907563025  loss:1.9564615488052368\n",
      "Worker10 Test accuracy:43.75  loss:2.0170607566833496\n",
      "Worker11 Valid accuracy:35.15625  loss:2.379431962966919\n",
      "Worker11 Test accuracy:42.99065420560748  loss:1.8501708507537842\n",
      "Worker12 Valid accuracy:35.416666666666664  loss:2.299994707107544\n",
      "Worker12 Test accuracy:45.0  loss:1.7691491842269897\n",
      "Worker13 Valid accuracy:45.5187921619017  loss:1.9069682359695435\n",
      "Worker13 Test accuracy:47.301464919043944  loss:1.8809199333190918\n",
      "Worker14 Valid accuracy:39.900249376558605  loss:2.164491891860962\n",
      "Worker14 Test accuracy:41.01796407185629  loss:2.2202749252319336\n",
      "Worker15 Valid accuracy:60.0  loss:1.2104495763778687\n",
      "Worker15 Test accuracy:62.5  loss:1.5815669298171997\n",
      "Worker16 Valid accuracy:45.40923399790137  loss:1.9587560296058655\n",
      "Worker16 Test accuracy:47.214353163361665  loss:1.888471096754074\n",
      "Worker17 Valid accuracy:51.06382978723404  loss:1.776110053062439\n",
      "Worker17 Test accuracy:37.17948717948718  loss:2.6713545322418213\n",
      "Worker18 Valid accuracy:48.86363636363637  loss:1.9559074640274048\n",
      "Worker18 Test accuracy:51.351351351351354  loss:1.4789774417877197\n",
      "Worker19 Valid accuracy:45.99030212607236  loss:1.9224346081415813\n",
      "Worker19 Test accuracy:45.837063563115485  loss:1.9795271555582683\n",
      "Worker20 Valid accuracy:33.333333333333336  loss:1.7993992567062378\n",
      "Worker20 Test accuracy:33.333333333333336  loss:1.6154876947402954\n",
      "Validation(tune)  loss:2.087541325555907  accuracy:39.83701915179159\n",
      "Test(tune)  loss:1.9070777038023583  accuracy:46.67516191814924\n"
     ]
    }
   ],
   "source": [
    "acc_tune_test = []\n",
    "loss_tune_test = []\n",
    "acc_tune_valid = []\n",
    "loss_tune_valid = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "    worker.model = copy.deepcopy(server.model)\n",
    "    worker.model = worker.model.to(args.device)\n",
    "    _,_,acc_tmp,loss_tmp = worker.local_train()\n",
    "    acc_tune_valid.append(acc_tmp)\n",
    "    loss_tune_valid.append(loss_tmp)\n",
    "    print('Worker{} Valid accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    \n",
    "    acc_tmp,loss_tmp = test(worker.model,args.criterion,worker.testloader)\n",
    "    acc_tune_test.append(acc_tmp)\n",
    "    loss_tune_test.append(loss_tmp)\n",
    "    print('Worker{} Test accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    worker.model = worker.model.to('cpu')\n",
    "    del worker.model\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_valid_avg = sum(acc_tune_valid)/len(acc_tune_valid)\n",
    "loss_valid_avg = sum(loss_tune_valid)/len(loss_tune_valid)\n",
    "print('Validation(tune)  loss:{}  accuracy:{}'.format(loss_valid_avg,acc_valid_avg))\n",
    "acc_test_avg = sum(acc_tune_test)/len(acc_tune_test)\n",
    "loss_test_avg = sum(loss_tune_test)/len(loss_tune_test)\n",
    "print('Test(tune)  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
