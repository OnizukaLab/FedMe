{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    # random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 42\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O0TfzOhU-QlG"
   },
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 10\n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 100\n",
    "    self.local_epochs = 2\n",
    "    self.lr = None\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 100\n",
    "    self.worker_num = 20\n",
    "    self.sample_num = 20\n",
    "    self.cluster_list = [1,2,3,4]\n",
    "    self.cluster_num = None\n",
    "    self.turn_of_cluster_num = [0,50,75,90]\n",
    "    self.turn_of_replacement_model = list(range(self.global_epochs))\n",
    "    self.unlabeleddata_size = 1000\n",
    "    self.device = device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion_ce = nn.CrossEntropyLoss()\n",
    "    self.criterion_kl = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = []\n",
    "lr_list.append(10**-3.0)\n",
    "lr_list.append(10**-2.5)\n",
    "lr_list.append(10**-2.0)\n",
    "lr_list.append(10**-1.5)\n",
    "lr_list.append(10**-1.0)\n",
    "lr_list.append(10**-0.5)\n",
    "lr_list.append(10**0.0)\n",
    "lr_list.append(10**0.5)\n",
    "\n",
    "args.lr = lr_list[lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r5PuCcqmJNUQ"
   },
   "outputs": [],
   "source": [
    "class LocalDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,dataset,worker_id):\n",
    "    self.data = []\n",
    "    self.target = []\n",
    "    self.id = worker_id\n",
    "    for i in range(len(dataset)):\n",
    "      self.data.append(dataset[i][0][0])\n",
    "      self.target.append(dataset[i][1][0])\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.data[index],self.target[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/federated_trainset_shakespeare.pickle', 'rb') as f:\n",
    "    all_federated_trainset = pickle.load(f)\n",
    "with open('../data/federated_testset_shakespeare.pickle', 'rb') as f:\n",
    "    all_federated_testset = pickle.load(f)\n",
    "all_worker_num = len(all_federated_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 6, 70, 62, 57, 35, 26, 139, 22, 108, 8, 7, 23, 55, 59, 129, 50, 107, 56, 114]\n"
     ]
    }
   ],
   "source": [
    "worker_id_list = random.sample(range(all_worker_num),args.worker_num)\n",
    "print(worker_id_list)\n",
    "federated_trainset = []\n",
    "federated_testset = []\n",
    "for i in worker_id_list:\n",
    "    federated_trainset.append(all_federated_trainset[i])\n",
    "    federated_testset.append(all_federated_testset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_valset = [None]*args.worker_num\n",
    "for i in range(args.worker_num):\n",
    "  n_samples = len(federated_trainset[i])\n",
    "  if n_samples==1:\n",
    "    federated_valset[i] = copy.deepcopy(federated_trainset[i])\n",
    "  else:\n",
    "    train_size = int(len(federated_trainset[i]) * 0.7) \n",
    "    val_size = n_samples - train_size \n",
    "    federated_trainset[i],federated_valset[i] = torch.utils.data.random_split(federated_trainset[i], [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self):\n",
    "    self.data = []\n",
    "    self.target = None\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.data[index],'unlabeled'\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dataset = UnlabeledDataset()\n",
    "\n",
    "for i in range(all_worker_num):\n",
    "    if i not in worker_id_list:\n",
    "        unlabeled_dataset.data = unlabeled_dataset.data + all_federated_trainset[i].data\n",
    "        \n",
    "unlabeled_dataset,_ = torch.utils.data.random_split(unlabeled_dataset, [args.unlabeleddata_size, len(unlabeled_dataset)-args.unlabeleddata_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZU3vAAb9-6SD"
   },
   "outputs": [],
   "source": [
    "class RNN1(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN1, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output\n",
    "\n",
    "class RNN2(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN2, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output\n",
    "\n",
    "class RNN3(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN3, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=3, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output\n",
    "    \n",
    "class RNN4(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN4, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=4, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans(object):\n",
    "    \"\"\"KMeans 法でクラスタリングするクラス\"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters=2, max_iter=300):\n",
    "        \"\"\"コンストラクタ\n",
    "\n",
    "        Args:\n",
    "            n_clusters (int): クラスタ数\n",
    "            max_iter (int): 最大イテレーション数\n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        self.cluster_centers_ = None\n",
    "\n",
    "    def fit_predict(self, features):\n",
    "        \"\"\"クラスタリングを実施する\n",
    "\n",
    "        Args:\n",
    "            features (numpy.ndarray): ラベル付けするデータ\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: ラベルデータ\n",
    "        \"\"\"\n",
    "            \n",
    "        # 要素の中からセントロイド (重心) の初期値となる候補をクラスタ数だけ選び出す\n",
    "        feature_indexes = np.arange(len(features))\n",
    "        np.random.shuffle(feature_indexes)\n",
    "        initial_centroid_indexes = feature_indexes[:self.n_clusters]\n",
    "        self.cluster_centers_ = features[initial_centroid_indexes]\n",
    "\n",
    "        # ラベル付けした結果となる配列はゼロで初期化しておく\n",
    "        pred = np.zeros(features.shape)\n",
    "        \n",
    "\n",
    "        # クラスタリングをアップデートする\n",
    "        for _ in range(self.max_iter):\n",
    "\n",
    "            # 各特徴ベクトルから最短距離となるセントロイドを基準に新しいラベルをつける\n",
    "            new_pred = np.array([\n",
    "                np.array([\n",
    "                    self.Euclidean_distance(p, centroid)\n",
    "                    for centroid in self.cluster_centers_\n",
    "                ]).argmin()\n",
    "                for p in features\n",
    "            ])\n",
    "\n",
    "            if np.all(new_pred == pred):\n",
    "                # 更新前と内容を比較して、もし同じなら終了\n",
    "                break\n",
    "\n",
    "            pred = new_pred\n",
    "            \n",
    "            # 各クラスタごとにセントロイド (重心) を再計算する\n",
    "            self.cluster_centers_ = np.array([features[pred == i].mean(axis=0)\n",
    "                                              for i in range(self.n_clusters)])\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def KLD(self, p0, p1):\n",
    "        P = torch.from_numpy(p0.astype(np.float32)).clone()\n",
    "        Q = torch.from_numpy(p1.astype(np.float32)).clone()\n",
    "        P = F.softmax(Variable(P), dim=1)\n",
    "        Q = F.softmax(Variable(Q), dim=1)\n",
    "        kld = ((P/(P+Q))*(P * (P / ((P/(P+Q))*P + (Q/(P+Q))*Q)).log())).sum() + ((Q/(P+Q))*(Q * (Q / ((P/(P+Q))*P + (Q/(P+Q))*Q)).log())).sum()\n",
    "        return kld\n",
    "    \n",
    "    def Euclidean_distance(self, p0, p1):\n",
    "        return np.sum((p0 - p1) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self,unlabeled_dataset):\n",
    "    self.cluster = None\n",
    "    self.models = None\n",
    "    self.unlabeled_dataloader = torch.utils.data.DataLoader(unlabeled_dataset,batch_size=args.batch_size,shuffle=False,num_workers=2)\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset,client_best_model):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(i,federated_trainset[i],federated_valset[i],federated_testset[i],client_best_model[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "  def collect_model(self,workers):\n",
    "    self.models = [None]*args.worker_num\n",
    "    for worker in workers:\n",
    "      self.models[worker.id] = copy.deepcopy(worker.local_model)\n",
    "\n",
    "  def send_model(self,workers):\n",
    "    for worker in workers:\n",
    "      worker.local_model = copy.deepcopy(self.models[worker.id])\n",
    "      worker.other_model = copy.deepcopy(self.models[worker.other_model_id])\n",
    "        \n",
    "  def return_model(self,workers):\n",
    "    for worker in workers:\n",
    "      worker.local_model = copy.deepcopy(self.models[worker.local_model_id])\n",
    "      worker.local_model_id = worker.id\n",
    "    del self.models\n",
    "    \n",
    "  def aggregate_model(self,workers):   \n",
    "    new_params = []\n",
    "    train_model_id = []\n",
    "    train_model_id_count = []\n",
    "    for worker in workers:\n",
    "      worker_state = worker.local_model.state_dict()\n",
    "      if worker.id in train_model_id:\n",
    "        i = train_model_id.index(worker.id)\n",
    "        for key in worker_state.keys():\n",
    "          new_params[i][key] += worker_state[key]\n",
    "        train_model_id_count[i] += 1\n",
    "      else:\n",
    "        new_params.append(OrderedDict())\n",
    "        train_model_id.append(worker.id)\n",
    "        train_model_id_count.append(1)\n",
    "        i = train_model_id.index(worker.id)\n",
    "        for key in worker_state.keys():\n",
    "          new_params[i][key] = worker_state[key]\n",
    "        \n",
    "      worker_state = worker.other_model.state_dict()\n",
    "      if worker.other_model_id in train_model_id:\n",
    "        i = train_model_id.index(worker.other_model_id)\n",
    "        for key in worker_state.keys():\n",
    "          new_params[i][key] += worker_state[key]\n",
    "        train_model_id_count[i] += 1\n",
    "      else:\n",
    "        new_params.append(OrderedDict())\n",
    "        train_model_id.append(worker.other_model_id)\n",
    "        train_model_id_count.append(1)\n",
    "        i = train_model_id.index(worker.other_model_id)\n",
    "        for key in worker_state.keys():\n",
    "          new_params[i][key] = worker_state[key]\n",
    "        \n",
    "      worker.local_model = worker.local_model.to('cpu')\n",
    "      worker.other_model = worker.other_model.to('cpu')\n",
    "      del worker.local_model,worker.other_model\n",
    "    \n",
    "    for i,model_id in enumerate(train_model_id):\n",
    "      for key in new_params[i].keys():\n",
    "        new_params[i][key] = new_params[i][key]/train_model_id_count[i]\n",
    "      self.models[model_id].load_state_dict(new_params[i])\n",
    "      \n",
    "  '''clustering by kmeans'''  \n",
    "  def clustering(self,workers):\n",
    "    if args.cluster_num==1:\n",
    "        pred = [0]*len(workers)\n",
    "        worker_id_list = []\n",
    "        for worker in workers:\n",
    "            worker_id_list.append(worker.id)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            worker_softmax_targets = [[] for i in range(len(workers))]\n",
    "            worker_id_list = []\n",
    "            count = 0\n",
    "            for i,model in enumerate(self.models):\n",
    "              if model==None:\n",
    "                pass\n",
    "              else:\n",
    "                model = model.to(args.device)\n",
    "                model.eval()\n",
    "                for data,_ in self.unlabeled_dataloader:\n",
    "                  data = data.to(args.device)\n",
    "                  worker_softmax_targets[count].append(model(data).to('cpu').detach().numpy())\n",
    "                worker_softmax_targets[count] = np.array(worker_softmax_targets[count])\n",
    "                model = model.to('cpu')\n",
    "                worker_id_list.append(i)\n",
    "                count += 1\n",
    "            worker_softmax_targets = np.array(worker_softmax_targets)\n",
    "            kmeans = KMeans(n_clusters=args.cluster_num)\n",
    "            pred = kmeans.fit_predict(worker_softmax_targets)\n",
    "    self.cluster = []\n",
    "    for i in range(args.cluster_num):\n",
    "      self.cluster.append([])\n",
    "    for i,cls in enumerate(pred):\n",
    "      self.cluster[cls].append(worker_id_list[i])\n",
    "    for worker in workers:\n",
    "      idx = worker_id_list.index(worker.id)\n",
    "      worker.cluster_num = pred[idx]\n",
    "        \n",
    "  def decide_other_model(self,workers):\n",
    "    for worker in workers:\n",
    "      cls = worker.cluster_num\n",
    "      '''if number of worker in cluster is one, other model is decided by random in all workers. '''\n",
    "      if len(self.cluster[cls])==1:\n",
    "        while True:\n",
    "          other_worker = random.choice(workers)\n",
    "          other_model_id = other_worker.id\n",
    "          if worker.id!=other_model_id:\n",
    "            break\n",
    "      else:\n",
    "        while True:\n",
    "          other_model_id = random.choice(self.cluster[cls])\n",
    "          if worker.id!=other_model_id:\n",
    "            break\n",
    "      worker.other_model_id = other_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,i,trainset,valset,testset,best_model):\n",
    "    self.id = i\n",
    "    self.cluster_num = None\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    if best_model==1:\n",
    "      self.local_model = RNN1()\n",
    "    elif best_model==2:\n",
    "      self.local_model = RNN2()\n",
    "    elif best_model==3:\n",
    "      self.local_model = RNN3()\n",
    "    elif best_model==4:\n",
    "      self.local_model = RNN4()\n",
    "    self.local_model_id = i\n",
    "    self.other_model = None\n",
    "    self.other_model_id = None\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "\n",
    "  def local_train(self):\n",
    "    self.local_model = self.local_model.to(args.device)\n",
    "    self.other_model = self.other_model.to(args.device)\n",
    "    local_optimizer = optim.SGD(self.local_model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "    other_optimizer = optim.SGD(self.other_model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "    self.local_model.train()\n",
    "    self.other_model.train()\n",
    "    for epoch in range(args.local_epochs):\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in self.trainloader:\n",
    "        data,labels = Variable(data),Variable(labels)\n",
    "        data,labels = data.to(args.device),labels.to(args.device)\n",
    "        local_optimizer.zero_grad()\n",
    "        other_optimizer.zero_grad()\n",
    "        local_outputs = self.local_model(data)\n",
    "        other_outputs = self.other_model(data)\n",
    "        #train local_model\n",
    "        ce_loss = args.criterion_ce(local_outputs,labels)\n",
    "        kl_loss = args.criterion_kl(F.log_softmax(local_outputs, dim = 1),F.softmax(Variable(other_outputs), dim=1))\n",
    "        loss = ce_loss + kl_loss\n",
    "        running_loss += loss.item()\n",
    "        predicted = torch.argmax(local_outputs,dim=1)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "        count += len(labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), args.clip)\n",
    "        local_optimizer.step()\n",
    "\n",
    "        #train other_model\n",
    "        ce_loss = args.criterion_ce(other_outputs,labels)\n",
    "        kl_loss = args.criterion_kl(F.log_softmax(other_outputs, dim = 1),F.softmax(Variable(local_outputs), dim=1))\n",
    "        loss = ce_loss + kl_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.other_model.parameters(), args.clip)\n",
    "        other_optimizer.step()\n",
    "        \n",
    "    return 100.0*correct/count,running_loss/len(self.trainloader)\n",
    "\n",
    "        \n",
    "  def validate(self):\n",
    "    acc,loss = test(self.local_model,args.criterion_ce,self.valloader)\n",
    "    return acc,loss\n",
    "\n",
    "\n",
    "  def model_replacement(self):\n",
    "    _,loss_local = test(self.local_model,args.criterion_ce,self.valloader)\n",
    "    _,loss_other = test(self.other_model,args.criterion_ce,self.valloader)\n",
    "    if loss_other<loss_local:\n",
    "        self.local_model_id = self.other_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7-GY66gROuEU"
   },
   "outputs": [],
   "source": [
    "def train(model,criterion,trainloader,epochs):\n",
    "  optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for (data,labels) in trainloader:\n",
    "      data,labels = Variable(data),Variable(labels)\n",
    "      data,labels = data.to(args.device),labels.to(args.device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(data)\n",
    "      loss = criterion(outputs,labels)\n",
    "      running_loss += loss.item()\n",
    "      predicted = torch.argmax(outputs,dim=1)\n",
    "      correct += (predicted==labels).sum().item()\n",
    "      count += len(labels)\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "      optimizer.step()\n",
    "\n",
    "  return 100.0*correct/count,running_loss/len(trainloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  count = 0\n",
    "  for (data,labels) in testloader:\n",
    "    data,labels = data.to(args.device),labels.to(args.device)\n",
    "    outputs = model(data)\n",
    "    running_loss += criterion(outputs,labels).item()\n",
    "    predicted = torch.argmax(outputs,dim=1)\n",
    "    correct += (predicted==labels).sum().item()\n",
    "    count += len(labels)\n",
    "\n",
    "  accuracy = 100.0*correct/count\n",
    "  loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "  return accuracy,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WMO7_WSLHeGl"
   },
   "outputs": [],
   "source": [
    "class Early_Stopping():\n",
    "  def __init__(self,partience):\n",
    "    self.step = 0\n",
    "    self.loss = float('inf')\n",
    "    self.partience = partience\n",
    "\n",
    "  def validate(self,loss):\n",
    "    if self.loss<loss:\n",
    "      self.step += 1\n",
    "      if self.step>self.partience:\n",
    "        return True\n",
    "    else:\n",
    "      self.step = 0\n",
    "      self.loss = loss\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../client_best_model/client_best_model_shakespeare_42.csv') as fp:\n",
    "    csvList = list(csv.reader(fp))\n",
    "client_best_model = [int(item) for subList in csvList for item in subList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "-noG_98IR-nZ",
    "outputId": "78a6ebe2-854a-4f83-dc45-5c4ac35b69e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1  loss:4.198258861899376  accuracy:10.651870824872297\n",
      "Epoch2  loss:3.983953463037808  accuracy:14.60677448747968\n",
      "Epoch3  loss:3.6042719238334238  accuracy:15.56038421250699\n",
      "Epoch4  loss:3.3906404561466648  accuracy:15.83442143055214\n",
      "Epoch5  loss:3.2383825984266066  accuracy:16.300496756722662\n",
      "Epoch6  loss:3.142006513145234  accuracy:17.441770613295063\n",
      "Epoch7  loss:3.08594811823633  accuracy:18.91312855055702\n",
      "Epoch8  loss:3.0313406692610845  accuracy:20.111726739634488\n",
      "Epoch9  loss:2.9843761026859283  accuracy:21.261069394706865\n",
      "Epoch10  loss:2.955580001738337  accuracy:22.16170537770375\n",
      "Epoch11  loss:2.9240746759706067  accuracy:21.35003284756231\n",
      "Epoch12  loss:2.8586263298988337  accuracy:23.329559869580272\n",
      "Epoch13  loss:2.8257503413491776  accuracy:24.177537814651917\n",
      "Epoch14  loss:2.8007778465747837  accuracy:25.115764823963303\n",
      "Epoch15  loss:2.7458615336153245  accuracy:27.32232453615374\n",
      "Epoch16  loss:2.7278159297174884  accuracy:25.581527947044666\n",
      "Epoch17  loss:2.7072291807995903  accuracy:25.28486497137859\n",
      "Epoch18  loss:2.688645538025432  accuracy:27.096381697804247\n",
      "Epoch19  loss:2.6664342181550134  accuracy:27.540198050846456\n",
      "Epoch20  loss:2.6301803880267673  accuracy:28.52523299734464\n",
      "Epoch21  loss:2.6166352109776603  accuracy:27.527628543899638\n",
      "Epoch22  loss:2.599237806929483  accuracy:28.505948344362455\n",
      "Epoch23  loss:2.5872025847435  accuracy:30.163886718657878\n",
      "Epoch24  loss:2.5520815468496743  accuracy:29.81016950426074\n",
      "Epoch25  loss:2.5284533497360018  accuracy:31.03137649504487\n",
      "Epoch26  loss:2.5054270293977523  accuracy:31.487844457473596\n",
      "Epoch27  loss:2.509881321589152  accuracy:32.13533039027341\n",
      "Epoch28  loss:2.50078208843867  accuracy:31.123390080954874\n",
      "Epoch29  loss:2.4706826508045197  accuracy:31.06604348753211\n",
      "Epoch30  loss:2.458924754460653  accuracy:31.223621429337566\n",
      "Epoch31  loss:2.4289790600538255  accuracy:30.754933755974935\n",
      "Epoch32  loss:2.4246306068367427  accuracy:31.593925992571027\n",
      "Epoch33  loss:2.433676999807358  accuracy:31.58835702165372\n",
      "Epoch34  loss:2.4036330425077015  accuracy:32.33098025536313\n",
      "Epoch35  loss:2.381109293633037  accuracy:30.164211520895734\n",
      "Epoch36  loss:2.3740396357244915  accuracy:31.069033119376872\n",
      "Epoch37  loss:2.3623814884159304  accuracy:31.06159434411104\n",
      "Epoch38  loss:2.347076459394561  accuracy:31.485500735409897\n",
      "Epoch39  loss:2.3403265800740987  accuracy:31.24157975999677\n",
      "Epoch40  loss:2.322243889504009  accuracy:31.356227306588938\n",
      "Epoch41  loss:2.321425480312772  accuracy:32.358732971166084\n",
      "Epoch42  loss:2.326825397875574  accuracy:31.504561440041712\n",
      "Epoch43  loss:2.3227625634935167  accuracy:31.63198144544735\n",
      "Epoch44  loss:2.302430568469895  accuracy:31.816662695163377\n",
      "Epoch45  loss:2.298965150449011  accuracy:31.643506326525795\n",
      "Epoch46  loss:2.292306400007672  accuracy:32.191056029697975\n",
      "Epoch47  loss:2.28069077283144  accuracy:32.862038715432774\n",
      "Epoch48  loss:2.2588079983989395  accuracy:34.162963286108806\n",
      "Epoch49  loss:2.2606221458978117  accuracy:32.85866077347222\n",
      "Epoch50  loss:2.2683654859662057  accuracy:33.91224184884456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acc12212vx/jupyter_env/lib/python3.6/site-packages/ipykernel_launcher.py:48: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch51  loss:2.2434962915049663  accuracy:34.457767671141546\n",
      "Epoch52  loss:2.245163856281174  accuracy:33.72202237568606\n",
      "Epoch53  loss:2.2416701871487827  accuracy:33.193129908532114\n",
      "Epoch54  loss:2.243698896798823  accuracy:32.76735209685523\n",
      "Epoch55  loss:2.2499532027377023  accuracy:33.13213596553535\n",
      "Epoch56  loss:2.2323292300105093  accuracy:33.62153202095396\n",
      "Epoch57  loss:2.2358024014367  accuracy:33.31124371994409\n",
      "Epoch58  loss:2.2343278924624124  accuracy:34.00073498238374\n",
      "Epoch59  loss:2.220729173388746  accuracy:33.844746650154654\n",
      "Epoch60  loss:2.2065521775020494  accuracy:33.33467048542896\n",
      "Epoch61  loss:2.1964914313620993  accuracy:33.68698167350901\n",
      "Epoch62  loss:2.1947868580619496  accuracy:33.315349843704766\n",
      "Epoch63  loss:2.1934416815638547  accuracy:34.25414464444166\n",
      "Epoch64  loss:2.193582003149721  accuracy:34.78852015926791\n",
      "Epoch65  loss:2.191911637451914  accuracy:34.06386726516609\n",
      "Epoch66  loss:2.1908940802017844  accuracy:34.240483985142774\n",
      "Epoch67  loss:2.185657610661454  accuracy:34.50579381889146\n",
      "Epoch68  loss:2.1857873330513637  accuracy:35.99928646487079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acc12212vx/jupyter_env/lib/python3.6/site-packages/ipykernel_launcher.py:56: RuntimeWarning: Mean of empty slice.\n",
      "/home/acc12212vx/jupyter_env/lib/python3.6/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch69  loss:2.1823716019590695  accuracy:34.9162722787044\n",
      "Epoch70  loss:2.1806348691384  accuracy:35.244986729257306\n",
      "Epoch71  loss:2.1775920857985813  accuracy:34.667028242400995\n",
      "Epoch72  loss:2.1819047002328764  accuracy:35.34386350932925\n",
      "Epoch73  loss:2.175524635116259  accuracy:35.34365956611227\n",
      "Epoch74  loss:2.172938975857364  accuracy:34.49066964509605\n",
      "Epoch75  loss:2.1717261882291896  accuracy:34.32120260185827\n",
      "Epoch76  loss:2.1865069578091307  accuracy:34.10645893685997\n",
      "Epoch77  loss:2.1807282881604304  accuracy:34.89359211120683\n",
      "Epoch78  loss:2.185183694958687  accuracy:34.02240935520897\n",
      "Epoch79  loss:2.185211420390342  accuracy:33.84983132342728\n",
      "Epoch80  loss:2.1769041723675198  accuracy:33.81795612601263\n",
      "Epoch81  loss:2.1744099178247978  accuracy:35.0802141123586\n",
      "Epoch82  loss:2.1723133357034787  accuracy:34.71955526644721\n",
      "Epoch83  loss:2.1767233891619573  accuracy:34.30402885843345\n",
      "Epoch84  loss:2.1664575151271293  accuracy:34.49093806612127\n",
      "Epoch85  loss:2.1682327172822418  accuracy:34.97958999482049\n",
      "Epoch86  loss:2.174193280438582  accuracy:33.697855390287295\n",
      "Epoch87  loss:2.1675960388448505  accuracy:34.31812629584838\n",
      "Epoch88  loss:2.167272402014998  accuracy:34.46713425056214\n",
      "Epoch89  loss:2.1805682029989035  accuracy:34.115476092638936\n",
      "Epoch90  loss:2.1874984307421577  accuracy:33.86533896001732\n",
      "Epoch91  loss:2.161120794216792  accuracy:35.06551257856759\n",
      "Epoch92  loss:2.162868480218781  accuracy:35.063539961262194\n",
      "Epoch93  loss:2.1644923037952846  accuracy:34.38076877188617\n",
      "Epoch94  loss:2.1682456170519195  accuracy:34.51747331291688\n",
      "Epoch95  loss:2.164604253570239  accuracy:34.62769761443836\n",
      "Epoch96  loss:2.152632303204801  accuracy:34.69760433476232\n",
      "Epoch97  loss:2.1665448710322384  accuracy:34.51467322195006\n",
      "Epoch98  loss:2.1612801437576614  accuracy:34.92293445598617\n",
      "Epoch99  loss:2.165070025788413  accuracy:34.723932931664336\n",
      "Epoch100  loss:2.162288851870431  accuracy:34.24218699481192\n"
     ]
    }
   ],
   "source": [
    "server = Server(unlabeled_dataset)\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset,client_best_model)\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "acc_valid = []\n",
    "loss_valid = []\n",
    "\n",
    "early_stopping = Early_Stopping(args.partience)\n",
    "\n",
    "start = time.time()#開始時刻\n",
    "\n",
    "for epoch in range(args.global_epochs):\n",
    "  if epoch in args.turn_of_cluster_num:\n",
    "    idx = args.turn_of_cluster_num.index(epoch)\n",
    "    args.cluster_num = args.cluster_list[idx]\n",
    "  sample_worker = server.sample_worker(workers)\n",
    "  server.collect_model(sample_worker)\n",
    "  server.clustering(sample_worker)\n",
    "  server.decide_other_model(sample_worker)\n",
    "  server.send_model(sample_worker)\n",
    "\n",
    "  acc_train_avg = 0.0\n",
    "  loss_train_avg = 0.0\n",
    "  acc_valid_avg = 0.0\n",
    "  loss_valid_avg = 0.0\n",
    "  for worker in sample_worker:\n",
    "    acc_train_tmp,loss_train_tmp = worker.local_train()\n",
    "    acc_valid_tmp,loss_valid_tmp = worker.validate()\n",
    "    acc_train_avg += acc_train_tmp/len(sample_worker)\n",
    "    loss_train_avg += loss_train_tmp/len(sample_worker)\n",
    "    acc_valid_avg += acc_valid_tmp/len(sample_worker)\n",
    "    loss_valid_avg += loss_valid_tmp/len(sample_worker)\n",
    "  if epoch in args.turn_of_replacement_model:\n",
    "    for worker in sample_worker:\n",
    "      worker.model_replacement()\n",
    "  server.aggregate_model(sample_worker)\n",
    "  server.return_model(sample_worker)\n",
    "  '''\n",
    "  server.model.to(args.device)\n",
    "  for worker in workers:\n",
    "    acc_valid_tmp,loss_valid_tmp = test(server.model,args.criterion,worker.valloader)\n",
    "    acc_valid_avg += acc_valid_tmp/len(workers)\n",
    "    loss_valid_avg += loss_valid_tmp/len(workers)\n",
    "  server.model.to('cpu')\n",
    "  '''\n",
    "  print('Epoch{}  loss:{}  accuracy:{}'.format(epoch+1,loss_valid_avg,acc_valid_avg))\n",
    "  acc_train.append(acc_train_avg)\n",
    "  loss_train.append(loss_train_avg)\n",
    "  acc_valid.append(acc_valid_avg)\n",
    "  loss_valid.append(loss_valid_avg)\n",
    "\n",
    "  if early_stopping.validate(loss_valid_avg):\n",
    "    print('Early Stop')\n",
    "    break\n",
    "    \n",
    "end = time.time()#終了時刻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習時間：14694.664063692093秒\n"
     ]
    }
   ],
   "source": [
    "print('学習時間：{}秒'.format(end-start))#終了時刻-開始時刻でかかった時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 accuracy:44.511388684790596  loss:1.9563690764563424\n",
      "Worker2 accuracy:42.61168384879725  loss:2.114173173904419\n",
      "Worker3 accuracy:43.18181818181818  loss:2.130749464035034\n",
      "Worker4 accuracy:50.0  loss:2.2862415313720703\n",
      "Worker5 accuracy:53.125  loss:1.8954030275344849\n",
      "Worker6 accuracy:52.38095238095238  loss:2.5655641555786133\n",
      "Worker7 accuracy:40.0  loss:1.4525091648101807\n",
      "Worker8 accuracy:54.13223140495868  loss:1.798909068107605\n",
      "Worker9 accuracy:66.66666666666667  loss:1.634102463722229\n",
      "Worker10 accuracy:38.104838709677416  loss:2.1451563835144043\n",
      "Worker11 accuracy:40.18691588785047  loss:2.0083608627319336\n",
      "Worker12 accuracy:45.0  loss:1.7421486377716064\n",
      "Worker13 accuracy:43.25366229760987  loss:1.9714645544687908\n",
      "Worker14 accuracy:37.125748502994014  loss:2.209486961364746\n",
      "Worker15 accuracy:50.0  loss:1.705851674079895\n",
      "Worker16 accuracy:43.90934844192635  loss:1.9525646269321442\n",
      "Worker17 accuracy:39.743589743589745  loss:2.620882749557495\n",
      "Worker18 accuracy:43.24324324324324  loss:1.7840313911437988\n",
      "Worker19 accuracy:43.2408236347359  loss:2.0071370204289756\n",
      "Worker20 accuracy:33.333333333333336  loss:1.834418773651123\n",
      "Test  loss:1.9907762380582947  accuracy:45.18756224814721\n"
     ]
    }
   ],
   "source": [
    "acc_test = []\n",
    "loss_test = []\n",
    "\n",
    "start = time.time()#開始時刻\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.local_model = worker.local_model.to(args.device)\n",
    "  acc_tmp,loss_tmp = test(worker.local_model,args.criterion_ce,worker.testloader)\n",
    "  acc_test.append(acc_tmp)\n",
    "  loss_test.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "  worker.local_model = worker.local_model.to('cpu')\n",
    "\n",
    "end = time.time()#終了時刻\n",
    "\n",
    "acc_test_avg = sum(acc_test)/len(acc_test)\n",
    "loss_test_avg = sum(loss_test)/len(loss_test)\n",
    "print('Test  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推論時間：4.832446813583374秒\n"
     ]
    }
   ],
   "source": [
    "print('推論時間：{}秒'.format(end-start))#終了時刻-開始時刻でかかった時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 Valid accuracy:44.237599510104104  loss:1.978326506084866\n",
      "Worker1 Test accuracy:43.27700220426157  loss:1.9968429122652327\n",
      "Worker2 Valid accuracy:40.11461318051576  loss:2.127464532852173\n",
      "Worker2 Test accuracy:41.92439862542955  loss:2.1207661628723145\n",
      "Worker3 Valid accuracy:33.9622641509434  loss:2.1820123195648193\n",
      "Worker3 Test accuracy:43.18181818181818  loss:2.1478054523468018\n",
      "Worker4 Valid accuracy:36.70886075949367  loss:2.243539810180664\n",
      "Worker4 Test accuracy:45.45454545454545  loss:2.2879741191864014\n",
      "Worker5 Valid accuracy:39.473684210526315  loss:2.4351000785827637\n",
      "Worker5 Test accuracy:50.0  loss:1.878347396850586\n",
      "Worker6 Valid accuracy:41.666666666666664  loss:1.9529699087142944\n",
      "Worker6 Test accuracy:52.38095238095238  loss:2.563145399093628\n",
      "Worker7 Valid accuracy:0.0  loss:2.6686911582946777\n",
      "Worker7 Test accuracy:40.0  loss:1.4974935054779053\n",
      "Worker8 Valid accuracy:48.45360824742268  loss:1.8956695795059204\n",
      "Worker8 Test accuracy:52.06611570247934  loss:1.8166923522949219\n",
      "Worker9 Valid accuracy:0.0  loss:2.900374174118042\n",
      "Worker9 Test accuracy:66.66666666666667  loss:1.63174307346344\n",
      "Worker10 Valid accuracy:41.34453781512605  loss:2.103214979171753\n",
      "Worker10 Test accuracy:37.29838709677419  loss:2.1726953983306885\n",
      "Worker11 Valid accuracy:35.15625  loss:2.3560304641723633\n",
      "Worker11 Test accuracy:40.18691588785047  loss:2.047077178955078\n",
      "Worker12 Valid accuracy:35.416666666666664  loss:2.1360771656036377\n",
      "Worker12 Test accuracy:40.0  loss:1.8610498905181885\n",
      "Worker13 Valid accuracy:42.04946996466431  loss:2.0253096520900726\n",
      "Worker13 Test accuracy:43.02235929067078  loss:1.9936400651931763\n",
      "Worker14 Valid accuracy:37.406483790523694  loss:2.297315835952759\n",
      "Worker14 Test accuracy:37.125748502994014  loss:2.2112786769866943\n",
      "Worker15 Valid accuracy:40.0  loss:1.6419610977172852\n",
      "Worker15 Test accuracy:50.0  loss:1.7533212900161743\n",
      "Worker16 Valid accuracy:42.91710388247639  loss:2.0371646881103516\n",
      "Worker16 Test accuracy:43.1853950267548  loss:1.98454150557518\n",
      "Worker17 Valid accuracy:43.61702127659574  loss:2.1036689281463623\n",
      "Worker17 Test accuracy:41.02564102564103  loss:2.6698756217956543\n",
      "Worker18 Valid accuracy:40.90909090909091  loss:2.107994794845581\n",
      "Worker18 Test accuracy:44.5945945945946  loss:1.7691423892974854\n",
      "Worker19 Valid accuracy:42.18575158522939  loss:2.0496888955434165\n",
      "Worker19 Test accuracy:42.16651745747538  loss:2.0204955339431763\n",
      "Worker20 Valid accuracy:0.0  loss:2.1293132305145264\n",
      "Worker20 Test accuracy:33.333333333333336  loss:1.845424771308899\n",
      "Validation(tune)  loss:2.168594389988317  accuracy:34.28098363080228\n",
      "Test(tune)  loss:2.0134676347885816  accuracy:44.34451957161209\n"
     ]
    }
   ],
   "source": [
    "acc_tune_test = []\n",
    "loss_tune_test = []\n",
    "acc_tune_valid = []\n",
    "loss_tune_valid = []\n",
    "\n",
    "start = time.time()#開始時刻\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "    worker.local_model = worker.local_model.to(args.device)\n",
    "    _,_ = train(worker.local_model,args.criterion_ce,worker.trainloader,args.local_epochs)\n",
    "    acc_tmp,loss_tmp = test(worker.local_model,args.criterion_ce,worker.valloader)\n",
    "    acc_tune_valid.append(acc_tmp)\n",
    "    loss_tune_valid.append(loss_tmp)\n",
    "    print('Worker{} Valid accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    \n",
    "    acc_tmp,loss_tmp = test(worker.local_model,args.criterion_ce,worker.testloader)\n",
    "    acc_tune_test.append(acc_tmp)\n",
    "    loss_tune_test.append(loss_tmp)\n",
    "    print('Worker{} Test accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    worker.local_model = worker.local_model.to('cpu')\n",
    "\n",
    "end = time.time()#終了時刻\n",
    "\n",
    "acc_valid_avg = sum(acc_tune_valid)/len(acc_tune_valid)\n",
    "loss_valid_avg = sum(loss_tune_valid)/len(loss_tune_valid)\n",
    "print('Validation(tune)  loss:{}  accuracy:{}'.format(loss_valid_avg,acc_valid_avg))\n",
    "acc_test_avg = sum(acc_tune_test)/len(acc_tune_test)\n",
    "loss_test_avg = sum(loss_tune_test)/len(loss_tune_test)\n",
    "print('Test(tune)  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習＋推論時間：75.32590579986572秒\n"
     ]
    }
   ],
   "source": [
    "print('学習＋推論時間：{}秒'.format(end-start))#終了時刻-開始時刻でかかった時間"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
