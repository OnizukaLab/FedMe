{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    # random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 42\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O0TfzOhU-QlG"
   },
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 10\n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 100\n",
    "    self.local_epochs = 2\n",
    "    self.lr = None\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 100\n",
    "    self.worker_num = 20\n",
    "    self.sample_num = 20\n",
    "    self.cluster_num = 2\n",
    "    self.device = device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned value\n",
    "lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = []\n",
    "lr_list.append(10**-3.0)\n",
    "lr_list.append(10**-2.5)\n",
    "lr_list.append(10**-2.0)\n",
    "lr_list.append(10**-1.5)\n",
    "lr_list.append(10**-1.0)\n",
    "lr_list.append(10**-0.5)\n",
    "lr_list.append(10**0.0)\n",
    "lr_list.append(10**0.5)\n",
    "\n",
    "args.lr = lr_list[lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r5PuCcqmJNUQ"
   },
   "outputs": [],
   "source": [
    "class LocalDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,dataset,worker_id):\n",
    "    self.data = []\n",
    "    self.target = []\n",
    "    self.id = worker_id\n",
    "    for i in range(len(dataset)):\n",
    "      self.data.append(dataset[i][0][0])\n",
    "      self.target.append(dataset[i][1][0])\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.data[index],self.target[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/federated_trainset_shakespeare.pickle', 'rb') as f:\n",
    "    all_federated_trainset = pickle.load(f)\n",
    "with open('../data/federated_testset_shakespeare.pickle', 'rb') as f:\n",
    "    all_federated_testset = pickle.load(f)\n",
    "all_worker_num = len(all_federated_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 6, 70, 62, 57, 35, 26, 139, 22, 108, 8, 7, 23, 55, 59, 129, 50, 107, 56, 114]\n"
     ]
    }
   ],
   "source": [
    "worker_id_list = random.sample(range(all_worker_num),args.worker_num)\n",
    "print(worker_id_list)\n",
    "federated_trainset = []\n",
    "federated_testset = []\n",
    "for i in worker_id_list:\n",
    "    federated_trainset.append(all_federated_trainset[i])\n",
    "    federated_testset.append(all_federated_testset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_valset = [None]*args.worker_num\n",
    "for i in range(args.worker_num):\n",
    "  n_samples = len(federated_trainset[i])\n",
    "  if n_samples==1:\n",
    "    federated_valset[i] = copy.deepcopy(federated_trainset[i])\n",
    "  else:\n",
    "    train_size = int(len(federated_trainset[i]) * 0.7) \n",
    "    val_size = n_samples - train_size \n",
    "    federated_trainset[i],federated_valset[i] = torch.utils.data.random_split(federated_trainset[i], [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZU3vAAb9-6SD"
   },
   "outputs": [],
   "source": [
    "class RNN1(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN1, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output\n",
    "\n",
    "class RNN2(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN2, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output\n",
    "\n",
    "class RNN3(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN3, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=3, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output\n",
    "    \n",
    "class RNN4(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN4, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=4, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self):\n",
    "    self.models = []\n",
    "    for i in range(args.cluster_num):\n",
    "      self.models.append(RNN2())\n",
    "\n",
    "  def model_initialize(self,workers):\n",
    "    sample_worker = self.sample_worker(workers)\n",
    "    self.send_models(sample_worker)\n",
    "    for i,worker in enumerate(sample_worker):\n",
    "      worker.cluster = i%args.cluster_num\n",
    "      _ = worker.local_train()\n",
    "    self.aggregate_models(sample_worker)\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(federated_trainset[i],federated_valset[i],federated_testset[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "\n",
    "  def send_models(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "\n",
    "    for worker in workers:\n",
    "      worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "      worker.models = copy.deepcopy(self.models)\n",
    "      for i in range(args.cluster_num):\n",
    "        worker.models[i] = worker.models[i].to(args.device)\n",
    "\n",
    "  def aggregate_models(self,workers):\n",
    "    new_params = []\n",
    "    for i in range(args.cluster_num):   \n",
    "      new_params.append(OrderedDict())\n",
    "    total_num = [0]*args.cluster_num\n",
    "    for worker in workers:\n",
    "      total_num[worker.cluster] += worker.train_data_num\n",
    "    count = [0]*args.cluster_num    \n",
    "    for worker in workers:\n",
    "      worker_state = worker.models[worker.cluster].state_dict()\n",
    "      for key in worker_state.keys():\n",
    "        if count[worker.cluster]==0:\n",
    "          new_params[worker.cluster][key] = 1.0*worker_state[key]*worker.train_data_num/total_num[worker.cluster]\n",
    "        else:\n",
    "          new_params[worker.cluster][key] += 1.0*worker_state[key]*worker.train_data_num/total_num[worker.cluster]\n",
    "      count[worker.cluster] += 1\n",
    "      for i in range(args.cluster_num):\n",
    "        worker.models[i] = worker.models[i].to('cpu')\n",
    "      del worker.models\n",
    "\n",
    "    for i in range(args.cluster_num):\n",
    "      if total_num[i]!=0:\n",
    "        self.models[i].load_state_dict(new_params[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,trainset,valset,testset):\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.models = None\n",
    "    self.cluster = None\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "    self.aggregation_weight = None\n",
    "\n",
    "  def local_train(self):\n",
    "    acc_train,loss_train = train(self.models[self.cluster],args.criterion,self.trainloader,args.local_epochs)\n",
    "    acc_valid,loss_valid = test(self.models[self.cluster],args.criterion,self.valloader)\n",
    "    return acc_train,loss_train,acc_valid,loss_valid\n",
    "\n",
    "  def clustering(self,models):\n",
    "    for i in range(args.cluster_num):\n",
    "      if i==0:\n",
    "        cluster = 0\n",
    "        _,loss = test(models[i],args.criterion,self.trainloader)\n",
    "      else:\n",
    "        _,tmp = test(models[i],args.criterion,self.trainloader)\n",
    "        if tmp<loss:\n",
    "          cluster = i\n",
    "    self.cluster = cluster\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7-GY66gROuEU"
   },
   "outputs": [],
   "source": [
    "def train(model,criterion,trainloader,epochs):\n",
    "  optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for (data,labels) in trainloader:\n",
    "      data,labels = Variable(data),Variable(labels)\n",
    "      data,labels = data.to(args.device),labels.to(args.device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(data)\n",
    "      loss = criterion(outputs,labels)\n",
    "      running_loss += loss.item()\n",
    "      predicted = torch.argmax(outputs,dim=1)\n",
    "      correct += (predicted==labels).sum().item()\n",
    "      count += len(labels)\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "      optimizer.step()\n",
    "\n",
    "  return 100.0*correct/count,running_loss/len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  count = 0\n",
    "  for (data,labels) in testloader:\n",
    "    data,labels = data.to(args.device),labels.to(args.device)\n",
    "    outputs = model(data)\n",
    "    running_loss += criterion(outputs,labels).item()\n",
    "    predicted = torch.argmax(outputs,dim=1)\n",
    "    correct += (predicted==labels).sum().item()\n",
    "    count += len(labels)\n",
    "\n",
    "  accuracy = 100.0*correct/count\n",
    "  loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "  return accuracy,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WMO7_WSLHeGl"
   },
   "outputs": [],
   "source": [
    "class Early_Stopping():\n",
    "  def __init__(self,partience):\n",
    "    self.step = 0\n",
    "    self.loss = float('inf')\n",
    "    self.partience = partience\n",
    "\n",
    "  def validate(self,loss):\n",
    "    if self.loss<loss:\n",
    "      self.step += 1\n",
    "      if self.step>self.partience:\n",
    "        return True\n",
    "    else:\n",
    "      self.step = 0\n",
    "      self.loss = loss\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "-noG_98IR-nZ",
    "outputId": "78a6ebe2-854a-4f83-dc45-5c4ac35b69e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1  loss:4.201233466466268  accuracy:9.54439483502384\n",
      "Epoch2  loss:3.3141179588105945  accuracy:15.101252036950212\n",
      "Epoch3  loss:3.3123446202940414  accuracy:15.101252036950212\n",
      "Epoch4  loss:3.306882296005884  accuracy:15.10125203695021\n",
      "Epoch5  loss:3.299684410625034  accuracy:15.10125203695021\n",
      "Epoch6  loss:3.287763931685024  accuracy:15.109212845278442\n",
      "Epoch7  loss:3.2674460957447686  accuracy:15.446789409585273\n",
      "Epoch8  loss:3.2369401610559887  accuracy:14.931556565141184\n",
      "Epoch9  loss:3.2055156489213306  accuracy:15.851263543966734\n",
      "Epoch10  loss:3.1522547483444217  accuracy:16.027887386081986\n",
      "Epoch11  loss:3.1114813973506292  accuracy:17.212489941683756\n",
      "Epoch12  loss:3.044049673610263  accuracy:18.421511728019393\n",
      "Epoch13  loss:3.0044684294197297  accuracy:20.698671854723216\n",
      "Epoch14  loss:2.9764884203672404  accuracy:19.866483974125824\n",
      "Epoch15  loss:2.955218358172311  accuracy:20.77859805934078\n",
      "Epoch16  loss:2.9104464319017196  accuracy:22.98955768536012\n",
      "Epoch17  loss:2.882225313782692  accuracy:23.778083741846544\n",
      "Epoch18  loss:2.860725386275186  accuracy:24.079013934776448\n",
      "Epoch19  loss:2.832328752676646  accuracy:24.55006834697161\n",
      "Epoch20  loss:2.809645551111963  accuracy:24.91595237688178\n",
      "Epoch21  loss:2.7773532602522106  accuracy:25.363960040694845\n",
      "Epoch22  loss:2.7542364650302464  accuracy:25.796520655029433\n",
      "Epoch23  loss:2.7262221005227834  accuracy:26.096559005253308\n",
      "Epoch25  loss:2.6989914857678947  accuracy:26.765342794203196\n",
      "Epoch26  loss:2.6751496367984346  accuracy:27.04540755525447\n",
      "Epoch27  loss:2.6302206012937766  accuracy:28.31810917959282\n",
      "Epoch28  loss:2.625028036038081  accuracy:28.376454592290045\n",
      "Epoch29  loss:2.5880885644091505  accuracy:28.723858711899883\n",
      "Epoch30  loss:2.5727328168021306  accuracy:29.050890190968474\n",
      "Epoch31  loss:2.548657934864362  accuracy:28.892182308655112\n",
      "Epoch32  loss:2.522021626432737  accuracy:29.34282848336885\n",
      "Epoch33  loss:2.5086664481295475  accuracy:29.559306404548327\n",
      "Epoch34  loss:2.4831109311845565  accuracy:29.848001499272165\n",
      "Epoch35  loss:2.4727624452776373  accuracy:30.055174489687378\n",
      "Epoch36  loss:2.458118960923619  accuracy:30.546171517311105\n",
      "Epoch37  loss:2.441420602798462  accuracy:30.6739738414014\n",
      "Epoch38  loss:2.417263676060571  accuracy:31.454800167491676\n",
      "Epoch39  loss:2.413905374209086  accuracy:30.18634971676229\n",
      "Epoch40  loss:2.395822762118446  accuracy:30.574278269403326\n",
      "Epoch41  loss:2.378066001004642  accuracy:30.878289633795777\n",
      "Epoch42  loss:2.369076055288315  accuracy:31.59910402666082\n",
      "Epoch43  loss:2.353707236051559  accuracy:31.738046109962163\n",
      "Epoch44  loss:2.3443771955039767  accuracy:31.55253340644199\n",
      "Epoch45  loss:2.3377800044086245  accuracy:32.53902635020081\n",
      "Epoch46  loss:2.3279264705048672  accuracy:32.282940921933445\n",
      "Epoch47  loss:2.315338038073646  accuracy:32.288113427072396\n",
      "Epoch48  loss:2.3074383199214936  accuracy:33.17983413961536\n",
      "Epoch49  loss:2.291846560769611  accuracy:34.895304255284785\n",
      "Epoch50  loss:2.2983917953239548  accuracy:32.8354599252014\n",
      "Epoch51  loss:2.2844881132245067  accuracy:33.44234345519916\n",
      "Epoch52  loss:2.2781586946712604  accuracy:33.18753502257281\n",
      "Epoch53  loss:2.25985774728987  accuracy:33.962848131990896\n",
      "Epoch54  loss:2.260991052289804  accuracy:34.196044729986596\n",
      "Epoch55  loss:2.246282654007276  accuracy:33.99957093293969\n",
      "Epoch56  loss:2.244556387762229  accuracy:33.528862068444624\n",
      "Epoch57  loss:2.2402800556686193  accuracy:33.94474197786306\n",
      "Epoch58  loss:2.2253981914785173  accuracy:37.22123567642514\n",
      "Epoch59  loss:2.2200563023487727  accuracy:37.28066966388956\n",
      "Epoch60  loss:2.2153411474492812  accuracy:36.02996708625479\n",
      "Epoch61  loss:2.215659414894051  accuracy:39.18624993358272\n",
      "Epoch62  loss:2.1948947373363703  accuracy:38.479435461715894\n",
      "Epoch63  loss:2.199372889598211  accuracy:38.57484962349728\n",
      "Epoch64  loss:2.1762273351351427  accuracy:40.10472478354508\n",
      "Epoch65  loss:2.190555846359995  accuracy:38.87686089645259\n",
      "Epoch66  loss:2.179999382462767  accuracy:39.52732663826364\n",
      "Epoch67  loss:2.1760353078444803  accuracy:39.84572562218288\n",
      "Epoch68  loss:2.1611309064759148  accuracy:39.943000891999056\n",
      "Epoch69  loss:2.1679345591200723  accuracy:40.157891289000936\n",
      "Epoch70  loss:2.166155468258593  accuracy:40.89188728528743\n",
      "Epoch71  loss:2.159028288887607  accuracy:40.03281353212165\n",
      "Epoch72  loss:2.158066617614693  accuracy:41.57278879724468\n",
      "Epoch73  loss:2.158659403357241  accuracy:40.637989741216934\n",
      "Epoch74  loss:2.147190792527464  accuracy:40.167592853279054\n",
      "Epoch75  loss:2.154016470246845  accuracy:42.22275437990893\n",
      "Epoch76  loss:2.1698368643720944  accuracy:41.027905886486664\n",
      "Epoch77  loss:2.149864327741994  accuracy:41.062745460477174\n",
      "Epoch78  loss:2.1578005606929462  accuracy:40.48965435069434\n",
      "Epoch79  loss:2.1499824201067286  accuracy:40.81380425730678\n",
      "Epoch80  loss:2.158348676727878  accuracy:42.0051095838911\n",
      "Epoch81  loss:2.1504304230213167  accuracy:42.51160096141058\n",
      "Epoch82  loss:2.163636598818832  accuracy:41.94789177973006\n",
      "Epoch83  loss:2.1704959722028843  accuracy:41.5540289795172\n",
      "Epoch84  loss:2.1820291202929285  accuracy:40.35525256369745\n",
      "Epoch85  loss:2.180753900607427  accuracy:41.41740061825952\n",
      "Epoch86  loss:2.188194083339638  accuracy:41.10137943017963\n",
      "Epoch87  loss:2.1971213817596436  accuracy:41.28418097068542\n",
      "Epoch88  loss:2.193083816104465  accuracy:40.21243414090262\n",
      "Epoch89  loss:2.1875079855322834  accuracy:41.438947685196624\n",
      "Epoch90  loss:2.1935877763562734  accuracy:41.53222397931489\n",
      "Epoch91  loss:2.2006076244844333  accuracy:40.358053494623874\n",
      "Epoch92  loss:2.215432112746768  accuracy:41.00112035211971\n",
      "Epoch93  loss:2.2182815763685437  accuracy:40.80331863844958\n",
      "Epoch94  loss:2.228008518781927  accuracy:40.26888971997927\n",
      "Epoch95  loss:2.2537123726473918  accuracy:41.2963526542699\n",
      "Epoch96  loss:2.263339122633139  accuracy:40.70675285892042\n",
      "Epoch97  loss:2.258310649957922  accuracy:40.06430763816392\n",
      "Epoch98  loss:2.268940258522828  accuracy:40.276062909196106\n",
      "Epoch99  loss:2.2672795481152  accuracy:40.74422241710627\n",
      "Epoch100  loss:2.2703592436181177  accuracy:41.06678072535113\n"
     ]
    }
   ],
   "source": [
    "server = Server()\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset)\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "acc_valid = []\n",
    "loss_valid = []\n",
    "\n",
    "early_stopping = Early_Stopping(args.partience)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(args.global_epochs):\n",
    "  sample_worker = server.sample_worker(workers)\n",
    "  server.send_models(sample_worker)\n",
    "\n",
    "  acc_train_avg = 0.0\n",
    "  loss_train_avg = 0.0\n",
    "  acc_valid_avg = 0.0\n",
    "  loss_valid_avg = 0.0\n",
    "  for worker in sample_worker:\n",
    "    worker.clustering(worker.models)\n",
    "    acc_train_tmp,loss_train_tmp,acc_valid_tmp,loss_valid_tmp = worker.local_train()\n",
    "    acc_train_avg += acc_train_tmp/len(sample_worker)\n",
    "    loss_train_avg += loss_train_tmp/len(sample_worker)\n",
    "    acc_valid_avg += acc_valid_tmp/len(sample_worker)\n",
    "    loss_valid_avg += loss_valid_tmp/len(sample_worker)\n",
    "  server.aggregate_models(sample_worker)\n",
    "  '''\n",
    "  for i in range(args.cluster_num):\n",
    "    server.models[i].to(args.device)\n",
    "  for worker in workers:\n",
    "    worker.clustering(server.models)\n",
    "    acc_valid_tmp,loss_valid_tmp = test(server.models[worker.cluster],args.criterion,worker.valloader)\n",
    "    acc_valid_avg += acc_valid_tmp/len(workers)\n",
    "    loss_valid_avg += loss_valid_tmp/len(workers)\n",
    "  for i in range(args.cluster_num):\n",
    "    server.models[i].to('cpu')\n",
    "  '''\n",
    "  print('Epoch{}  loss:{}  accuracy:{}'.format(epoch+1,loss_valid_avg,acc_valid_avg))\n",
    "  acc_train.append(acc_train_avg)\n",
    "  loss_train.append(loss_train_avg)\n",
    "  acc_valid.append(acc_valid_avg)\n",
    "  loss_valid.append(loss_valid_avg)\n",
    "\n",
    "  if early_stopping.validate(loss_valid_avg):\n",
    "    print('Early Stop')\n",
    "    break\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "mi_uceyoptLP",
    "outputId": "bc067e09-01bc-4e65-daf9-ac2f42373cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 accuracy:45.96620132255694  loss:1.9606729064668929\n",
      "Worker2 accuracy:42.955326460481096  loss:2.163719415664673\n",
      "Worker3 accuracy:36.36363636363637  loss:2.4885435104370117\n",
      "Worker4 accuracy:37.878787878787875  loss:2.4763472080230713\n",
      "Worker5 accuracy:56.25  loss:1.847248911857605\n",
      "Worker6 accuracy:47.61904761904762  loss:2.7726168632507324\n",
      "Worker7 accuracy:60.0  loss:1.3037159442901611\n",
      "Worker8 accuracy:45.45454545454545  loss:2.046386241912842\n",
      "Worker9 accuracy:66.66666666666667  loss:1.3058642148971558\n",
      "Worker10 accuracy:41.935483870967744  loss:2.0918877124786377\n",
      "Worker11 accuracy:38.3177570093458  loss:2.1396853923797607\n",
      "Worker12 accuracy:50.0  loss:1.6683387756347656\n",
      "Worker13 accuracy:44.17887432536623  loss:2.011270046234131\n",
      "Worker14 accuracy:35.92814371257485  loss:2.2958807945251465\n",
      "Worker15 accuracy:50.0  loss:1.7816612720489502\n",
      "Worker16 accuracy:45.16839785961599  loss:1.9780920147895813\n",
      "Worker17 accuracy:39.743589743589745  loss:2.7724156379699707\n",
      "Worker18 accuracy:48.648648648648646  loss:1.6473751068115234\n",
      "Worker19 accuracy:42.88272157564906  loss:2.091259479522705\n",
      "Worker20 accuracy:33.333333333333336  loss:1.9628709554672241\n",
      "Test  loss:2.040292620233127  accuracy:45.46455809224067\n"
     ]
    }
   ],
   "source": [
    "acc_test = []\n",
    "loss_test = []\n",
    "\n",
    "for i in range(args.cluster_num):\n",
    "  server.models[i].to(args.device)\n",
    "\n",
    "nums = 0\n",
    "for worker in workers:\n",
    "  nums += worker.test_data_num\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.aggregation_weight = 1.0*worker.test_data_num/nums\n",
    "  worker.clustering(server.models)\n",
    "  acc_tmp,loss_tmp = test(server.models[worker.cluster],args.criterion,worker.testloader)\n",
    "  acc_test.append(acc_tmp)\n",
    "  loss_test.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_test_avg = sum(acc_test)/len(acc_test)\n",
    "loss_test_avg = sum(loss_test)/len(loss_test)\n",
    "print('Test  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 Valid accuracy:44.629516227801595  loss:2.052854988310072\n",
      "Worker1 Test accuracy:44.15870683321087  loss:2.0860771451677596\n",
      "Worker2 Valid accuracy:43.83954154727794  loss:2.0413084030151367\n",
      "Worker2 Test accuracy:41.23711340206186  loss:2.138051986694336\n",
      "Worker3 Valid accuracy:45.283018867924525  loss:2.155996084213257\n",
      "Worker3 Test accuracy:27.272727272727273  loss:2.636253595352173\n",
      "Worker4 Valid accuracy:40.50632911392405  loss:2.357844591140747\n",
      "Worker4 Test accuracy:37.878787878787875  loss:2.454663038253784\n",
      "Worker5 Valid accuracy:39.473684210526315  loss:2.6450445652008057\n",
      "Worker5 Test accuracy:50.0  loss:1.7835218906402588\n",
      "Worker6 Valid accuracy:29.166666666666668  loss:2.2516353130340576\n",
      "Worker6 Test accuracy:42.857142857142854  loss:2.7330124378204346\n",
      "Worker7 Valid accuracy:20.0  loss:3.1428864002227783\n",
      "Worker7 Test accuracy:60.0  loss:1.3584991693496704\n",
      "Worker8 Valid accuracy:51.89003436426117  loss:1.8967803716659546\n",
      "Worker8 Test accuracy:51.239669421487605  loss:1.8780072927474976\n",
      "Worker9 Valid accuracy:33.333333333333336  loss:3.4406557083129883\n",
      "Worker9 Test accuracy:66.66666666666667  loss:1.3265935182571411\n",
      "Worker10 Valid accuracy:42.35294117647059  loss:2.0529026985168457\n",
      "Worker10 Test accuracy:41.12903225806452  loss:2.128546714782715\n",
      "Worker11 Valid accuracy:36.71875  loss:2.538672924041748\n",
      "Worker11 Test accuracy:37.38317757009346  loss:2.1989052295684814\n",
      "Worker12 Valid accuracy:33.333333333333336  loss:2.1592743396759033\n",
      "Worker12 Test accuracy:50.0  loss:1.7852426767349243\n",
      "Worker13 Valid accuracy:41.40700289110183  loss:2.043412208557129\n",
      "Worker13 Test accuracy:43.71626831148805  loss:2.0258912245432534\n",
      "Worker14 Valid accuracy:38.65336658354115  loss:2.367025852203369\n",
      "Worker14 Test accuracy:38.02395209580838  loss:2.3806703090667725\n",
      "Worker15 Valid accuracy:50.0  loss:1.9633945226669312\n",
      "Worker15 Test accuracy:37.5  loss:1.7645964622497559\n",
      "Worker16 Valid accuracy:43.80902413431269  loss:2.0402159988880157\n",
      "Worker16 Test accuracy:43.877872206484106  loss:1.9963783919811249\n",
      "Worker17 Valid accuracy:46.808510638297875  loss:2.2024855613708496\n",
      "Worker17 Test accuracy:41.02564102564103  loss:2.7973568439483643\n",
      "Worker18 Valid accuracy:51.13636363636363  loss:2.1780948638916016\n",
      "Worker18 Test accuracy:48.648648648648646  loss:1.6503194570541382\n",
      "Worker19 Valid accuracy:43.56583364416262  loss:2.0299124717712402\n",
      "Worker19 Test accuracy:43.86750223813787  loss:2.0803867975870767\n",
      "Worker20 Valid accuracy:33.333333333333336  loss:2.618072509765625\n",
      "Worker20 Test accuracy:33.333333333333336  loss:1.9976054430007935\n",
      "Validation(tune)  loss:2.308923518823253  accuracy:40.462029185131634\n",
      "Test(tune)  loss:2.0600289812400225  accuracy:43.99081210098922\n"
     ]
    }
   ],
   "source": [
    "acc_tune_test = []\n",
    "loss_tune_test = []\n",
    "acc_tune_valid = []\n",
    "loss_tune_valid = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "    worker.models = copy.deepcopy(server.models)\n",
    "    worker.models[worker.cluster] = worker.models[worker.cluster].to(args.device)\n",
    "    _,_,acc_tmp,loss_tmp = worker.local_train()\n",
    "    acc_tune_valid.append(acc_tmp)\n",
    "    loss_tune_valid.append(loss_tmp)\n",
    "    print('Worker{} Valid accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    \n",
    "    acc_tmp,loss_tmp = test(worker.models[worker.cluster],args.criterion,worker.testloader)\n",
    "    acc_tune_test.append(acc_tmp)\n",
    "    loss_tune_test.append(loss_tmp)\n",
    "    print('Worker{} Test accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    for i in range(args.cluster_num):\n",
    "        worker.models[i] = worker.models[i].to('cpu')\n",
    "    del worker.models\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_valid_avg = sum(acc_tune_valid)/len(acc_tune_valid)\n",
    "loss_valid_avg = sum(loss_tune_valid)/len(loss_tune_valid)\n",
    "print('Validation(tune)  loss:{}  accuracy:{}'.format(loss_valid_avg,acc_valid_avg))\n",
    "acc_test_avg = sum(acc_tune_test)/len(acc_tune_test)\n",
    "loss_test_avg = sum(loss_tune_test)/len(loss_tune_test)\n",
    "print('Test(tune)  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 20]\n"
     ]
    }
   ],
   "source": [
    "cluster = [0]*args.cluster_num\n",
    "for worker in workers:\n",
    "  cluster[worker.cluster] += 1\n",
    "\n",
    "print(cluster)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
