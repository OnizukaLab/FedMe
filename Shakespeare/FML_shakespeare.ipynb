{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    # random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 42\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O0TfzOhU-QlG"
   },
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 10\n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 100\n",
    "    self.local_epochs = 2\n",
    "    self.lr = None\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 100\n",
    "    self.worker_num = 20\n",
    "    self.sample_num = 20\n",
    "    self.device = device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion_ce = nn.CrossEntropyLoss()\n",
    "    self.criterion_kl = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = []\n",
    "lr_list.append(10**-3.0)\n",
    "lr_list.append(10**-2.5)\n",
    "lr_list.append(10**-2.0)\n",
    "lr_list.append(10**-1.5)\n",
    "lr_list.append(10**-1.0)\n",
    "lr_list.append(10**-0.5)\n",
    "lr_list.append(10**0.0)\n",
    "lr_list.append(10**0.5)\n",
    "\n",
    "args.lr = lr_list[lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r5PuCcqmJNUQ"
   },
   "outputs": [],
   "source": [
    "class LocalDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,dataset,worker_id):\n",
    "    self.data = []\n",
    "    self.target = []\n",
    "    self.id = worker_id\n",
    "    for i in range(len(dataset)):\n",
    "      self.data.append(dataset[i][0][0])\n",
    "      self.target.append(dataset[i][1][0])\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.data[index],self.target[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/federated_trainset_shakespeare.pickle', 'rb') as f:\n",
    "    all_federated_trainset = pickle.load(f)\n",
    "with open('../data/federated_testset_shakespeare.pickle', 'rb') as f:\n",
    "    all_federated_testset = pickle.load(f)\n",
    "all_worker_num = len(all_federated_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 6, 70, 62, 57, 35, 26, 139, 22, 108, 8, 7, 23, 55, 59, 129, 50, 107, 56, 114]\n"
     ]
    }
   ],
   "source": [
    "worker_id_list = random.sample(range(all_worker_num),args.worker_num)\n",
    "print(worker_id_list)\n",
    "federated_trainset = []\n",
    "federated_testset = []\n",
    "for i in worker_id_list:\n",
    "    federated_trainset.append(all_federated_trainset[i])\n",
    "    federated_testset.append(all_federated_testset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_valset = [None]*args.worker_num\n",
    "for i in range(args.worker_num):\n",
    "  n_samples = len(federated_trainset[i])\n",
    "  if n_samples==1:\n",
    "    federated_valset[i] = copy.deepcopy(federated_trainset[i])\n",
    "  else:\n",
    "    train_size = int(len(federated_trainset[i]) * 0.7) \n",
    "    val_size = n_samples - train_size \n",
    "    federated_trainset[i],federated_valset[i] = torch.utils.data.random_split(federated_trainset[i], [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZU3vAAb9-6SD"
   },
   "outputs": [],
   "source": [
    "class RNN1(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN1, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output\n",
    "\n",
    "class RNN2(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN2, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output\n",
    "\n",
    "class RNN3(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN3, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=3, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output\n",
    "    \n",
    "class RNN4(nn.Module):\n",
    "    def __init__(self, embedding_dim=8, vocab_size=90, hidden_size=256):\n",
    "        super(RNN4, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=4, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embeds = self.embeddings(input_seq)\n",
    "        # Note that the order of mini-batch is random so there is no hidden relationship among batches.\n",
    "        # So we do not input the previous batch's hidden state,\n",
    "        # leaving the first hidden state zero `self.lstm(embeds, None)`.\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # use the final hidden state as the next character prediction\n",
    "        final_hidden_state = lstm_out[:, -1]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self):\n",
    "    self.global_model = RNN2()\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(federated_trainset[i],federated_valset[i],federated_testset[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "\n",
    "  def send_model(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "\n",
    "    for worker in workers:\n",
    "      worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "      worker.global_model = copy.deepcopy(self.global_model)\n",
    "\n",
    "  def aggregate_model(self,workers):   \n",
    "    new_params = OrderedDict()\n",
    "    for i,worker in enumerate(workers):\n",
    "      worker_state = worker.global_model.state_dict()\n",
    "      for key in worker_state.keys():\n",
    "        if i==0:\n",
    "          new_params[key] = worker_state[key]*worker.aggregation_weight\n",
    "        else:\n",
    "          new_params[key] += worker_state[key]*worker.aggregation_weight\n",
    "      worker.global_model = worker.global_model.to('cpu')\n",
    "      del worker.global_model\n",
    "    self.global_model.load_state_dict(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,trainset,valset,testset):\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.local_model = RNN2()\n",
    "    self.global_model = None\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "    self.aggregation_weight = None\n",
    "\n",
    "  def local_train(self):\n",
    "    self.local_model = self.local_model.to(args.device)\n",
    "    self.global_model = self.global_model.to(args.device)\n",
    "    local_optimizer = optim.SGD(self.local_model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "    global_optimizer = optim.SGD(self.global_model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "    self.local_model.train()\n",
    "    self.global_model.train()\n",
    "    for epoch in range(args.local_epochs):\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in self.trainloader:\n",
    "        data,labels = Variable(data),Variable(labels)\n",
    "        data,labels = data.to(args.device),labels.to(args.device)\n",
    "        local_optimizer.zero_grad()\n",
    "        global_optimizer.zero_grad()\n",
    "        local_outputs = self.local_model(data)\n",
    "        global_outputs = self.global_model(data)\n",
    "        #train local_model\n",
    "        ce_loss = args.criterion_ce(local_outputs,labels)\n",
    "        kl_loss = args.criterion_kl(F.log_softmax(local_outputs, dim = 1),F.softmax(Variable(global_outputs), dim=1))\n",
    "        loss = ce_loss + kl_loss\n",
    "        running_loss += loss.item()\n",
    "        predicted = torch.argmax(local_outputs,dim=1)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "        count += len(labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), args.clip)\n",
    "        local_optimizer.step()\n",
    "\n",
    "        #train global_model\n",
    "        ce_loss = args.criterion_ce(global_outputs,labels)\n",
    "        kl_loss = args.criterion_kl(F.log_softmax(global_outputs, dim = 1),F.softmax(Variable(local_outputs), dim=1))\n",
    "        loss = ce_loss + kl_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.global_model.parameters(), args.clip)\n",
    "        global_optimizer.step()\n",
    "        \n",
    "    return 100.0*correct/count,running_loss/len(self.trainloader)\n",
    "\n",
    "\n",
    "  def validate(self):\n",
    "    acc,loss = test(self.local_model,args.criterion_ce,self.valloader)\n",
    "    self.local_model = self.local_model.to('cpu')\n",
    "    return acc,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7-GY66gROuEU"
   },
   "outputs": [],
   "source": [
    "def train(model,criterion,trainloader,epochs):\n",
    "  optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for (data,labels) in trainloader:\n",
    "      data,labels = Variable(data),Variable(labels)\n",
    "      data,labels = data.to(args.device),labels.to(args.device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(data)\n",
    "      loss = criterion(outputs,labels)\n",
    "      running_loss += loss.item()\n",
    "      predicted = torch.argmax(outputs,dim=1)\n",
    "      correct += (predicted==labels).sum().item()\n",
    "      count += len(labels)\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "      optimizer.step()\n",
    "\n",
    "  return 100.0*correct/count,running_loss/len(trainloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  count = 0\n",
    "  for (data,labels) in testloader:\n",
    "    data,labels = data.to(args.device),labels.to(args.device)\n",
    "    outputs = model(data)\n",
    "    running_loss += criterion(outputs,labels).item()\n",
    "    predicted = torch.argmax(outputs,dim=1)\n",
    "    correct += (predicted==labels).sum().item()\n",
    "    count += len(labels)\n",
    "\n",
    "  accuracy = 100.0*correct/count\n",
    "  loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "  return accuracy,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WMO7_WSLHeGl"
   },
   "outputs": [],
   "source": [
    "class Early_Stopping():\n",
    "  def __init__(self,partience):\n",
    "    self.step = 0\n",
    "    self.loss = float('inf')\n",
    "    self.partience = partience\n",
    "\n",
    "  def validate(self,loss):\n",
    "    if self.loss<loss:\n",
    "      self.step += 1\n",
    "      if self.step>self.partience:\n",
    "        return True\n",
    "    else:\n",
    "      self.step = 0\n",
    "      self.loss = loss\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "-noG_98IR-nZ",
    "outputId": "78a6ebe2-854a-4f83-dc45-5c4ac35b69e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1  loss:4.201752684844866  accuracy:9.991663776151153\n",
      "Epoch2  loss:4.11811772816711  accuracy:12.767918703616878\n",
      "Epoch3  loss:4.028105622529983  accuracy:13.39291870361688\n",
      "Epoch4  loss:3.9563148683971834  accuracy:13.602476776693015\n",
      "Epoch5  loss:3.9112512191136677  accuracy:13.625746831806303\n",
      "Epoch6  loss:3.880119789308971  accuracy:13.666754761533863\n",
      "Epoch7  loss:3.852002244194349  accuracy:13.823440992693357\n",
      "Epoch8  loss:3.822513095537821  accuracy:14.975727404993139\n",
      "Epoch9  loss:3.7927302247948123  accuracy:15.032598244452782\n",
      "Epoch10  loss:3.761910260717074  accuracy:15.235072496112004\n",
      "Epoch11  loss:3.7314223925272625  accuracy:15.318936746342745\n",
      "Epoch12  loss:3.7005935543113284  accuracy:15.414924272069769\n",
      "Epoch13  loss:3.672152870562341  accuracy:16.006347114447205\n",
      "Epoch14  loss:3.647387276093165  accuracy:16.106579773230095\n",
      "Epoch15  loss:3.625156699617704  accuracy:16.266819148918334\n",
      "Epoch16  loss:3.603428922096888  accuracy:16.294620694748566\n",
      "Epoch17  loss:3.5818585852781935  accuracy:16.585628117928405\n",
      "Epoch18  loss:3.5640908088949  accuracy:16.565964683048435\n",
      "Epoch19  loss:3.54630746046702  accuracy:16.667431865437653\n",
      "Epoch20  loss:3.5305907448132836  accuracy:16.83672389076566\n",
      "Epoch21  loss:3.5186319440603255  accuracy:16.926368993173334\n",
      "Epoch22  loss:3.5051205962896352  accuracy:16.976095510287575\n",
      "Epoch23  loss:3.492055555515819  accuracy:17.01030599736731\n",
      "Epoch24  loss:3.478784756859143  accuracy:17.213975825956172\n",
      "Epoch25  loss:3.4673939701583643  accuracy:17.35359930692422\n",
      "Epoch26  loss:3.4547971155908375  accuracy:17.487874983759514\n",
      "Epoch27  loss:3.4403896580139803  accuracy:17.604002602344092\n",
      "Epoch28  loss:3.4311485826969146  accuracy:17.61248502354974\n",
      "Epoch29  loss:3.420880882276429  accuracy:17.657231370121554\n",
      "Epoch30  loss:3.4090920163525475  accuracy:17.811428065733114\n",
      "Epoch31  loss:3.403750887844298  accuracy:17.874558063312026\n",
      "Epoch32  loss:3.3932745420270494  accuracy:17.934337557941582\n",
      "Epoch33  loss:3.385076506601439  accuracy:18.066325751176336\n",
      "Epoch34  loss:3.3782944930924312  accuracy:18.117510170933972\n",
      "Epoch35  loss:3.3717871566613518  accuracy:18.06590098052116\n",
      "Epoch36  loss:3.3633159283134675  accuracy:18.42889999984192\n",
      "Epoch37  loss:3.360071831610467  accuracy:18.43298265277558\n",
      "Epoch38  loss:3.3501684135860854  accuracy:18.626107535226353\n",
      "Epoch39  loss:3.3469841937224074  accuracy:18.513798260782043\n",
      "Epoch40  loss:3.337461247709062  accuracy:18.746208768715757\n",
      "Epoch41  loss:3.333815669351154  accuracy:18.846468593415523\n",
      "Epoch42  loss:3.3291514734427134  accuracy:18.89444084767645\n",
      "Epoch43  loss:3.3247827738523488  accuracy:19.05988354049358\n",
      "Epoch44  loss:3.320531151692072  accuracy:19.0314380169376\n",
      "Epoch45  loss:3.313844546344545  accuracy:19.135567678146828\n",
      "Epoch46  loss:3.306819238265355  accuracy:19.276574896883563\n",
      "Epoch47  loss:3.304878894819153  accuracy:19.308950546615435\n",
      "Epoch48  loss:3.299048771460851  accuracy:19.430960630165117\n",
      "Epoch49  loss:3.2932331899801897  accuracy:19.46996125149422\n",
      "Epoch50  loss:3.288272025518947  accuracy:19.629575868301192\n",
      "Epoch51  loss:3.283490772710907  accuracy:19.586866105303088\n",
      "Epoch52  loss:3.2819845762517716  accuracy:19.764620476497846\n",
      "Epoch53  loss:3.2756937344868975  accuracy:19.79219030002392\n",
      "Epoch54  loss:3.273137400547663  accuracy:19.74149446048364\n",
      "Epoch55  loss:3.2696994665596226  accuracy:19.84776439707817\n",
      "Epoch56  loss:3.2658386737108227  accuracy:19.88613975433553\n",
      "Epoch57  loss:3.262797204322285  accuracy:19.904869983155873\n",
      "Epoch58  loss:3.2587445992562505  accuracy:20.045837464297374\n",
      "Epoch59  loss:3.258472399579154  accuracy:19.988244487315182\n",
      "Epoch60  loss:3.2557162228557797  accuracy:19.97216084738998\n",
      "Epoch61  loss:3.2494488514131974  accuracy:20.130917445194875\n",
      "Epoch62  loss:3.2463255196809766  accuracy:20.06941188944746\n",
      "Epoch63  loss:3.2434040278196328  accuracy:20.175512346917394\n",
      "Epoch64  loss:3.2424863343437518  accuracy:20.253068331413655\n",
      "Epoch65  loss:3.23943870332506  accuracy:20.18095379369625\n",
      "Epoch66  loss:3.237355119321082  accuracy:20.17070917524713\n",
      "Epoch67  loss:3.2342887236012356  accuracy:20.314227309119627\n",
      "Epoch68  loss:3.232761565513081  accuracy:20.25445405462363\n",
      "Epoch69  loss:3.228663290374809  accuracy:20.37319448144859\n",
      "Epoch70  loss:3.228141396741073  accuracy:20.547141201692853\n",
      "Epoch71  loss:3.223775049216217  accuracy:20.178605338192945\n",
      "Epoch72  loss:3.224806486566861  accuracy:20.524139109719123\n",
      "Epoch73  loss:3.22440042015579  accuracy:20.63280456089529\n",
      "Epoch74  loss:3.22258382803864  accuracy:20.433797215780594\n",
      "Epoch75  loss:3.2211559144987  accuracy:20.61219410298169\n",
      "Epoch76  loss:3.2179938866032494  accuracy:20.440500981545483\n",
      "Epoch77  loss:3.2185553083817164  accuracy:20.592984607842922\n",
      "Epoch78  loss:3.215201684335867  accuracy:20.45372807536913\n",
      "Epoch79  loss:3.217617191871007  accuracy:20.48892964223366\n",
      "Epoch80  loss:3.2135187660654383  accuracy:20.428562282973715\n",
      "Epoch81  loss:3.212858670618799  accuracy:20.731917636060675\n",
      "Epoch82  loss:3.2130925034483266  accuracy:20.585140821668286\n",
      "Epoch83  loss:3.2112561666303208  accuracy:20.42347299490905\n",
      "Epoch84  loss:3.213144917454985  accuracy:20.541751934488925\n",
      "Epoch85  loss:3.210526457097795  accuracy:20.528814469547072\n",
      "Epoch86  loss:3.2098564964201715  accuracy:20.5823104398793\n",
      "Epoch87  loss:3.208383245600594  accuracy:20.671988960125507\n",
      "Epoch88  loss:3.2081154068311055  accuracy:20.477449009399955\n",
      "Epoch89  loss:3.210470388995277  accuracy:20.60138644335146\n",
      "Epoch90  loss:3.2077024241288505  accuracy:20.527783217899586\n",
      "Epoch91  loss:3.2089454346232946  accuracy:20.683059955391137\n",
      "Epoch92  loss:3.2078776677449543  accuracy:20.696527032663194\n",
      "Epoch93  loss:3.2098754001988303  accuracy:20.705717372677412\n",
      "Epoch94  loss:3.207548501094182  accuracy:20.66269734843198\n",
      "Epoch95  loss:3.206856035192808  accuracy:20.833307774711866\n",
      "Epoch96  loss:3.20891008608871  accuracy:20.706275816970837\n",
      "Epoch97  loss:3.211802479624748  accuracy:20.821075821684982\n",
      "Epoch98  loss:3.2086269362105257  accuracy:20.74901576780247\n",
      "Epoch99  loss:3.2095289362801442  accuracy:20.696230815659778\n",
      "Epoch100  loss:3.2100875473684733  accuracy:20.61610914644373\n"
     ]
    }
   ],
   "source": [
    "server = Server()\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset)\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "acc_valid = []\n",
    "loss_valid = []\n",
    "\n",
    "early_stopping = Early_Stopping(args.partience)\n",
    "\n",
    "start = time.time()#開始時刻\n",
    "\n",
    "for epoch in range(args.global_epochs):\n",
    "  sample_worker = server.sample_worker(workers)\n",
    "  server.send_model(sample_worker)\n",
    "\n",
    "  acc_train_avg = 0.0\n",
    "  loss_train_avg = 0.0\n",
    "  acc_valid_avg = 0.0\n",
    "  loss_valid_avg = 0.0\n",
    "  for worker in sample_worker:\n",
    "    acc_train_tmp,loss_train_tmp = worker.local_train()\n",
    "    acc_valid_tmp,loss_valid_tmp = worker.validate()\n",
    "    acc_train_avg += acc_train_tmp/len(sample_worker)\n",
    "    loss_train_avg += loss_train_tmp/len(sample_worker)\n",
    "    acc_valid_avg += acc_valid_tmp/len(sample_worker)\n",
    "    loss_valid_avg += loss_valid_tmp/len(sample_worker)\n",
    "  server.aggregate_model(sample_worker)\n",
    "  '''\n",
    "  server.model.to(args.device)\n",
    "  for worker in workers:\n",
    "    acc_valid_tmp,loss_valid_tmp = test(server.model,args.criterion,worker.valloader)\n",
    "    acc_valid_avg += acc_valid_tmp/len(workers)\n",
    "    loss_valid_avg += loss_valid_tmp/len(workers)\n",
    "  server.model.to('cpu')\n",
    "  '''\n",
    "  print('Epoch{}  loss:{}  accuracy:{}'.format(epoch+1,loss_valid_avg,acc_valid_avg))\n",
    "  acc_train.append(acc_train_avg)\n",
    "  loss_train.append(loss_train_avg)\n",
    "  acc_valid.append(acc_valid_avg)\n",
    "  loss_valid.append(loss_valid_avg)\n",
    "\n",
    "  if early_stopping.validate(loss_valid_avg):\n",
    "    print('Early Stop')\n",
    "    break\n",
    "    \n",
    "end = time.time()#終了時刻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習時間：48188.966247558594秒\n"
     ]
    }
   ],
   "source": [
    "print('学習時間：{}秒'.format(end-start))#終了時刻-開始時刻でかかった時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 accuracy:43.39456282145481  loss:2.1597980090550015\n",
      "Worker2 accuracy:21.305841924398624  loss:2.992985248565674\n",
      "Worker3 accuracy:11.363636363636363  loss:3.0394513607025146\n",
      "Worker4 accuracy:19.696969696969695  loss:3.1893320083618164\n",
      "Worker5 accuracy:25.0  loss:2.9922149181365967\n",
      "Worker6 accuracy:19.047619047619047  loss:3.425443649291992\n",
      "Worker7 accuracy:20.0  loss:4.347556114196777\n",
      "Worker8 accuracy:25.619834710743802  loss:2.904737949371338\n",
      "Worker9 accuracy:33.333333333333336  loss:4.417794704437256\n",
      "Worker10 accuracy:23.991935483870968  loss:2.7937862873077393\n",
      "Worker11 accuracy:24.299065420560748  loss:2.992295980453491\n",
      "Worker12 accuracy:17.5  loss:3.0784547328948975\n",
      "Worker13 accuracy:40.362374710871244  loss:2.177002509435018\n",
      "Worker14 accuracy:25.149700598802394  loss:2.755488634109497\n",
      "Worker15 accuracy:12.5  loss:4.086031913757324\n",
      "Worker16 accuracy:40.60434372049103  loss:2.155559480190277\n",
      "Worker17 accuracy:19.23076923076923  loss:3.3155150413513184\n",
      "Worker18 accuracy:22.972972972972972  loss:3.129605770111084\n",
      "Worker19 accuracy:39.256938227394805  loss:2.2275853157043457\n",
      "Worker20 accuracy:0.0  loss:4.5039544105529785\n",
      "Test  loss:3.134229701899347  accuracy:24.231494913194453\n"
     ]
    }
   ],
   "source": [
    "acc_test = []\n",
    "loss_test = []\n",
    "\n",
    "start = time.time()#開始時刻\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.global_model = copy.deepcopy(server.global_model)\n",
    "  _,_ = worker.local_train()\n",
    "  acc_tmp,loss_tmp = test(worker.local_model,args.criterion_ce,worker.testloader)\n",
    "  acc_test.append(acc_tmp)\n",
    "  loss_test.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "  worker.local_model = worker.local_model.to('cpu')\n",
    "  worker.global_model = worker.global_model.to('cpu')\n",
    "  del worker.global_model\n",
    "\n",
    "end = time.time()#終了時刻\n",
    "\n",
    "acc_test_avg = sum(acc_test)/len(acc_test)\n",
    "loss_test_avg = sum(loss_test)/len(loss_test)\n",
    "print('Test  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推論時間：476.4003014564514秒\n"
     ]
    }
   ],
   "source": [
    "print('推論時間：{}秒'.format(end-start))#終了時刻-開始時刻でかかった時間"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
